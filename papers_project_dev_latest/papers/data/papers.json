[
    {
        "new_title": "MetaGFN: Exploring Distant Modes with Adapted Metadynamics for\n  Continuous GFlowNets",
        "new_link": "http://arxiv.org/abs/2408.15905v1",
        "new_summary": "  Generative Flow Networks (GFlowNets) are a class of generative models that\nsample objects in proportion to a specified reward function through a learned\npolicy. They can be trained either on-policy or off-policy, needing a balance\nbetween exploration and exploitation for fast convergence to a target\ndistribution. While exploration strategies for discrete GFlowNets have been\nstudied, exploration in the continuous case remains to be investigated, despite\nthe potential for novel exploration algorithms due to the local connectedness\nof continuous domains. Here, we introduce Adapted Metadynamics, a variant of\nmetadynamics that can be applied to arbitrary black-box reward functions on\ncontinuous domains. We use Adapted Metadynamics as an exploration strategy for\ncontinuous GFlowNets. We show three continuous domains where the resulting\nalgorithm, MetaGFN, accelerates convergence to the target distribution and\ndiscovers more distant reward modes than previous off-policy exploration\nstrategies used for GFlowNets.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15905v1.pdf",
        "similarity": 0.9999999946686063,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Statistical QoS Provision in Business-Centric Networks",
        "new_link": "http://arxiv.org/abs/2408.15609v1",
        "new_summary": "  More refined resource management and Quality of Service (QoS) provisioning is\na critical goal of wireless communication technologies. In this paper, we\npropose a novel Business-Centric Network (BCN) aimed at enabling scalable QoS\nprovisioning, based on a cross-layer framework that captures the relationship\nbetween application, transport parameters, and channels. We investigate both\ncontinuous flow and event-driven flow models, presenting key QoS metrics such\nas throughput, delay, and reliability. By jointly considering power and\nbandwidth allocation, transmission parameters, and AP network topology across\nlayers, we optimize weighted resource efficiency with statistical QoS\nprovisioning. To address the coupling among parameters, we propose a novel deep\nreinforcement learning (DRL) framework, which is Collaborative Optimization\namong Heterogeneous Actors with Experience Sharing (COHA-ES). Power and\nsub-channel (SC) Actors representing multiple APs are jointly optimized under\nthe unified guidance of a common critic. Additionally, we introduce a novel\nmultithreaded experience-sharing mechanism to accelerate training and enhance\nrewards. Extensive comparative experiments validate the effectiveness of our\nDRL framework in terms of convergence and efficiency. Moreover, comparative\nanalyses demonstrate the comprehensive advantages of the BCN structure in\nenhancing both spectral and energy efficiency.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15609v1.pdf",
        "similarity": 0.5203727024470809,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Atari-GPT: Investigating the Capabilities of Multimodal Large Language\n  Models as Low-Level Policies for Atari Games",
        "new_link": "http://arxiv.org/abs/2408.15950v1",
        "new_summary": "  Recent advancements in large language models (LLMs) have expanded their\ncapabilities beyond traditional text-based tasks to multimodal domains,\nintegrating visual, auditory, and textual data. While multimodal LLMs have been\nextensively explored for high-level planning in domains like robotics and\ngames, their potential as low-level controllers remains largely untapped. This\npaper explores the application of multimodal LLMs as low-level controllers in\nthe domain of Atari video games, introducing Atari game performance as a new\nbenchmark for evaluating the ability of multimodal LLMs to perform low-level\ncontrol tasks. Unlike traditional reinforcement learning (RL) and imitation\nlearning (IL) methods that require extensive computational resources as well as\nreward function specification, these LLMs utilize pre-existing multimodal\nknowledge to directly engage with game environments. Our study assesses\nmultiple multimodal LLMs performance against traditional RL agents, human\nplayers, and random agents, focusing on their ability to understand and\ninteract with complex visual scenes and formulate strategic responses.\nAdditionally, we examine the impact of In-Context Learning (ICL) by\nincorporating human-demonstrated game-play trajectories to enhance the models\ncontextual understanding. Through this investigation, we aim to determine the\nextent to which multimodal LLMs can leverage their extensive training to\neffectively function as low-level controllers, thereby redefining potential\napplications in dynamic and visually complex environments. Additional results\nand videos are available at our project webpage:\nhttps://sites.google.com/view/atari-gpt/.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15950v1.pdf",
        "similarity": 0.5101879235518196,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning",
        "new_link": "http://arxiv.org/abs/2408.03195v1",
        "new_summary": "  The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.03195v1.pdf",
        "similarity": 0.5060021257923437,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-06"
    },
    {
        "new_title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2408.10504v1",
        "new_summary": "  Prompt engineering has demonstrated remarkable success in enhancing the\nperformance of large language models (LLMs) across diverse tasks. However, most\nexisting prompt optimization methods only focus on the task-level performance,\noverlooking the importance of query-preferred prompts, which leads to\nsuboptimal performances. Additionally, these methods rely heavily on frequent\ninteractions with LLMs to obtain feedback for guiding the optimization process,\nincurring substantial redundant interaction costs. In this paper, we introduce\nQuery-dependent Prompt Optimization (QPO), which leverages multi-loop offline\nreinforcement learning to iteratively fine-tune a small pretrained language\nmodel to generate optimal prompts tailored to the input queries, thus\nsignificantly improving the prompting effect on the large target LLM. We derive\ninsights from offline prompting demonstration data, which already exists in\nlarge quantities as a by-product of benchmarking diverse prompts on\nopen-sourced tasks, thereby circumventing the expenses of online interactions.\nFurthermore, we continuously augment the offline dataset with the generated\nprompts in each loop, as the prompts from the fine-tuned model are supposed to\noutperform the source prompts in the original dataset. These iterative loops\nbootstrap the model towards generating optimal prompts. Experiments on various\nLLM scales and diverse NLP and math tasks demonstrate the efficacy and\ncost-efficiency of our method in both zero-shot and few-shot scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.10504v1.pdf",
        "similarity": 0.4929255040158345,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-20"
    },
    {
        "new_title": "Deep Reinforcement Learning for Radiative Heat Transfer Optimization\n  Problems",
        "new_link": "http://arxiv.org/abs/2408.15727v1",
        "new_summary": "  Reinforcement learning is a subfield of machine learning that is having a\nhuge impact in the different conventional disciplines, including physical\nsciences. Here, we show how reinforcement learning methods can be applied to\nsolve optimization problems in the context of radiative heat transfer. We\nillustrate their use with the optimization of the near-field radiative heat\ntransfer between multilayer hyperbolic metamaterials. Specifically, we show how\nthis problem can be formulated in the language of reinforcement learning and\ntackled with a variety of algorithms. We show that these algorithms allow us to\nfind solutions that outperform those obtained using physical intuition.\nOverall, our work shows the power and potential of reinforcement learning\nmethods for the investigation of a wide variety of problems in the context of\nradiative heat transfer and related topics.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15727v1.pdf",
        "similarity": 0.4908033674065337,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Large Language Models Prompting With Episodic Memory",
        "new_link": "http://arxiv.org/abs/2408.07465v1",
        "new_summary": "  Prompt optimization is essential for enhancing the performance of Large\nLanguage Models (LLMs) in a range of Natural Language Processing (NLP) tasks,\nparticularly in scenarios of few-shot learning where training examples are\nincorporated directly into the prompt. Despite the growing interest in\noptimizing prompts with few-shot examples, existing methods for prompt\noptimization are often resource-intensive or perform inadequately. In this\nwork, we propose PrOmpting with Episodic Memory (POEM), a novel prompt\noptimization technique that is simple, efficient, and demonstrates strong\ngeneralization capabilities. We approach prompt optimization as a Reinforcement\nLearning (RL) challenge, using episodic memory to archive combinations of input\ndata, permutations of few-shot examples, and the rewards observed during\ntraining. In the testing phase, we optimize the sequence of examples for each\ntest query by selecting the sequence that yields the highest total rewards from\nthe top-k most similar training examples in the episodic memory. Our results\nshow that POEM outperforms recent techniques like TEMPERA and RLPrompt by over\n5.3% in various text classification tasks. Furthermore, our approach adapts\nwell to broader language understanding tasks, consistently outperforming\nconventional heuristic methods for ordering examples.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.07465v1.pdf",
        "similarity": 0.4764720765087034,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-14"
    },
    {
        "new_title": "DeMoBot: Deformable Mobile Manipulation with Vision-based Sub-goal\n  Retrieval",
        "new_link": "http://arxiv.org/abs/2408.15919v1",
        "new_summary": "  Imitation learning (IL) algorithms typically distill experience into\nparametric behavior policies to mimic expert demonstrations. Despite their\neffectiveness, previous methods often struggle with data efficiency and\naccurately aligning the current state with expert demonstrations, especially in\ndeformable mobile manipulation tasks characterized by partial observations and\ndynamic object deformations. In this paper, we introduce \\textbf{DeMoBot}, a\nnovel IL approach that directly retrieves observations from demonstrations to\nguide robots in \\textbf{De}formable \\textbf{Mo}bile manipulation tasks. DeMoBot\nutilizes vision foundation models to identify relevant expert data based on\nvisual similarity and matches the current trajectory with demonstrated\ntrajectories using trajectory similarity and forward reachability constraints\nto select suitable sub-goals. Once a goal is determined, a motion generation\npolicy will guide the robot to the next state until the task is completed. We\nevaluated DeMoBot using a Spot robot in several simulated and real-world\nsettings, demonstrating its effectiveness and generalizability. With only 20\ndemonstrations, DeMoBot significantly outperforms the baselines, reaching a\n50\\% success rate in curtain opening and 85\\% in gap covering in simulation.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15919v1.pdf",
        "similarity": 0.46926089764142637,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Dissipation-driven quantum generative adversarial networks",
        "new_link": "http://arxiv.org/abs/2408.15597v1",
        "new_summary": "  Quantum machine learning holds the promise of harnessing quantum advantage to\nachieve speedup beyond classical algorithms. Concurrently, research indicates\nthat dissipation can serve as an effective resource in quantum computation. In\nthis paper, we introduce a novel dissipation-driven quantum generative\nadversarial network (DQGAN) architecture specifically tailored for generating\nclassical data. Our DQGAN comprises two interacting networks: a generative\nnetwork and a discriminative network, both constructed from qubits. The\nclassical data is encoded into the input qubits of the input layer via strong\ntailored dissipation processes. This encoding scheme enables us to extract both\nthe generated data and the classification results by measuring the observables\nof the steady state of the output qubits. The network coupling weight, i.e.,\nthe strength of the interaction Hamiltonian between layers, is iteratively\nupdated during the training process. This training procedure closely resembles\nthe training of conventional generative adversarial networks (GANs). By\nalternately updating the two networks, we foster adversarial learning until the\nequilibrium point is reached. Our preliminary numerical test on a simplified\ninstance of the task substantiate the feasibility of our DQGAN model.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15597v1.pdf",
        "similarity": 0.4654160620768271,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing",
        "new_link": "http://arxiv.org/abs/2408.15800v1",
        "new_summary": "  Achieving personalized intelligence at the edge with real-time learning\ncapabilities holds enormous promise in enhancing our daily experiences and\nhelping decision making, planning, and sensing. However, efficient and reliable\nedge learning remains difficult with current technology due to the lack of\npersonalized data, insufficient hardware capabilities, and inherent challenges\nposed by online learning.\n  Over time and across multiple developmental stages, the brain has evolved to\nefficiently incorporate new knowledge by gradually building on previous\nknowledge. In this work, we emulate the multiple stages of learning with\ndigital neuromorphic technology that simulates the neural and synaptic\nprocesses of the brain using two stages of learning. First, a meta-training\nstage trains the hyperparameters of synaptic plasticity for one-shot learning\nusing a differentiable simulation of the neuromorphic hardware. This\nmeta-training process refines a hardware local three-factor synaptic plasticity\nrule and its associated hyperparameters to align with the trained task domain.\nIn a subsequent deployment stage, these optimized hyperparameters enable fast,\ndata-efficient, and accurate learning of new classes. We demonstrate our\napproach using event-driven vision sensor data and the Intel Loihi neuromorphic\nprocessor with its plasticity dynamics, achieving real-time one-shot learning\nof new classes that is vastly improved over transfer learning. Our methodology\ncan be deployed with arbitrary plasticity models and can be applied to\nsituations demanding quick learning and adaptation at the edge, such as\nnavigating unfamiliar environments or learning unexpected categories of data\nthrough user engagement.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15800v1.pdf",
        "similarity": 0.46539387923969777,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "GANs Conditioning Methods: A Survey",
        "new_link": "http://arxiv.org/abs/2408.15640v1",
        "new_summary": "  In recent years, Generative Adversarial Networks (GANs) have seen significant\nadvancements, leading to their widespread adoption across various fields. The\noriginal GAN architecture enables the generation of images without any specific\ncontrol over the content, making it an unconditional generation process.\nHowever, many practical applications require precise control over the generated\noutput, which has led to the development of conditional GANs (cGANs) that\nincorporate explicit conditioning to guide the generation process. cGANs extend\nthe original framework by incorporating additional information (conditions),\nenabling the generation of samples that adhere to that specific criteria.\nVarious conditioning methods have been proposed, each differing in how they\nintegrate the conditioning information into both the generator and the\ndiscriminator networks. In this work, we review the conditioning methods\nproposed for GANs, exploring the characteristics of each method and\nhighlighting their unique mechanisms and theoretical foundations. Furthermore,\nwe conduct a comparative analysis of these methods, evaluating their\nperformance on various image datasets. Through these analyses, we aim to\nprovide insights into the strengths and limitations of various conditioning\ntechniques, guiding future research and application in generative modeling.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15640v1.pdf",
        "similarity": 0.4652478479028213,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "In-Context Imitation Learning via Next-Token Prediction",
        "new_link": "http://arxiv.org/abs/2408.15980v1",
        "new_summary": "  We explore how to enhance next-token prediction models to perform in-context\nimitation learning on a real robot, where the robot executes new tasks by\ninterpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot\nTransformer (ICRT), a causal transformer that performs autoregressive\nprediction on sensorimotor trajectories without relying on any linguistic data\nor reward function. This formulation enables flexible and training-free\nexecution of new tasks at test time, achieved by prompting the model with\nsensorimotor trajectories of the new task composing of image observations,\nactions and states tuples, collected through human teleoperation. Experiments\nwith a Franka Emika robot demonstrate that the ICRT can adapt to new tasks\nspecified by prompts, even in environment configurations that differ from both\nthe prompt and the training data. In a multitask environment setup, ICRT\nsignificantly outperforms current state-of-the-art next-token prediction models\nin robotics on generalizing to unseen tasks. Code, checkpoints and data are\navailable on https://icrt.dev/\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15980v1.pdf",
        "similarity": 0.4591168018799277,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Skills Regularized Task Decomposition for Multi-task Offline\n  Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2408.15593v1",
        "new_summary": "  Reinforcement learning (RL) with diverse offline datasets can have the\nadvantage of leveraging the relation of multiple tasks and the common skills\nlearned across those tasks, hence allowing us to deal with real-world complex\nproblems efficiently in a data-driven way. In offline RL where only offline\ndata is used and online interaction with the environment is restricted, it is\nyet difficult to achieve the optimal policy for multiple tasks, especially when\nthe data quality varies for the tasks. In this paper, we present a skill-based\nmulti-task RL technique on heterogeneous datasets that are generated by\nbehavior policies of different quality. To learn the shareable knowledge across\nthose datasets effectively, we employ a task decomposition method for which\ncommon skills are jointly learned and used as guidance to reformulate a task in\nshared and achievable subtasks. In this joint learning, we use Wasserstein\nauto-encoder (WAE) to represent both skills and tasks on the same latent space\nand use the quality-weighted loss as a regularization term to induce tasks to\nbe decomposed into subtasks that are more consistent with high-quality skills\nthan others. To improve the performance of offline RL agents learned on the\nlatent space, we also augment datasets with imaginary trajectories relevant to\nhigh-quality skills for each task. Through experiments, we show that our\nmulti-task offline RL approach is robust to the mixed configurations of\ndifferent-quality datasets and it outperforms other state-of-the-art algorithms\nfor several robotic manipulation tasks and drone navigation tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15593v1.pdf",
        "similarity": 0.45774616005340446,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video\n  Generative Model",
        "new_link": "http://arxiv.org/abs/2408.15868v1",
        "new_summary": "  Autonomous driving training requires a diverse range of datasets encompassing\nvarious traffic conditions, weather scenarios, and road types. Traditional data\naugmentation methods often struggle to generate datasets that represent rare\noccurrences. To address this challenge, we propose GenDDS, a novel approach for\ngenerating driving scenarios generation by leveraging the capabilities of\nStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology\ninvolves the use of descriptive prompts to guide the synthesis process, aimed\nat producing realistic and diverse driving scenarios. With the power of the\nlatest computer vision techniques, such as ControlNet and Hotshot-XL, we have\nbuilt a complete pipeline for video generation together with SDXL. We employ\nthe KITTI dataset, which includes real-world driving videos, to train the\nmodel. Through a series of experiments, we demonstrate that our model can\ngenerate high-quality driving videos that closely replicate the complexity and\nvariability of real-world driving scenarios. This research contributes to the\ndevelopment of sophisticated training data for autonomous driving systems and\nopens new avenues for creating virtual environments for simulation and\nvalidation purposes.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15868v1.pdf",
        "similarity": 0.45661165379852825,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Thoughtseeds: Evolutionary Priors, Nested Markov Blankets, and the\n  Emergence of Embodied Cognition",
        "new_link": "http://arxiv.org/abs/2408.15982v1",
        "new_summary": "  The emergence of cognition requires a framework that bridges evolutionary\nprinciples with neurocomputational mechanisms. This paper introduces the\n\"thoughtseed\" framework, proposing that cognition arises from the dynamic\ninteraction of self-organizing units of embodied knowledge called\n\"thoughtseeds.\" We leverage evolutionary theory, \"neuronal packets,\" and the\n\"Inner Screen\" hypothesis within Free Energy Principle, and propose a\nfour-level hierarchical model of the cognitive agent's internal states:\nNeuronal Packet Domains (NPDs), Knowledge Domains (KDs), thoughtseeds network,\nand meta-cognition. The dynamic interplay within this hierarchy, mediated by\nnested Markov blankets and reciprocal message passing, facilitates the\nemergence of thoughtseeds as coherent patterns of activity that guide\nperception, action, and learning. The framework further explores the role of\nthe organism's Umwelt and the principles of active inference, especially the\ngenerative model at each nested level, in shaping the selection and activation\nof thoughtseeds, leading to adaptive behavior through surprise minimization.\nThe \"Inner Screen\" is posited as the locus of conscious experience, where the\ncontent of the dominant thoughtseed is projected, maintaining a unitary\nconscious experience. Active thoughtseeds are proposed as the fundamental units\nof thought that contribute to the \"content of consciousness.\" We present a\nmathematical framework grounded in active inference and dynamical systems\ntheory. The thoughtseed framework represents an initial but promising step\ntowards a novel, biologically-grounded model for understanding the organizing\nprinciples and emergence of embodied cognition, offering a unified account of\ncognitive phenomena, from basic physiological regulation to higher-order\nthought processes, and potentially bridge neuroscience and contemplative\ntraditions.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15982v1.pdf",
        "similarity": 0.4527627502239058,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Nexus: Specialization meets Adaptability for Efficiently Training\n  Mixture of Experts",
        "new_link": "http://arxiv.org/abs/2408.15901v1",
        "new_summary": "  Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15901v1.pdf",
        "similarity": 0.4421051453177624,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "chemtrain: Learning Deep Potential Models via Automatic Differentiation\n  and Statistical Physics",
        "new_link": "http://arxiv.org/abs/2408.15852v1",
        "new_summary": "  Neural Networks (NNs) are promising models for refining the accuracy of\nmolecular dynamics, potentially opening up new fields of application. Typically\ntrained bottom-up, atomistic NN potential models can reach first-principle\naccuracy, while coarse-grained implicit solvent NN potentials surpass classical\ncontinuum solvent models. However, overcoming the limitations of costly\ngeneration of accurate reference data and data inefficiency of common bottom-up\ntraining demands efficient incorporation of data from many sources. This paper\nintroduces the framework chemtrain to learn sophisticated NN potential models\nthrough customizable training routines and advanced training algorithms. These\nroutines can combine multiple top-down and bottom-up algorithms, e.g., to\nincorporate both experimental and simulation data or pre-train potentials with\nless costly algorithms. chemtrain provides an object-oriented high-level\ninterface to simplify the creation of custom routines. On the lower level,\nchemtrain relies on JAX to compute gradients and scale the computations to use\navailable resources. We demonstrate the simplicity and importance of combining\nmultiple algorithms in the examples of parametrizing an all-atomistic model of\ntitanium and a coarse-grained implicit solvent model of alanine dipeptide.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15852v1.pdf",
        "similarity": 0.42865616358227243,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Towards Graph Prompt Learning: A Survey and Beyond",
        "new_link": "http://arxiv.org/abs/2408.14520v1",
        "new_summary": "  Large-scale \"pre-train and prompt learning\" paradigms have demonstrated\nremarkable adaptability, enabling broad applications across diverse domains\nsuch as question answering, image recognition, and multimodal retrieval. This\napproach fully leverages the potential of large-scale pre-trained models,\nreducing downstream data requirements and computational costs while enhancing\nmodel applicability across various tasks. Graphs, as versatile data structures\nthat capture relationships between entities, play pivotal roles in fields such\nas social network analysis, recommender systems, and biological graphs. Despite\nthe success of pre-train and prompt learning paradigms in Natural Language\nProcessing (NLP) and Computer Vision (CV), their application in graph domains\nremains nascent. In graph-structured data, not only do the node and edge\nfeatures often have disparate distributions, but the topological structures\nalso differ significantly. This diversity in graph data can lead to\nincompatible patterns or gaps between pre-training and fine-tuning on\ndownstream graphs. We aim to bridge this gap by summarizing methods for\nalleviating these disparities. This includes exploring prompt design\nmethodologies, comparing related techniques, assessing application scenarios\nand datasets, and identifying unresolved problems and challenges. This survey\ncategorizes over 100 relevant works in this field, summarizing general design\nprinciples and the latest applications, including text-attributed graphs,\nmolecules, proteins, and recommendation systems. Through this extensive review,\nwe provide a foundational understanding of graph prompt learning, aiming to\nimpact not only the graph mining community but also the broader Artificial\nGeneral Intelligence (AGI) community.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.14520v1.pdf",
        "similarity": 0.425492446279629,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-26"
    },
    {
        "new_title": "Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones",
        "new_link": "http://arxiv.org/abs/2408.15899v1",
        "new_summary": "  Gen-Swarms is an innovative method that leverages and combines the\ncapabilities of deep generative models with reactive navigation algorithms to\nautomate the creation of drone shows. Advancements in deep generative models,\nparticularly diffusion models, have demonstrated remarkable effectiveness in\ngenerating high-quality 2D images. Building on this success, various works have\nextended diffusion models to 3D point cloud generation. In contrast,\nalternative generative models such as flow matching have been proposed,\noffering a simple and intuitive transition from noise to meaningful outputs.\nHowever, the application of flow matching models to 3D point cloud generation\nremains largely unexplored. Gen-Swarms adapts these models to automatically\ngenerate drone shows. Existing 3D point cloud generative models create point\ntrajectories which are impractical for drone swarms. In contrast, our method\nnot only generates accurate 3D shapes but also guides the swarm motion,\nproducing smooth trajectories and accounting for potential collisions through a\nreactive navigation algorithm incorporated into the sampling process. For\nexample, when given a text category like Airplane, Gen-Swarms can rapidly and\ncontinuously generate numerous variations of 3D airplane shapes. Our\nexperiments demonstrate that this approach is particularly well-suited for\ndrone shows, providing feasible trajectories, creating representative final\nshapes, and significantly enhancing the overall performance of drone show\ngeneration.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15899v1.pdf",
        "similarity": 0.4245417253596862,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Adaptive Traffic Signal Control Using Reinforcement Learning",
        "new_link": "http://arxiv.org/abs/2408.15751v1",
        "new_summary": "  Traffic demand is continuously increasing, leading to significant congestion\nissues in major urban areas. Constructing new infrastructure is a potential\nsolution but presents a substantial financial burden on national economies. An\nalternative approach involves optimizing existing traffic networks through the\ndynamic control of traffic signals at intersections. Recent advancements in\nReinforcement Learning (RL) techniques have demonstrated their capability to\naddress the complexities associated with traffic congestion. In this paper, we\npropose a solution to traffic congestion using reinforcement learning. We\ndefine the state as a scalar representing the queue length, demonstrating that\nthe algorithm can effectively learn from this simplified state representation.\nThis approach can potentially reduce deployment costs by minimizing the number\nof sensors required at intersections. We have developed two RL algorithms: a\nturn-based agent, which prioritizes traffic signals for the intersection side\nwith higher traffic, and a time-based agent, which adheres to a fixed phase\ncycle, adjusting the phase duration based on traffic conditions. To assess the\nperformance of these algorithms, we designed four distinct traffic scenarios\nand computed seven evaluation metrics for each. Simulation results indicate\nthat both algorithms outperform conventional traffic signal control systems.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15751v1.pdf",
        "similarity": 0.42289819682033497,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Exploring Selective Layer Fine-Tuning in Federated Learning",
        "new_link": "http://arxiv.org/abs/2408.15600v1",
        "new_summary": "  Federated learning (FL) has emerged as a promising paradigm for fine-tuning\nfoundation models using distributed data in a privacy-preserving manner. Under\nlimited computational resources, clients often find it more practical to\nfine-tune a selected subset of layers, rather than the entire model, based on\ntheir task-specific data. In this study, we provide a thorough theoretical\nexploration of selective layer fine-tuning in FL, emphasizing a flexible\napproach that allows the clients to adjust their selected layers according to\ntheir local data and resources. We theoretically demonstrate that the layer\nselection strategy has a significant impact on model convergence in two\ncritical aspects: the importance of selected layers and the heterogeneous\nchoices across clients. Drawing from these insights, we further propose a\nstrategic layer selection method that utilizes local gradients and regulates\nlayer selections across clients. The extensive experiments on both image and\ntext datasets demonstrate the effectiveness of the proposed strategy compared\nwith several baselines, highlighting its advances in identifying critical\nlayers that adapt to the client heterogeneity and training dynamics in FL.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15600v1.pdf",
        "similarity": 0.42224141248075997,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "A Neural Material Point Method for Particle-based Simulations",
        "new_link": "http://arxiv.org/abs/2408.15753v1",
        "new_summary": "  Mesh-free Lagrangian methods are widely used for simulating fluids, solids,\nand their complex interactions due to their ability to handle large\ndeformations and topological changes. These physics simulators, however,\nrequire substantial computational resources for accurate simulations. To\naddress these issues, deep learning emulators promise faster and scalable\nsimulations, yet they often remain expensive and difficult to train, limiting\ntheir practical use. Inspired by the Material Point Method (MPM), we present\nNeuralMPM, a neural emulation framework for particle-based simulations.\nNeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes\nupdates on grid nodes using image-to-image neural networks, and interpolates\nback to the particles. Similarly to MPM, NeuralMPM benefits from the regular\nvoxelized representation to simplify the computation of the state dynamics,\nwhile avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the\nadvantages of NeuralMPM on several datasets, including fluid dynamics and\nfluid-solid interactions. Compared to existing methods, NeuralMPM reduces\ntraining times from days to hours, while achieving comparable or superior\nlong-term accuracy, making it a promising approach for practical forward and\ninverse problems. A project page is available at https://neuralmpm.isach.be\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15753v1.pdf",
        "similarity": 0.4013413853841878,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Auxiliary Input in Training: Incorporating Catheter Features into Deep\n  Learning Models for ECG-Free Dynamic Coronary Roadmapping",
        "new_link": "http://arxiv.org/abs/2408.15947v1",
        "new_summary": "  Dynamic coronary roadmapping is a technology that overlays the vessel maps\n(the \"roadmap\") extracted from an offline image sequence of X-ray angiography\nonto a live stream of X-ray fluoroscopy in real-time. It aims to offer\nnavigational guidance for interventional surgeries without the need for\nrepeated contrast agent injections, thereby reducing the risks associated with\nradiation exposure and kidney failure. The precision of the roadmaps is\ncontingent upon the accurate alignment of angiographic and fluoroscopic images\nbased on their cardiac phases, as well as precise catheter tip tracking. The\nformer ensures the selection of a roadmap that closely matches the vessel shape\nin the current frame, while the latter uses catheter tips as reference points\nto adjust for translational motion between the roadmap and the present vessel\ntree. Training deep learning models for both tasks is challenging and\nunderexplored. However, incorporating catheter features into the models could\noffer substantial benefits, given humans heavily rely on catheters to complete\nthe tasks. To this end, we introduce a simple but effective method, auxiliary\ninput in training (AIT), and demonstrate that it enhances model performance\nacross both tasks, outperforming baseline methods in knowledge incorporation\nand transfer learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15947v1.pdf",
        "similarity": 0.40018742250599637,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation",
        "new_link": "http://arxiv.org/abs/2408.15881v1",
        "new_summary": "  We introduce LLaVA-MoD, a novel framework designed to enable the efficient\ntraining of small-scale Multimodal Language Models (s-MLLM) by distilling\nknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental\nchallenges in MLLM distillation. First, we optimize the network structure of\ns-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the\nlanguage model, striking a balance between computational efficiency and model\nexpressiveness. Second, we propose a progressive knowledge transfer strategy to\nensure comprehensive knowledge migration. This strategy begins with mimic\ndistillation, where we minimize the Kullback-Leibler (KL) divergence between\noutput distributions to enable the student model to emulate the teacher\nnetwork's understanding. Following this, we introduce preference distillation\nvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLM\nas the reference model. During this phase, the s-MLLM's ability to discriminate\nbetween superior and inferior examples is significantly enhanced beyond l-MLLM,\nleading to a better student that surpasses its teacher, particularly in\nhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD\noutperforms existing models across various multimodal benchmarks while\nmaintaining a minimal number of activated parameters and low computational\ncosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses\nQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of\nthe training data and 23% trainable parameters. These results underscore\nLLaVA-MoD's ability to effectively distill comprehensive knowledge from its\nteacher model, paving the way for the development of more efficient MLLMs. The\ncode will be available on: https://github.com/shufangxun/LLaVA-MoD.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15881v1.pdf",
        "similarity": 0.39315428968297345,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Unleashing the Temporal-Spatial Reasoning Capacity of GPT for\n  Training-Free Audio and Language Referenced Video Object Segmentation",
        "new_link": "http://arxiv.org/abs/2408.15876v1",
        "new_summary": "  In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2)\npipeline to explore the training-free paradigm for audio and\nlanguage-referenced video object segmentation, namely AVS and RVOS tasks. The\nintuitive solution leverages GroundingDINO to identify the target object from a\nsingle frame and SAM 2 to segment the identified object throughout the video,\nwhich is less robust to spatiotemporal variations due to a lack of video\ncontext exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel\nGPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform\ntwo-step temporal-spatial reasoning for sequentially selecting pivot frames and\npivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.\nWithin GPT-PS, two task-specific Chain-of-Thought prompts are designed to\nunleash GPT's temporal-spatial reasoning capacity by guiding GPT to make\nselections based on a comprehensive understanding of video and reference\ninformation. Furthermore, we propose a Language-Binded Reference Unification\n(LBRU) module to convert audio signals into language-formatted references,\nthereby unifying the formats of AVS and RVOS tasks in the same pipeline.\nExtensive experiments on both tasks show that our training-free AL-Ref-SAM 2\npipeline achieves performances comparable to or even better than\nfully-supervised fine-tuning methods. The code is available at:\nhttps://github.com/appletea233/AL-Ref-SAM2.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15876v1.pdf",
        "similarity": 0.39064922056409773,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Comparison of Model Predictive Control and Proximal Policy Optimization\n  for a 1-DOF Helicopter System",
        "new_link": "http://arxiv.org/abs/2408.15633v1",
        "new_summary": "  This study conducts a comparative analysis of Model Predictive Control (MPC)\nand Proximal Policy Optimization (PPO), a Deep Reinforcement Learning (DRL)\nalgorithm, applied to a 1-Degree of Freedom (DOF) Quanser Aero 2 system.\nClassical control techniques such as MPC and Linear Quadratic Regulator (LQR)\nare widely used due to their theoretical foundation and practical\neffectiveness. However, with advancements in computational techniques and\nmachine learning, DRL approaches like PPO have gained traction in solving\noptimal control problems through environment interaction. This paper\nsystematically evaluates the dynamic response characteristics of PPO and MPC,\ncomparing their performance, computational resource consumption, and\nimplementation complexity. Experimental results show that while LQR achieves\nthe best steady-state accuracy, PPO excels in rise-time and adaptability,\nmaking it a promising approach for applications requiring rapid response and\nadaptability. Additionally, we have established a baseline for future\nRL-related research on this specific testbed. We also discuss the strengths and\nlimitations of each control strategy, providing recommendations for selecting\nappropriate controllers for real-world scenarios.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15633v1.pdf",
        "similarity": 0.3902258257778007,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts",
        "new_link": "http://arxiv.org/abs/2408.15664v1",
        "new_summary": "  For Mixture-of-Experts (MoE) models, an unbalanced expert load will lead to\nrouting collapse or increased computational overhead. Existing methods commonly\nemploy an auxiliary loss to encourage load balance, but a large auxiliary loss\nwill introduce non-negligible interference gradients into training and thus\nimpair the model performance. In order to control load balance while not\nproducing undesired gradients during training, we propose Loss-Free Balancing,\nfeatured by an auxiliary-loss-free load balancing strategy. To be specific,\nbefore the top-K routing decision, Loss-Free Balancing will first apply an\nexpert-wise bias to the routing scores of each expert. By dynamically updating\nthe bias of each expert according to its recent load, Loss-Free Balancing can\nconsistently maintain a balanced distribution of expert load. In addition,\nsince Loss-Free Balancing does not produce any interference gradients, it also\nelevates the upper bound of model performance gained from MoE training. We\nvalidate the performance of Loss-Free Balancing on MoE models with up to 3B\nparameters trained on up to 200B tokens. Experimental results show that\nLoss-Free Balancing achieves both better performance and better load balance\ncompared with traditional auxiliary-loss-controlled load balancing strategies.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15664v1.pdf",
        "similarity": 0.3876666996163337,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "ULLME: A Unified Framework for Large Language Model Embeddings with\n  Generation-Augmented Learning",
        "new_link": "http://arxiv.org/abs/2408.03402v1",
        "new_summary": "  Large Language Models (LLMs) excel in various natural language processing\ntasks, but leveraging them for dense passage embedding remains challenging.\nThis is due to their causal attention mechanism and the misalignment between\ntheir pre-training objectives and the text ranking tasks. Despite some recent\nefforts to address these issues, existing frameworks for LLM-based text\nembeddings have been limited by their support for only a limited range of LLM\narchitectures and fine-tuning strategies, limiting their practical application\nand versatility. In this work, we introduce the Unified framework for Large\nLanguage Model Embedding (ULLME), a flexible, plug-and-play implementation that\nenables bidirectional attention across various LLMs and supports a range of\nfine-tuning strategies. We also propose Generation-augmented Representation\nLearning (GRL), a novel fine-tuning method to boost LLMs for text embedding\ntasks. GRL enforces consistency between representation-based and\ngeneration-based relevance scores, leveraging LLMs' powerful generative\nabilities for learning passage embeddings. To showcase our framework's\nflexibility and effectiveness, we release three pre-trained models from ULLME\nwith different backbone architectures, ranging from 1.5B to 8B parameters, all\nof which demonstrate strong performance on the Massive Text Embedding\nBenchmark. Our framework is publicly available at:\nhttps://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found\nat https://rb.gy/ws1ile.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.03402v1.pdf",
        "similarity": 0.3873778125238628,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-06"
    },
    {
        "new_title": "Bayesian optimization of atomic structures with prior probabilities from\n  universal interatomic potentials",
        "new_link": "http://arxiv.org/abs/2408.15590v1",
        "new_summary": "  The optimization of atomic structures plays a pivotal role in understanding\nand designing materials with desired properties. However, conventional methods\noften struggle with the formidable task of navigating the vast potential energy\nsurface, especially in high-dimensional spaces with numerous local minima.\nRecent advancements in machine learning-driven surrogate models offer a\npromising avenue for alleviating this computational burden. In this study, we\npropose a novel approach that combines the strengths of universal machine\nlearning potentials with a Bayesian approach of the GOFEE/BEACON framework. By\nleveraging the comprehensive chemical knowledge encoded in pretrained universal\nmachine learning potentials as a prior estimate of energy and forces, we enable\nthe Gaussian process to focus solely on capturing the intricate nuances of the\npotential energy surface. We demonstrate the efficacy of our approach through\ncomparative analyses across diverse systems, including periodic bulk materials,\nsurface structures, and a cluster.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15590v1.pdf",
        "similarity": 0.3865114843772033,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Conan-embedding: General Text Embedding with More and Better Negative\n  Samples",
        "new_link": "http://arxiv.org/abs/2408.15710v1",
        "new_summary": "  With the growing popularity of RAG, the capabilities of embedding models are\ngaining increasing attention. Embedding models are primarily trained through\ncontrastive loss learning, with negative examples being a key component.\nPrevious work has proposed various hard negative mining strategies, but these\nstrategies are typically employed as preprocessing steps. In this paper, we\npropose the conan-embedding model, which maximizes the utilization of more and\nhigher-quality negative examples. Specifically, since the model's ability to\nhandle preprocessed negative examples evolves during training, we propose\ndynamic hard negative mining method to expose the model to more challenging\nnegative examples throughout the training process. Secondly, contrastive\nlearning requires as many negative examples as possible but is limited by GPU\nmemory constraints. Therefore, we use a Cross-GPU balancing Loss to provide\nmore negative examples for embedding training and balance the batch size across\nmultiple tasks. Moreover, we also discovered that the prompt-response pairs\nfrom LLMs can be used for embedding training. Our approach effectively enhances\nthe capabilities of embedding models, currently ranking first on the Chinese\nleaderboard of Massive text embedding benchmark\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15710v1.pdf",
        "similarity": 0.38614317674153553,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Autoregressive model path dependence near Ising criticality",
        "new_link": "http://arxiv.org/abs/2408.15715v1",
        "new_summary": "  Autoregressive models are a class of generative model that probabilistically\npredict the next output of a sequence based on previous inputs. The\nautoregressive sequence is by definition one-dimensional (1D), which is natural\nfor language tasks and hence an important component of modern architectures\nlike recurrent neural networks (RNNs) and transformers. However, when language\nmodels are used to predict outputs on physical systems that are not\nintrinsically 1D, the question arises of which choice of autoregressive\nsequence -- if any -- is optimal. In this paper, we study the reconstruction of\ncritical correlations in the two-dimensional (2D) Ising model, using RNNs and\ntransformers trained on binary spin data obtained near the thermal phase\ntransition. We compare the training performance for a number of different 1D\nautoregressive sequences imposed on finite-size 2D lattices. We find that paths\nwith long 1D segments are more efficient at training the autoregressive models\ncompared to space-filling curves that better preserve the 2D locality. Our\nresults illustrate the potential importance in choosing the optimal\nautoregressive sequence ordering when training modern language models for tasks\nin physics.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15715v1.pdf",
        "similarity": 0.3859706845594123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An\n  Exhaustive Review of Technologies, Research, Best Practices, Applied Research\n  Challenges and Opportunities",
        "new_link": "http://arxiv.org/abs/2408.13296v1",
        "new_summary": "  This report examines the fine-tuning of Large Language Models (LLMs),\nintegrating theoretical insights with practical applications. It outlines the\nhistorical evolution of LLMs from traditional Natural Language Processing (NLP)\nmodels to their pivotal role in AI. A comparison of fine-tuning methodologies,\nincluding supervised, unsupervised, and instruction-based approaches,\nhighlights their applicability to different tasks. The report introduces a\nstructured seven-stage pipeline for fine-tuning LLMs, spanning data\npreparation, model initialization, hyperparameter tuning, and model deployment.\nEmphasis is placed on managing imbalanced datasets and optimization techniques.\nParameter-efficient methods like Low-Rank Adaptation (LoRA) and Half\nFine-Tuning are explored for balancing computational efficiency with\nperformance. Advanced techniques such as memory fine-tuning, Mixture of Experts\n(MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized\nnetworks and multi-agent collaboration. The report also examines novel\napproaches like Proximal Policy Optimization (PPO) and Direct Preference\nOptimization (DPO), which align LLMs with human preferences, alongside pruning\nand routing optimizations to improve efficiency. Further sections cover\nvalidation frameworks, post-deployment monitoring, and inference optimization,\nwith attention to deploying LLMs on distributed and cloud-based platforms.\nEmerging areas such as multimodal LLMs, fine-tuning for audio and speech, and\nchallenges related to scalability, privacy, and accountability are also\naddressed. This report offers actionable insights for researchers and\npractitioners navigating LLM fine-tuning in an evolving landscape.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.13296v1.pdf",
        "similarity": 0.38235602668540497,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-23"
    },
    {
        "new_title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language\n  Models",
        "new_link": "http://arxiv.org/abs/2408.15915v1",
        "new_summary": "  The cultivation of expertise for large language models (LLMs) to solve tasks\nof specific areas often requires special-purpose tuning with calibrated\nbehaviors on the expected stable outputs. To avoid huge cost brought by manual\npreparation of instruction datasets and training resources up to hundreds of\nhours, the exploitation of open knowledge including a wealth of low rank\nadaptation (LoRA) models and instruction datasets serves as a good starting\npoint. However, existing methods on model and data selection focus on the\nperformance of general-purpose capabilities while neglecting the knowledge gap\nexposed in domain-specific deployment. In the present study, we propose to\nbridge such gap by introducing few human-annotated samples (i.e., K-shot) for\nadvancing task expertise of LLMs with open knowledge. Specifically, we develop\nan efficient and scalable pipeline to cost-efficiently produce task experts\nwhere K-shot data intervene in selecting the most promising expert candidates\nand the task-relevant instructions. A mixture-of-expert (MoE) system is built\nto make the best use of individual-yet-complementary knowledge between multiple\nexperts. We unveil the two keys to the success of a MoE system, 1) the abidance\nby K-shot, and 2) the insistence on diversity. For the former, we ensure that\nmodels that truly possess problem-solving abilities on K-shot are selected\nrather than those blind guessers. Besides, during data selection, instructions\nthat share task-relevant contexts with K-shot are prioritized. For the latter,\nwe highlight the diversity of constituting experts and that of the fine-tuning\ninstructions throughout the model and data selection process. Extensive\nexperimental results confirm the superiority of our approach over existing\nmethods on utilization of open knowledge across various tasks. Codes and models\nwill be released later.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15915v1.pdf",
        "similarity": 0.3818268473862558,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Making Long-Context Language Models Better Multi-Hop Reasoners",
        "new_link": "http://arxiv.org/abs/2408.03246v1",
        "new_summary": "  Recent advancements in long-context modeling have enhanced language models\n(LMs) for complex tasks across multiple NLP applications. Despite this\nprogress, we find that these models struggle with multi-hop reasoning and\nexhibit decreased performance in the presence of noisy contexts. In this paper,\nwe introduce Reasoning with Attributions, a novel approach that prompts LMs to\nsupply attributions for each assertion during their reasoning. We validate our\napproach through experiments on three multi-hop datasets, employing both\nproprietary and open-source models, and demonstrate its efficacy and\nresilience. Furthermore, we explore methods to augment reasoning capabilities\nvia fine-tuning and offer an attribution-annotated dataset and a specialized\ntraining strategy. Our fine-tuned model achieves competitive performance on\nmulti-hop reasoning benchmarks, closely paralleling proprietary LMs such as\nChatGPT and Claude-instant.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.03246v1.pdf",
        "similarity": 0.3795495790668628,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-06"
    },
    {
        "new_title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel\n  Chatbot Use Case",
        "new_link": "http://arxiv.org/abs/2408.03562v1",
        "new_summary": "  This research compares large language model (LLM) fine-tuning methods,\nincluding Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning\n(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally\ncompared LLM evaluation methods including End to End (E2E) benchmark method of\n\"Golden Answers\", traditional natural language processing (NLP) metrics, RAG\nAssessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,\nusing the travel chatbot use case. The travel dataset was sourced from the the\nReddit API by requesting posts from travel-related subreddits to get\ntravel-related conversation prompts and personalized travel experiences, and\naugmented for each fine-tuning method. We used two pretrained LLMs utilized for\nfine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to\nthe two pretrained models. The inferences from these models are extensively\nevaluated against the aforementioned metrics. The best model according to human\nevaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a\nReinforcement Learning from Human Feedback (RLHF) training pipeline, and\nultimately was evaluated as the best model. Our main findings are that: 1)\nquantitative and Ragas metrics do not align with human evaluation, 2) Open AI\nGPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep\nhumans in the loop for evaluation because, 4) traditional NLP metrics\ninsufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms\nQLoRA, but still needs postprocessing, 7) RLHF improves model performance\nsignificantly. Next steps include improving data quality, increasing data\nquantity, exploring RAG methods, and focusing data collection on a specific\ncity, which would improve data quality by narrowing the focus, while creating a\nuseful product.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.03562v1.pdf",
        "similarity": 0.3773168268161782,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-07"
    },
    {
        "new_title": "Retrieval-Augmented Instruction Tuning for Automated Process Engineering\n  Calculations : A Tool-Chaining Problem-Solving Framework with Attributable\n  Reflection",
        "new_link": "http://arxiv.org/abs/2408.15866v1",
        "new_summary": "  The current technology landscape lacks a foundational AI model for solving\nprocess engineering calculations. In this work, we introduce a novel autonomous\nagent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to\nenhance open, customizable small code language models (SLMs) for these\ncalculations. By combining instruction tuned code SLMs with Retrieval-Augmented\nCode Generation (RACG) using external tools, the agent generates, debugs, and\noptimizes code from natural language specifications. Our approach addresses the\nlimitations of the current lack of a foundational AI model for specialized\nprocess engineering tasks and offers benefits of explainability, knowledge\nediting, and cost-effectiveness. Additionally, we curate custom datasets of\nchemical and process engineering problems and solutions to overcome data\nscarcity. Experimental results show that our framework matches the performance\nof large-scale proprietary models on benchmark datasets, proving its\neffectiveness and usability.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15866v1.pdf",
        "similarity": 0.3769269283047341,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Structural Optimization of Lightweight Bipedal Robot via SERL",
        "new_link": "http://arxiv.org/abs/2408.15632v1",
        "new_summary": "  Designing a bipedal robot is a complex and challenging task, especially when\ndealing with a multitude of structural parameters. Traditional design methods\noften rely on human intuition and experience. However, such approaches are\ntime-consuming, labor-intensive, lack theoretical guidance and hard to obtain\noptimal design results within vast design spaces, thus failing to full exploit\nthe inherent performance potential of robots. In this context, this paper\nintroduces the SERL (Structure Evolution Reinforcement Learning) algorithm,\nwhich combines reinforcement learning for locomotion tasks with evolution\nalgorithms. The aim is to identify the optimal parameter combinations within a\ngiven multidimensional design space. Through the SERL algorithm, we\nsuccessfully designed a bipedal robot named Wow Orin, where the optimal leg\nlength are obtained through optimization based on body structure and motor\ntorque. We have experimentally validated the effectiveness of the SERL\nalgorithm, which is capable of optimizing the best structure within specified\ndesign space and task conditions. Additionally, to assess the performance gap\nbetween our designed robot and the current state-of-the-art robots, we compared\nWow Orin with mainstream bipedal robots Cassie and Unitree H1. A series of\nexperimental results demonstrate the Outstanding energy efficiency and\nperformance of Wow Orin, further validating the feasibility of applying the\nSERL algorithm to practical design.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15632v1.pdf",
        "similarity": 0.372205192333655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "The Role of Fibration Symmetries in Geometric Deep Learning",
        "new_link": "http://arxiv.org/abs/2408.15894v1",
        "new_summary": "  Geometric Deep Learning (GDL) unifies a broad class of machine learning\ntechniques from the perspectives of symmetries, offering a framework for\nintroducing problem-specific inductive biases like Graph Neural Networks\n(GNNs). However, the current formulation of GDL is limited to global symmetries\nthat are not often found in real-world problems. We propose to relax GDL to\nallow for local symmetries, specifically fibration symmetries in graphs, to\nleverage regularities of realistic instances. We show that GNNs apply the\ninductive bias of fibration symmetries and derive a tighter upper bound for\ntheir expressive power. Additionally, by identifying symmetries in networks, we\ncollapse network nodes, thereby increasing their computational efficiency\nduring both inference and training of deep neural networks. The mathematical\nextension introduced here applies beyond graphs to manifolds, bundles, and\ngrids for the development of models with inductive biases induced by local\nsymmetries that can lead to better generalization.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15894v1.pdf",
        "similarity": 0.37087689484891195,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Modeling and Analyzing the Influence of Non-Item Pages on Sequential\n  Next-Item Prediction",
        "new_link": "http://arxiv.org/abs/2408.15953v1",
        "new_summary": "  Analyzing the sequence of historical interactions between users and items,\nsequential recommendation models learn user intent and make predictions about\nthe next item of interest. Next to these item interactions, most systems also\nhave interactions with pages not related to specific items, for example\nnavigation pages, account pages, and pages for a specific category, which may\nprovide additional insights into the user's interests. However, while there are\nseveral approaches to integrate additional information about items and users,\nthe topic of integrating non-item pages has been less explored. We use the\nhypotheses testing framework HypTrails to show that there is indeed a\nrelationship between these non-item pages and the items of interest and fill\nthis gap by proposing various approaches of representing non-item pages (e.g,\nbased on their content) to use them as an additional information source for the\ntask of sequential next-item prediction.\n  We create a synthetic dataset with non-item pages highly related to the\nsubsequent item to show that the models are generally capable of learning from\nthese interactions, and subsequently evaluate the improvements gained by\nincluding non-item pages in two real-world datasets.\n  We adapt eight popular sequential recommender models, covering CNN-, RNN- and\ntransformer-based architectures, to integrate non-item pages and investigate\nthe capabilities of these models to leverage their information for next item\nprediction. We also analyze their behavior on noisy data and compare different\nitem representation strategies.\n  Our results show that non-item pages are a valuable source of information,\nbut representing such a page well is the key to successfully leverage them. The\ninclusion of non-item pages can increase the performance for next-item\nprediction in all examined model architectures with a varying degree.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15953v1.pdf",
        "similarity": 0.3697300141241815,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Decentralized LLM Inference over Edge Networks with Energy Harvesting",
        "new_link": "http://arxiv.org/abs/2408.15907v1",
        "new_summary": "  Large language models have significantly transformed multiple fields with\ntheir exceptional performance in natural language tasks, but their deployment\nin resource-constrained environments like edge networks presents an ongoing\nchallenge. Decentralized techniques for inference have emerged, distributing\nthe model blocks among multiple devices to improve flexibility and cost\neffectiveness. However, energy limitations remain a significant concern for\nedge devices. We propose a sustainable model for collaborative inference on\ninterconnected, battery-powered edge devices with energy harvesting. A\nsemi-Markov model is developed to describe the states of the devices,\nconsidering processing parameters and average green energy arrivals. This\ninforms the design of scheduling algorithms that aim to minimize device\ndowntimes and maximize network throughput. Through empirical evaluations and\nsimulated runs, we validate the effectiveness of our approach, paving the way\nfor energy-efficient decentralized inference over edge networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15907v1.pdf",
        "similarity": 0.3683631609636312,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Nonlocal Attention Operator: Materializing Hidden Knowledge Towards\n  Interpretable Physics Discovery",
        "new_link": "http://arxiv.org/abs/2408.07307v1",
        "new_summary": "  Despite the recent popularity of attention-based neural architectures in core\nAI fields like natural language processing (NLP) and computer vision (CV),\ntheir potential in modeling complex physical systems remains under-explored.\nLearning problems in physical systems are often characterized as discovering\noperators that map between function spaces based on a few instances of function\npairs. This task frequently presents a severely ill-posed PDE inverse problem.\nIn this work, we propose a novel neural operator architecture based on the\nattention mechanism, which we coin Nonlocal Attention Operator (NAO), and\nexplore its capability towards developing a foundation physical model. In\nparticular, we show that the attention mechanism is equivalent to a double\nintegral operator that enables nonlocal interactions among spatial tokens, with\na data-dependent kernel characterizing the inverse mapping from data to the\nhidden parameter field of the underlying operator. As such, the attention\nmechanism extracts global prior information from training data generated by\nmultiple systems, and suggests the exploratory space in the form of a nonlinear\nkernel map. Consequently, NAO can address ill-posedness and rank deficiency in\ninverse PDE problems by encoding regularization and achieving generalizability.\nWe empirically demonstrate the advantages of NAO over baseline neural models in\nterms of generalizability to unseen data resolutions and system states. Our\nwork not only suggests a novel neural operator architecture for learning\ninterpretable foundation models of physical systems, but also offers a new\nperspective towards understanding the attention mechanism.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.07307v1.pdf",
        "similarity": 0.3676591293186759,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-14"
    },
    {
        "new_title": "VFLIP: A Backdoor Defense for Vertical Federated Learning via\n  Identification and Purification",
        "new_link": "http://arxiv.org/abs/2408.15591v1",
        "new_summary": "  Vertical Federated Learning (VFL) focuses on handling vertically partitioned\ndata over FL participants. Recent studies have discovered a significant\nvulnerability in VFL to backdoor attacks which specifically target the distinct\ncharacteristics of VFL. Therefore, these attacks may neutralize existing\ndefense mechanisms designed primarily for Horizontal Federated Learning (HFL)\nand deep neural networks. In this paper, we present the first backdoor defense,\ncalled VFLIP, specialized for VFL. VFLIP employs the identification and\npurification techniques that operate at the inference stage, consequently\nimproving the robustness against backdoor attacks to a great extent. VFLIP\nfirst identifies backdoor-triggered embeddings by adopting a participant-wise\nanomaly detection approach. Subsequently, VFLIP conducts purification which\nremoves the embeddings identified as malicious and reconstructs all the\nembeddings based on the remaining embeddings. We conduct extensive experiments\non CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate\nthat VFLIP can effectively mitigate backdoor attacks in VFL.\nhttps://github.com/blingcho/VFLIP-esorics24\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15591v1.pdf",
        "similarity": 0.3622525409317285,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Investigating a Benchmark for Training-set free Evaluation of Linguistic\n  Capabilities in Machine Reading Comprehension",
        "new_link": "http://arxiv.org/abs/2408.05023v1",
        "new_summary": "  Performance of NLP systems is typically evaluated by collecting a large-scale\ndataset by means of crowd-sourcing to train a data-driven model and evaluate it\non a held-out portion of the data. This approach has been shown to suffer from\nspurious correlations and the lack of challenging examples that represent the\ndiversity of natural language. Instead, we examine a framework for evaluating\noptimised models in training-set free setting on synthetically generated\nchallenge sets. We find that despite the simplicity of the generation method,\nthe data can compete with crowd-sourced datasets with regard to naturalness and\nlexical diversity for the purpose of evaluating the linguistic capabilities of\nMRC models. We conduct further experiments and show that state-of-the-art\nlanguage model-based MRC systems can learn to succeed on the challenge set\ncorrectly, although, without capturing the general notion of the evaluated\nphenomenon.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.05023v1.pdf",
        "similarity": 0.36187014415554747,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-09"
    },
    {
        "new_title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
        "new_link": "http://arxiv.org/abs/2408.15778v1",
        "new_summary": "  Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15778v1.pdf",
        "similarity": 0.35729875027555774,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Great Memory, Shallow Reasoning: Limits of $k$NN-LMs",
        "new_link": "http://arxiv.org/abs/2408.11815v1",
        "new_summary": "  $K$-nearest neighbor language models ($k$NN-LMs), which integrate retrieval\nwith next-word prediction, have demonstrated strong performance in language\nmodeling as well as downstream NLP benchmarks. These results have led\nresearchers to argue that models trained on poor quality or outdated data could\nperform well by employing a $k$NN extension that has access to a higher-quality\ndatastore. In this work, we ask whether this improved ability to recall\ninformation really translates into downstream abilities. We extensively\nevaluate $k$NN-LMs on a diverse set of tasks, ranging from sentiment\nclassification and commonsense reasoning to multi-hop reasoning. Results show\nthat $k$NN-LMs excel at memory-intensive tasks, where utilizing the patterns in\nthe input is sufficient for determining the output, but struggle with reasoning\ntasks that require integrating multiple pieces of information to derive new\nknowledge. We further demonstrate through oracle experiments and qualitative\nanalysis that even with perfect retrieval, $k$NN-LMs still fail to determine\nthe correct answers, placing an upper bound on their reasoning performance.\nCode and datastores are released at https://github.com/GSYfate/knnlm-limits/.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11815v1.pdf",
        "similarity": 0.35608544831209626,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "Disentangled Diffusion Autoencoder for Harmonization of Multi-site\n  Neuroimaging Data",
        "new_link": "http://arxiv.org/abs/2408.15890v1",
        "new_summary": "  Combining neuroimaging datasets from multiple sites and scanners can help\nincrease statistical power and thus provide greater insight into subtle\nneuroanatomical effects. However, site-specific effects pose a challenge by\npotentially obscuring the biological signal and introducing unwanted variance.\nExisting harmonization techniques, which use statistical models to remove such\neffects, have been shown to incompletely remove site effects while also failing\nto preserve biological variability. More recently, generative models using GANs\nor autoencoder-based approaches, have been proposed for site adjustment.\nHowever, such methods are known for instability during training or blurry image\ngeneration. In recent years, diffusion models have become increasingly popular\nfor their ability to generate high-quality synthetic images. In this work, we\nintroduce the disentangled diffusion autoencoder (DDAE), a novel diffusion\nmodel designed for controlling specific aspects of an image. We apply the DDAE\nto the task of harmonizing MR images by generating high-quality site-adjusted\nimages that preserve biological variability. We use data from 7 different sites\nand demonstrate the DDAE's superiority in generating high-resolution,\nharmonized 2D MR images over previous approaches. As far as we are aware, this\nwork marks the first diffusion-based model for site adjustment of neuroimaging\ndata.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15890v1.pdf",
        "similarity": 0.3542754092428203,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation",
        "new_link": "http://arxiv.org/abs/2408.15992v1",
        "new_summary": "  Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15992v1.pdf",
        "similarity": 0.35379268844103795,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "ModalityMirror: Improving Audio Classification in Modality Heterogeneity\n  Federated Learning with Multimodal Distillation",
        "new_link": "http://arxiv.org/abs/2408.15803v1",
        "new_summary": "  Multimodal Federated Learning frequently encounters challenges of client\nmodality heterogeneity, leading to undesired performances for secondary\nmodality in multimodal learning. It is particularly prevalent in audiovisual\nlearning, with audio is often assumed to be the weaker modality in recognition\ntasks. To address this challenge, we introduce ModalityMirror to improve audio\nmodel performance by leveraging knowledge distillation from an audiovisual\nfederated learning model. ModalityMirror involves two phases: a modality-wise\nFL stage to aggregate uni-modal encoders; and a federated knowledge\ndistillation stage on multi-modality clients to train an unimodal student\nmodel. Our results demonstrate that ModalityMirror significantly improves the\naudio classification compared to the state-of-the-art FL methods such as\nHarmony, particularly in audiovisual FL facing video missing. Our approach\nunlocks the potential for exploiting the diverse modality spectrum inherent in\nmulti-modal FL.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15803v1.pdf",
        "similarity": 0.35239089256403333,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Harnessing the Intrinsic Knowledge of Pretrained Language Models for\n  Challenging Text Classification Settings",
        "new_link": "http://arxiv.org/abs/2408.15650v1",
        "new_summary": "  Text classification is crucial for applications such as sentiment analysis\nand toxic text filtering, but it still faces challenges due to the complexity\nand ambiguity of natural language. Recent advancements in deep learning,\nparticularly transformer architectures and large-scale pretraining, have\nachieved inspiring success in NLP fields. Building on these advancements, this\nthesis explores three challenging settings in text classification by leveraging\nthe intrinsic knowledge of pretrained language models (PLMs). Firstly, to\naddress the challenge of selecting misleading yet incorrect distractors for\ncloze questions, we develop models that utilize features based on\ncontextualized word representations from PLMs, achieving performance that\nrivals or surpasses human accuracy. Secondly, to enhance model generalization\nto unseen labels, we create small finetuning datasets with domain-independent\ntask label descriptions, improving model performance and robustness. Lastly, we\ntackle the sensitivity of large language models to in-context learning prompts\nby selecting effective demonstrations, focusing on misclassified examples and\nresolving model ambiguity regarding test example labels.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15650v1.pdf",
        "similarity": 0.35223683046490056,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic\n  Segmentation",
        "new_link": "http://arxiv.org/abs/2408.15657v1",
        "new_summary": "  In autonomous driving, 3D LiDAR plays a crucial role in understanding the\nvehicle's surroundings. However, the newly emerged, unannotated objects\npresents few-shot learning problem for semantic segmentation. This paper\naddresses the limitations of current few-shot semantic segmentation by\nexploiting the temporal continuity of LiDAR data. Employing a tracking model to\ngenerate pseudo-ground-truths from a sequence of LiDAR frames, our method\nsignificantly augments the dataset, enhancing the model's ability to learn on\nnovel classes. However, this approach introduces a data imbalance biased to\nnovel data that presents a new challenge of catastrophic forgetting. To\nmitigate this, we incorporate LoRA, a technique that reduces the number of\ntrainable parameters, thereby preserving the model's performance on base\nclasses while improving its adaptability to novel classes. This work represents\na significant step forward in few-shot 3D LiDAR semantic segmentation for\nautonomous driving. Our code is available at\nhttps://github.com/junbao-zhou/Track-no-forgetting.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15657v1.pdf",
        "similarity": 0.3470776629799489,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking\n  Across Diverse Vocabularies",
        "new_link": "http://arxiv.org/abs/2408.11327v1",
        "new_summary": "  Recent advancements in NLP have resulted in models with specialized\nstrengths, such as processing multimodal inputs or excelling in specific\ndomains. However, real-world tasks, like multimodal translation, often require\na combination of these strengths, such as handling both translation and image\nprocessing. While individual translation and vision models are powerful, they\ntypically lack the ability to perform both tasks in a single system. Combining\nthese models poses challenges, particularly due to differences in their\nvocabularies, which limit the effectiveness of traditional ensemble methods to\npost-generation techniques like N-best list re-ranking. In this work, we\npropose a novel zero-shot ensembling strategy that allows for the integration\nof different models during the decoding phase without the need for additional\ntraining. Our approach re-ranks beams during decoding by combining scores at\nthe word level, using heuristics to predict when a word is completed. We\ndemonstrate the effectiveness of this method in machine translation scenarios,\nshowing that it enables the generation of translations that are both speech-\nand image-aware while also improving overall translation quality\\footnote{We\nwill release the code upon paper acceptance.}.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11327v1.pdf",
        "similarity": 0.34372286360994536,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "Large-Scale Demand Prediction in Urban Rail using Multi-Graph Inductive\n  Representation Learning",
        "new_link": "http://arxiv.org/abs/2408.15619v1",
        "new_summary": "  With the expansion of cities over time, URT (Urban Rail Transit) networks\nhave also grown significantly. Demand prediction plays an important role in\nsupporting planning, scheduling, fleet management, and other operational\ndecisions. In this study, we propose an Origin-Destination (OD) demand\nprediction model called Multi-Graph Inductive Representation Learning\n(mGraphSAGE) for large-scale URT networks under operational uncertainties. Our\nmain contributions are twofold: we enhance prediction results while ensuring\nscalability for large networks by relying simultaneously on multiple graphs,\nwhere each OD pair is a node on a graph and distinct OD relationships, such as\ntemporal and spatial correlations; we show the importance of including\noperational uncertainties such as train delays and cancellations as inputs in\ndemand prediction for daily operations. The model is validated on three\ndifferent scales of the URT network in Copenhagen, Denmark. Experimental\nresults show that by leveraging information from neighboring ODs and learning\nnode representations via sampling and aggregation, mGraphSAGE is particularly\nsuitable for OD demand prediction in large-scale URT networks, outperforming\nreference machine learning methods. Furthermore, during periods with train\ncancellations and delays, the performance gap between mGraphSAGE and other\nmethods improves compared to normal operating conditions, demonstrating its\nability to leverage system reliability information for predicting OD demand\nunder uncertainty.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15619v1.pdf",
        "similarity": 0.33853395758862065,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of\n  Large Language Models",
        "new_link": "http://arxiv.org/abs/2408.04556v1",
        "new_summary": "  Large language models (LLMs) have exhibited remarkable proficiency across a\ndiverse array of natural language processing (NLP) tasks. However, adapting\nLLMs to downstream applications typically necessitates computationally\nintensive and memory-demanding fine-tuning procedures. To mitigate these\nburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a\npromising approach to tailor LLMs with minimal computational overhead. While\nPEFT methods offer substantial advantages, they do not fully address the\npervasive issue of bias propagation from pre-training data. In this work, we\nintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method\ndesigned to counteract bias inheritance. BA-LoRA incorporates three distinct\nregularization terms: (1) consistency regularizer, (2) diversity regularizer,\nand (3) singular vector decomposition regularizer. These regularizers\ncollectively aim to improve the generative models' consistency, diversity, and\ngeneralization capabilities during the fine-tuning process. Through extensive\nexperiments on a variety of natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,\nMistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of\nLoRA and its state-of-the-art variants. Moreover, our method effectively\nmitigates the deleterious effects of pre-training bias, leading to more\nreliable and robust model outputs. The code is available at\nhttps://github.com/cyp-jlu-ai/BA-LoRA.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.04556v1.pdf",
        "similarity": 0.3382885603730768,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-08"
    },
    {
        "new_title": "Distribution Backtracking Builds A Faster Convergence Trajectory for\n  One-step Diffusion Distillation",
        "new_link": "http://arxiv.org/abs/2408.15991v1",
        "new_summary": "  Accelerating the sampling speed of diffusion models remains a significant\nchallenge. Recent score distillation methods distill a heavy teacher model into\nan one-step student generator, which is optimized by calculating the difference\nbetween the two score functions on the samples generated by the student model.\nHowever, there is a score mismatch issue in the early stage of the distillation\nprocess, because existing methods mainly focus on using the endpoint of\npre-trained diffusion models as teacher models, overlooking the importance of\nthe convergence trajectory between the student generator and the teacher model.\nTo address this issue, we extend the score distillation process by introducing\nthe entire convergence trajectory of teacher models and propose Distribution\nBacktracking Distillation (DisBack) for distilling student generators. DisBask\nis composed of two stages: Degradation Recording and Distribution Backtracking.\nDegradation Recording is designed to obtain the convergence trajectory of\nteacher models, which records the degradation path from the trained teacher\nmodel to the untrained initial student generator. The degradation path\nimplicitly represents the intermediate distributions of teacher models. Then\nDistribution Backtracking trains a student generator to backtrack the\nintermediate distributions for approximating the convergence trajectory of\nteacher models. Extensive experiments show that DisBack achieves faster and\nbetter convergence than the existing distillation method and accomplishes\ncomparable generation performance. Notably, DisBack is easy to implement and\ncan be generalized to existing distillation methods to boost performance. Our\ncode is publicly available on https://github.com/SYZhang0805/DisBack.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15991v1.pdf",
        "similarity": 0.3356106866873084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free\n  Backdoor Mitigation",
        "new_link": "http://arxiv.org/abs/2408.15861v1",
        "new_summary": "  Backdoor attacks present a serious security threat to deep neuron networks\n(DNNs). Although numerous effective defense techniques have been proposed in\nrecent years, they inevitably rely on the availability of either clean or\npoisoned data. In contrast, data-free defense techniques have evolved slowly\nand still lag significantly in performance. To address this issue, different\nfrom the traditional approach of pruning followed by fine-tuning, we propose a\nnovel data-free defense method named Optimal Transport-based Backdoor Repairing\n(OTBR) in this work. This method, based on our findings on neuron weight\nchanges (NWCs) of random unlearning, uses optimal transport (OT)-based model\nfusion to combine the advantages of both pruned and backdoored models.\nSpecifically, we first demonstrate our findings that the NWCs of random\nunlearning are positively correlated with those of poison unlearning. Based on\nthis observation, we propose a random-unlearning NWC pruning technique to\neliminate the backdoor effect and obtain a backdoor-free pruned model. Then,\nmotivated by the OT-based model fusion, we propose the pruned-to-backdoored\nOT-based fusion technique, which fuses pruned and backdoored models to combine\nthe advantages of both, resulting in a model that demonstrates high clean\naccuracy and a low attack success rate. To our knowledge, this is the first\nwork to apply OT and model fusion techniques to backdoor defense. Extensive\nexperiments show that our method successfully defends against all seven\nbackdoor attacks across three benchmark datasets, outperforming both\nstate-of-the-art (SOTA) data-free and data-dependent methods. The code\nimplementation and Appendix are provided in the Supplementary Material.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15861v1.pdf",
        "similarity": 0.33273339544890673,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of\n  Relational Knowledge in Language Models",
        "new_link": "http://arxiv.org/abs/2408.15729v1",
        "new_summary": "  Knowledge probing evaluates the extent to which a language model (LM) has\nacquired relational knowledge during its pre-training phase. It provides a\ncost-effective means of comparing LMs of different sizes and training setups\nand is useful for monitoring knowledge gained or lost during continual learning\n(CL). In prior work, we presented an improved knowledge probe called BEAR\n(Wiland et al., 2024), which enables the comparison of LMs trained with\ndifferent pre-training objectives (causal and masked LMs) and addresses issues\nof skewed distributions in previous probes to deliver a more unbiased reading\nof LM knowledge. With this paper, we present LM-PUB- QUIZ, a Python framework\nand leaderboard built around the BEAR probing mechanism that enables\nresearchers and practitioners to apply it in their work. It provides options\nfor standalone evaluation and direct integration into the widely-used training\npipeline of the Hugging Face TRANSFORMERS library. Further, it provides a\nfine-grained analysis of different knowledge types to assist users in better\nunderstanding the knowledge in each evaluated LM. We publicly release\nLM-PUB-QUIZ as an open-source project.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15729v1.pdf",
        "similarity": 0.33266409382301765,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Neural density functional theory of liquid-gas phase coexistence",
        "new_link": "http://arxiv.org/abs/2408.15835v1",
        "new_summary": "  We use supervised machine learning together with the concepts of classical\ndensity functional theory to investigate the effects of interparticle\nattraction on the pair structure, thermodynamics, bulk liquid-gas coexistence,\nand associated interfacial phenomena in many-body systems. Local learning of\nthe one-body direct correlation functional is based on Monte Carlo simulations\nof inhomogeneous systems with randomized thermodynamic conditions, randomized\nplanar shapes of the external potential, and randomized box sizes. Focusing on\nthe prototypical Lennard-Jones system, we test predictions of the resulting\nneural attractive density functional across a broad spectrum of physical\nbehaviour associated with liquid-gas phase coexistence in bulk and at\ninterfaces. We analyse the bulk radial distribution function $g(r)$ obtained\nfrom automatic differentiation and the Ornstein-Zernike route and determine i)\nthe Fisher-Widom line, i.e.\\ the crossover of the asymptotic (large distance)\ndecay of $g(r)$ from monotonic to oscillatory, ii) the (Widom) line of maximal\ncorrelation length, iii) the line of maximal isothermal compressibility and iv)\nthe spinodal by calculating the poles of the structure factor in the complex\nplane. The bulk binodal and the density profile of the free liquid-gas\ninterface are obtained from density functional minimization and the\ncorresponding surface tension from functional line integration. We also show\nthat the neural functional describes accurately the phenomena of drying at a\nhard wall and of capillary evaporation for a liquid confined in a slit pore.\nOur neural framework yields results that improve significantly upon standard\nmean-field treatments of interparticle attraction. Comparison with independent\nsimulation results demonstrates a consistent picture of phase separation even\nwhen restricting the training to supercritical states only.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15835v1.pdf",
        "similarity": 0.330021444257229,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Optimizing Vision Transformers with Data-Free Knowledge Transfer",
        "new_link": "http://arxiv.org/abs/2408.05952v1",
        "new_summary": "  The groundbreaking performance of transformers in Natural Language Processing\n(NLP) tasks has led to their replacement of traditional Convolutional Neural\nNetworks (CNNs), owing to the efficiency and accuracy achieved through the\nself-attention mechanism. This success has inspired researchers to explore the\nuse of transformers in computer vision tasks to attain enhanced long-term\nsemantic awareness. Vision transformers (ViTs) have excelled in various\ncomputer vision tasks due to their superior ability to capture long-distance\ndependencies using the self-attention mechanism. Contemporary ViTs like Data\nEfficient Transformers (DeiT) can effectively learn both global semantic\ninformation and local texture information from images, achieving performance\ncomparable to traditional CNNs. However, their impressive performance comes\nwith a high computational cost due to very large number of parameters,\nhindering their deployment on devices with limited resources like smartphones,\ncameras, drones etc. Additionally, ViTs require a large amount of data for\ntraining to achieve performance comparable to benchmark CNN models. Therefore,\nwe identified two key challenges in deploying ViTs on smaller form factor\ndevices: the high computational requirements of large models and the need for\nextensive training data. As a solution to these challenges, we propose\ncompressing large ViT models using Knowledge Distillation (KD), which is\nimplemented data-free to circumvent limitations related to data availability.\nAdditionally, we conducted experiments on object detection within the same\nenvironment in addition to classification tasks. Based on our analysis, we\nfound that datafree knowledge distillation is an effective method to overcome\nboth issues, enabling the deployment of ViTs on less resourceconstrained\ndevices.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.05952v1.pdf",
        "similarity": 0.3292410349001799,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-12"
    },
    {
        "new_title": "Realigned Softmax Warping for Deep Metric Learning",
        "new_link": "http://arxiv.org/abs/2408.15656v1",
        "new_summary": "  Deep Metric Learning (DML) loss functions traditionally aim to control the\nforces of separability and compactness within an embedding space so that the\nsame class data points are pulled together and different class ones are pushed\napart. Within the context of DML, a softmax operation will typically normalize\ndistances into a probability for optimization, thus coupling all the push/pull\nforces together. This paper proposes a potential new class of loss functions\nthat operate within a euclidean domain and aim to take full advantage of the\ncoupled forces governing embedding space formation under a softmax. These\nforces of compactness and separability can be boosted or mitigated within\ncontrolled locations at will by using a warping function. In this work, we\nprovide a simple example of a warping function and use it to achieve\ncompetitive, state-of-the-art results on various metric learning benchmarks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15656v1.pdf",
        "similarity": 0.32891481703147696,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Network transferability of adversarial patches in real-time object\n  detection",
        "new_link": "http://arxiv.org/abs/2408.15833v1",
        "new_summary": "  Adversarial patches in computer vision can be used, to fool deep neural\nnetworks and manipulate their decision-making process. One of the most\nprominent examples of adversarial patches are evasion attacks for object\ndetectors. By covering parts of objects of interest, these patches suppress the\ndetections and thus make the target object 'invisible' to the object detector.\nSince these patches are usually optimized on a specific network with a specific\ntrain dataset, the transferability across multiple networks and datasets is not\ngiven. This paper addresses these issues and investigates the transferability\nacross numerous object detector architectures. Our extensive evaluation across\nvarious models on two distinct datasets indicates that patches optimized with\nlarger models provide better network transferability than patches that are\noptimized with smaller models.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15833v1.pdf",
        "similarity": 0.3276862395502996,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Harmonized Speculative Sampling",
        "new_link": "http://arxiv.org/abs/2408.15766v1",
        "new_summary": "  Speculative sampling has proven to be an effective solution to accelerate\ndecoding from large language models, where the acceptance rate significantly\ndetermines the performance. Most previous works on improving the acceptance\nrate focus on aligned training and efficient decoding, implicitly paying less\nattention to the linkage of training and decoding. In this work, we first\ninvestigate the linkage of training and decoding for speculative sampling and\nthen propose a solution named HArmonized Speculative Sampling (HASS). HASS\nimproves the acceptance rate without extra inference overhead by harmonizing\ntraining and decoding on their objectives and contexts. Experiments on three\nLLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup\nratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15766v1.pdf",
        "similarity": 0.32733814892295227,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Online pre-training with long-form videos",
        "new_link": "http://arxiv.org/abs/2408.15651v1",
        "new_summary": "  In this study, we investigate the impact of online pre-training with\ncontinuous video clips. We will examine three methods for pre-training (masked\nimage modeling, contrastive learning, and knowledge distillation), and assess\nthe performance on downstream action recognition tasks. As a result, online\npre-training with contrast learning showed the highest performance in\ndownstream tasks. Our findings suggest that learning from long-form videos can\nbe helpful for action recognition with short videos.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15651v1.pdf",
        "similarity": 0.32578627844202085,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition\n  Capabilities of Language Models in Multi-Agent Systems",
        "new_link": "http://arxiv.org/abs/2408.15971v1",
        "new_summary": "  Large Language Models (LLMs) are becoming increasingly powerful and capable\nof handling complex tasks, e.g., building single agents and multi-agent\nsystems. Compared to single agents, multi-agent systems have higher\nrequirements for the collaboration capabilities of language models. Many\nbenchmarks are proposed to evaluate their collaborative abilities. However,\nthese benchmarks lack fine-grained evaluations of LLM collaborative\ncapabilities. Additionally, multi-agent collaborative and competitive scenarios\nare ignored in existing works. To address these two problems, we propose a\nbenchmark, called BattleAgentBench, which defines seven sub-stages of three\nvarying difficulty levels and conducts a fine-grained evaluation of language\nmodels in terms of single-agent scenario navigation capabilities, paired-agent\ntask execution abilities, and multi-agent collaboration and competition\ncapabilities. We conducted extensive evaluations on leading four closed-source\nand seven open-source models. Experimental results indicate that API-based\nmodels perform excellently on simple tasks but open-source small models\nstruggle with simple tasks. Regarding difficult tasks that require\ncollaborative and competitive abilities, although API-based models have\ndemonstrated some collaborative capabilities, there is still enormous room for\nimprovement.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15971v1.pdf",
        "similarity": 0.3256803491784595,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume",
        "new_link": "http://arxiv.org/abs/2408.15958v1",
        "new_summary": "  Current anomaly detection methods excel with benchmark industrial data but\nstruggle with natural images and medical data due to varying definitions of\n'normal' and 'abnormal.' This makes accurate identification of deviations in\nthese fields particularly challenging. Especially for 3D brain MRI data, all\nthe state-of-the-art models are reconstruction-based with 3D convolutional\nneural networks which are memory-intensive, time-consuming and producing noisy\noutputs that require further post-processing. We propose a framework called\nSimple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trained\non ImageNet and fine-tuned on a separate MRI dataset as a 2D slice feature\nextractor to reduce computational cost. We aggregate the extracted features to\nperform anomaly detection tasks on 3D brain MRI volumes. Our model integrates a\nconditional normalizing flow to calculate log likelihood of features and\nemploys the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. The\nresults indicate improved performance, showcasing our model's remarkable\nadaptability and effectiveness when addressing the challenges exists in brain\nMRI data. In addition, for the large-scale 3D brain volumes, our model\nSimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms of\naccuracy, memory usage and time consumption. Code is available at:\nhttps://anonymous.4open.science/r/SimpleSliceNet-8EA3.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15958v1.pdf",
        "similarity": 0.32446492288841566,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "TempoFormer: A Transformer for Temporally-aware Representations in\n  Change Detection",
        "new_link": "http://arxiv.org/abs/2408.15689v1",
        "new_summary": "  Dynamic representation learning plays a pivotal role in understanding the\nevolution of linguistic content over time. On this front both context and time\ndynamics as well as their interplay are of prime importance. Current approaches\nmodel context via pre-trained representations, which are typically temporally\nagnostic. Previous work on modeling context and temporal dynamics has used\nrecurrent methods, which are slow and prone to overfitting. Here we introduce\nTempoFormer, the fist task-agnostic transformer-based and temporally-aware\nmodel for dynamic representation learning. Our approach is jointly trained on\ninter and intra context dynamics and introduces a novel temporal variation of\nrotary positional embeddings. The architecture is flexible and can be used as\nthe temporal representation foundation of other models or applied to different\ntransformer-based architectures. We show new SOTA performance on three\ndifferent real-time change detection tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15689v1.pdf",
        "similarity": 0.3227736038155479,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Fast and accurate machine-learned interatomic potentials for large-scale\n  simulations of Cu, Al and Ni",
        "new_link": "http://arxiv.org/abs/2408.15779v1",
        "new_summary": "  Machine learning (ML) has become widely used in the development of\ninteratomic potentials for molecular dynamics simulations. However, most ML\npotentials are still much slower than classical interatomic potentials and are\nusually trained with near equilibrium simulations in mind. In this work, we\ndevelop ML potentials for Cu, Al and Ni using the Gaussian approximation\npotential (GAP) method. Specifically, we create the low-dimensional tabulated\nversions (tabGAP) of the potentials, which allow for two orders of magnitude\nhigher computational efficiency than the GAPs, enabling simulations of large\nmulti-million atomic systems. The ML potentials are trained using diverse\ncurated databases of structures and include fixed external repulsive potentials\nfor short-range interactions. The potentials are extensively validated and used\nto simulate a wide range of fundamental materials properties, such as stacking\nfaults and threshold displacement energies. Furthermore, we use the potentials\nto simulate single-crystal uniaxial compressive loading in different crystal\norientations with both pristine simulation cells and cells containing\npre-existing defects.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15779v1.pdf",
        "similarity": 0.32261076004675776,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil\n  Financial Time Series Regularities",
        "new_link": "http://arxiv.org/abs/2408.10111v2",
        "new_summary": "  Financial time series modeling is crucial for understanding and predicting\nmarket behaviors but faces challenges such as non-linearity, non-stationarity,\nand high noise levels. Traditional models struggle to capture complex patterns\ndue to these issues, compounded by limitations in computational resources and\nmodel capacity. Inspired by the success of large language models in NLP, we\nintroduce $\\textbf{PLUTUS}$, a $\\textbf{P}$re-trained $\\textbf{L}$arge\n$\\textbf{U}$nified $\\textbf{T}$ransformer-based model that $\\textbf{U}$nveils\nregularities in financial time $\\textbf{S}$eries. PLUTUS uses an invertible\nembedding module with contrastive learning and autoencoder techniques to create\nan approximate one-to-one mapping between raw data and patch embeddings.\nTimeFormer, an attention based architecture, forms the core of PLUTUS,\neffectively modeling high-noise time series. We incorporate a novel attention\nmechanisms to capture features across both variable and temporal dimensions.\nPLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,\ndesigned to thrive in noisy financial environments. To our knowledge, PLUTUS is\nthe first open-source, large-scale, pre-trained financial time series model\nwith over one billion parameters. It achieves state-of-the-art performance in\nvarious tasks, demonstrating strong transferability and establishing a robust\nfoundational model for finance. Our research provides technical guidance for\npre-training financial time series data, setting a new standard in the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.10111v2.pdf",
        "similarity": 0.3218679942635552,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-19"
    },
    {
        "new_title": "Generalisation First, Memorisation Second? Memorisation Localisation for\n  Natural Language Classification Tasks",
        "new_link": "http://arxiv.org/abs/2408.04965v1",
        "new_summary": "  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.04965v1.pdf",
        "similarity": 0.31875799356837686,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-09"
    },
    {
        "new_title": "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting\n  and Permitting domain",
        "new_link": "http://arxiv.org/abs/2408.11800v1",
        "new_summary": "  In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11800v1.pdf",
        "similarity": 0.3186596208392308,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "Sigma Flows for Image and Data Labeling and Learning Structured\n  Prediction",
        "new_link": "http://arxiv.org/abs/2408.15946v1",
        "new_summary": "  This paper introduces the sigma flow model for the prediction of structured\nlabelings of data observed on Riemannian manifolds, including Euclidean image\ndomains as special case. The approach combines the Laplace-Beltrami framework\nfor image denoising and enhancement, introduced by Sochen, Kimmel and Malladi\nabout 25 years ago, and the assignment flow approach introduced and studied by\nthe authors.\n  The sigma flow arises as Riemannian gradient flow of generalized harmonic\nenergies and thus is governed by a nonlinear geometric PDE which determines a\nharmonic map from a closed Riemannian domain manifold to a statistical\nmanifold, equipped with the Fisher-Rao metric from information geometry. A\nspecific ingredient of the sigma flow is the mutual dependency of the\nRiemannian metric of the domain manifold on the evolving state. This makes the\napproach amenable to machine learning in a specific way, by realizing this\ndependency through a mapping with compact time-variant parametrization that can\nbe learned from data. Proof of concept experiments demonstrate the expressivity\nof the sigma flow model and prediction performance.\n  Structural similarities to transformer network architectures and networks\ngenerated by the geometric integration of sigma flows are pointed out, which\nhighlights the connection to deep learning and, conversely, may stimulate the\nuse of geometric design principles for structured prediction in other areas of\nscientific machine learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15946v1.pdf",
        "similarity": 0.3172074424595383,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Multi-modal Adversarial Training for Zero-Shot Voice Cloning",
        "new_link": "http://arxiv.org/abs/2408.15916v1",
        "new_summary": "  A text-to-speech (TTS) model trained to reconstruct speech given text tends\ntowards predictions that are close to the average characteristics of a dataset,\nfailing to model the variations that make human speech sound natural. This\nproblem is magnified for zero-shot voice cloning, a task that requires training\ndata with high variance in speaking styles. We build off of recent works which\nhave used Generative Advsarial Networks (GAN) by proposing a Transformer\nencoder-decoder architecture to conditionally discriminates between real and\ngenerated speech features. The discriminator is used in a training pipeline\nthat improves both the acoustic and prosodic features of a TTS model. We\nintroduce our novel adversarial training technique by applying it to a\nFastSpeech2 acoustic model and training on Libriheavy, a large multi-speaker\ndataset, for the task of zero-shot voice cloning. Our model achieves\nimprovements over the baseline in terms of speech quality and speaker\nsimilarity. Audio examples from our system are available online.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15916v1.pdf",
        "similarity": 0.3166044814440349,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
        "new_link": "http://arxiv.org/abs/2408.06941v1",
        "new_summary": "  The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.06941v1.pdf",
        "similarity": 0.3159816104172574,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-13"
    },
    {
        "new_title": "Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language\n  Adaptation of LLMs for Low-Resource NLP",
        "new_link": "http://arxiv.org/abs/2408.04303v1",
        "new_summary": "  The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.04303v1.pdf",
        "similarity": 0.31483751052576087,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-08"
    },
    {
        "new_title": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals",
        "new_link": "http://arxiv.org/abs/2408.04575v2",
        "new_summary": "  Explainable Artificial Intelligence (XAI) plays a crucial role in enhancing\nthe transparency and accountability of AI models, particularly in natural\nlanguage processing (NLP) tasks. However, popular XAI methods such as LIME and\nSHAP have been found to be unstable and potentially misleading, underscoring\nthe need for a standardized evaluation approach. This paper introduces SCENE\n(Soft Counterfactual Evaluation for Natural language Explainability), a novel\nevaluation method that leverages large language models (LLMs) to generate Soft\nCounterfactual explanations in a zero-shot manner. By focusing on token-based\nsubstitutions, SCENE creates contextually appropriate and semantically\nmeaningful Soft Counterfactuals without extensive fine-tuning. SCENE adopts\nValiditysoft and Csoft metrics to assess the effectiveness of model-agnostic\nXAI methods in text classification tasks. Applied to CNN, RNN, and Transformer\narchitectures, SCENE provides valuable insights into the strengths and\nlimitations of various XAI techniques.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.04575v2.pdf",
        "similarity": 0.3144001779846977,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-08"
    },
    {
        "new_title": "Benchmarking foundation models as feature extractors for\n  weakly-supervised computational pathology",
        "new_link": "http://arxiv.org/abs/2408.15823v1",
        "new_summary": "  Advancements in artificial intelligence have driven the development of\nnumerous pathology foundation models capable of extracting clinically relevant\ninformation. However, there is currently limited literature independently\nevaluating these foundation models on truly external cohorts and\nclinically-relevant tasks to uncover adjustments for future improvements. In\nthis study, we benchmarked ten histopathology foundation models on 13 patient\ncohorts with 6,791 patients and 9,493 slides from lung, colorectal, gastric,\nand breast cancers. The models were evaluated on weakly-supervised tasks\nrelated to biomarkers, morphological properties, and prognostic outcomes. We\nshow that a vision-language foundation model, CONCH, yielded the highest\nperformance in 42% of tasks when compared to vision-only foundation models. The\nexperiments reveal that foundation models trained on distinct cohorts learn\ncomplementary features to predict the same label, and can be fused to\noutperform the current state of the art. Creating an ensemble of complementary\nfoundation models outperformed CONCH in 66% of tasks. Moreover, our findings\nsuggest that data diversity outweighs data volume for foundation models. Our\nwork highlights actionable adjustments to improve pathology foundation models.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15823v1.pdf",
        "similarity": 0.31307687302114606,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models",
        "new_link": "http://arxiv.org/abs/2408.15796v1",
        "new_summary": "  This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15796v1.pdf",
        "similarity": 0.3117079543680261,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Visual Prompt Engineering for Medical Vision Language Models in\n  Radiology",
        "new_link": "http://arxiv.org/abs/2408.15802v1",
        "new_summary": "  Medical image classification in radiology faces significant challenges,\nparticularly in generalizing to unseen pathologies. In contrast, CLIP offers a\npromising solution by leveraging multimodal learning to improve zero-shot\nclassification performance. However, in the medical domain, lesions can be\nsmall and might not be well represented in the embedding space. Therefore, in\nthis paper, we explore the potential of visual prompt engineering to enhance\nthe capabilities of Vision Language Models (VLMs) in radiology. Leveraging\nBiomedCLIP, trained on extensive biomedical image-text pairs, we investigate\nthe impact of embedding visual markers directly within radiological images to\nguide the model's attention to critical regions. Our evaluation on the JSRT\ndataset, focusing on lung nodule malignancy classification, demonstrates that\nincorporating visual prompts $\\unicode{x2013}$ such as arrows, circles, and\ncontours $\\unicode{x2013}$ significantly improves classification metrics\nincluding AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides\nattention maps, showcasing enhanced model interpretability and focus on\nclinically relevant areas. These findings underscore the efficacy of visual\nprompt engineering as a straightforward yet powerful approach to advance VLM\nperformance in medical image analysis.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15802v1.pdf",
        "similarity": 0.307256642262731,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "TEDRA: Text-based Editing of Dynamic and Photoreal Actors",
        "new_link": "http://arxiv.org/abs/2408.15995v1",
        "new_summary": "  Over the past years, significant progress has been made in creating\nphotorealistic and drivable 3D avatars solely from videos of real humans.\nHowever, a core remaining challenge is the fine-grained and user-friendly\nediting of clothing styles by means of textual descriptions. To this end, we\npresent TEDRA, the first method allowing text-based edits of an avatar, which\nmaintains the avatar's high fidelity, space-time coherency, as well as\ndynamics, and enables skeletal pose and view control. We begin by training a\nmodel to create a controllable and high-fidelity digital replica of the real\nactor. Next, we personalize a pretrained generative diffusion model by\nfine-tuning it on various frames of the real character captured from different\ncamera angles, ensuring the digital representation faithfully captures the\ndynamics and movements of the real person. This two-stage process lays the\nfoundation for our approach to dynamic human avatar editing. Utilizing this\npersonalized diffusion model, we modify the dynamic avatar based on a provided\ntext prompt using our Personalized Normal Aligned Score Distillation Sampling\n(PNA-SDS) within a model-based guidance framework. Additionally, we propose a\ntime step annealing strategy to ensure high-quality edits. Our results\ndemonstrate a clear improvement over prior work in functionality and visual\nquality.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15995v1.pdf",
        "similarity": 0.3044028901268442,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "DiffAge3D: Diffusion-based 3D-aware Face Aging",
        "new_link": "http://arxiv.org/abs/2408.15922v1",
        "new_summary": "  Face aging is the process of converting an individual's appearance to a\nyounger or older version of themselves. Existing face aging techniques have\nbeen limited to 2D settings, which often weaken their applications as there is\na growing demand for 3D face modeling. Moreover, existing aging methods\nstruggle to perform faithful aging, maintain identity, and retain the fine\ndetails of the input images. Given these limitations and the need for a\n3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging framework\nthat not only performs faithful aging and identity preservation but also\noperates in a 3D setting. Our aging framework allows to model the aging and\ncamera pose separately by only taking a single image with a target age. Our\nframework includes a robust 3D-aware aging dataset generation pipeline by\nutilizing a pre-trained 3D GAN and the rich text embedding capabilities within\nCLIP model. Notably, we do not employ any inversion bottleneck in dataset\ngeneration. Instead, we randomly generate training samples from the latent\nspace of 3D GAN, allowing us to manipulate the rich latent space of GAN to\ngenerate ages even with large gaps. With the generated dataset, we train a\nviewpoint-aware diffusion-based aging model to control the camera pose and\nfacial age. Through quantitative and qualitative evaluations, we demonstrate\nthat DiffAge3D outperforms existing methods, particularly in\nmultiview-consistent aging and fine details preservation.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15922v1.pdf",
        "similarity": 0.3040487188962006,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Goldfish: Monolingual Language Models for 350 Languages",
        "new_link": "http://arxiv.org/abs/2408.10441v1",
        "new_summary": "  For many low-resource languages, the only available language models are large\nmultilingual models trained on many languages simultaneously. However, using\nFLORES perplexity as a metric, we find that these models perform worse than\nbigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM\n7.1B). To facilitate research that focuses on low-resource languages, we\npre-train and release Goldfish, a suite of monolingual autoregressive\nTransformer language models up to 125M parameters for 350 languages. The\nGoldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98\nof 204 FLORES languages, despite each Goldfish model being over 10x smaller.\nHowever, the Goldfish significantly underperform larger multilingual models on\nreasoning benchmarks, suggesting that for low-resource languages,\nmultilinguality primarily improves general reasoning abilities rather than\nbasic text generation. We release models trained on 5MB (350 languages), 10MB\n(288 languages), 100MB (166 languages), and 1GB (83 languages) of text data\nwhere available. The Goldfish models are available as baselines, fine-tuning\nsources, or augmentations to existing models in low-resource NLP research, and\nthey are further useful for crosslinguistic studies requiring maximally\ncomparable models across languages.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.10441v1.pdf",
        "similarity": 0.3019290985772762,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-19"
    },
    {
        "new_title": "Learning dynamics models for velocity estimation in autonomous racing",
        "new_link": "http://arxiv.org/abs/2408.15610v1",
        "new_summary": "  Velocity estimation is of great importance in autonomous racing. Still,\nexisting solutions are characterized by limited accuracy, especially in the\ncase of aggressive driving or poor generalization to unseen road conditions. To\naddress these issues, we propose to utilize Unscented Kalman Filter (UKF) with\na learned dynamics model that is optimized directly for the state estimation\ntask. Moreover, we propose to aid this model with the online-estimated friction\ncoefficient, which increases the estimation accuracy and enables zero-shot\nadaptation to the new road conditions. To evaluate the UKF-based velocity\nestimator with the proposed dynamics model, we introduced a publicly available\ndataset of aggressive manoeuvres performed by an F1TENTH car, with sideslip\nangles reaching 40{\\deg}. Using this dataset, we show that learning the\ndynamics model through UKF leads to improved estimation performance and that\nthe proposed solution outperforms state-of-the-art learning-based state\nestimators by 17% in the nominal scenario. Moreover, we present unseen\nzero-shot adaptation abilities of the proposed method to the new road surface\nthanks to the use of the proposed learning-based tire dynamics model with\nonline friction estimation.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15610v1.pdf",
        "similarity": 0.3011370098850574,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "PointEMRay: A Novel Efficient SBR Framework on Point Based Geometry",
        "new_link": "http://arxiv.org/abs/2408.15583v1",
        "new_summary": "  The rapid computation of electromagnetic (EM) fields across various scenarios\nhas long been a challenge, primarily due to the need for precise geometric\nmodels. The emergence of point cloud data offers a potential solution to this\nissue. However, the lack of electromagnetic simulation algorithms optimized for\npoint-based models remains a significant limitation. In this study, we propose\nPointEMRay, an innovative shooting and bouncing ray (SBR) framework designed\nexplicitly for point-based geometries. To enable SBR on point clouds, we\naddress two critical challenges: point-ray intersection (PRI) and multiple\nbounce computation (MBC). For PRI, we propose a screen-based method leveraging\ndeep learning. Initially, we obtain coarse depth maps through ray tube tracing,\nwhich are then transformed by a neural network into dense depth maps, normal\nmaps, and intersection masks, collectively referred to as geometric frame\nbuffers (GFBs). For MBC, inspired by simultaneous localization and mapping\n(SLAM) techniques, we introduce a GFB-assisted approach. This involves\naggregating GFBs from various observation angles and integrating them to\nrecover the complete geometry. Subsequently, a ray tracing algorithm is applied\nto these GFBs to compute the scattering electromagnetic field. Numerical\nexperiments demonstrate the superior performance of PointEMRay in terms of both\naccuracy and efficiency, including support for real-time simulation. To the\nbest of our knowledge, this study represents the first attempt to develop an\nSBR framework specifically tailored for point-based models.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15583v1.pdf",
        "similarity": 0.30086371481197993,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and\n  Attribution",
        "new_link": "http://arxiv.org/abs/2408.15993v1",
        "new_summary": "  Detecting and attributing temperature increases due to climate change is\ncrucial for understanding global warming and guiding adaptation strategies. The\ncomplexity of distinguishing human-induced climate signals from natural\nvariability has challenged traditional detection and attribution (D&A)\napproaches, which seek to identify specific \"fingerprints\" in climate response\nvariables. Deep learning offers potential for discerning these complex patterns\nin expansive spatial datasets. However, lack of standard protocols has hindered\nconsistent comparisons across studies. We introduce ClimDetect, a standardized\ndataset of over 816k daily climate snapshots, designed to enhance model\naccuracy in identifying climate change signals. ClimDetect integrates various\ninput and target variables used in past research, ensuring comparability and\nconsistency. We also explore the application of vision transformers (ViT) to\nclimate data, a novel and modernizing approach in this context. Our open-access\ndata and code serve as a benchmark for advancing climate science through\nimproved model evaluations. ClimDetect is publicly accessible via Huggingface\ndataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15993v1.pdf",
        "similarity": 0.30064778343659515,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Segmentation-guided Layer-wise Image Vectorization with Gradient Fills",
        "new_link": "http://arxiv.org/abs/2408.15741v1",
        "new_summary": "  The widespread use of vector graphics creates a significant demand for\nvectorization methods. While recent learning-based techniques have shown their\ncapability to create vector images of clear topology, filling these primitives\nwith gradients remains a challenge. In this paper, we propose a\nsegmentation-guided vectorization framework to convert raster images into\nconcise vector graphics with radial gradient fills. With the guidance of an\nembedded gradient-aware segmentation subroutine, our approach progressively\nappends gradient-filled B\\'ezier paths to the output, where primitive\nparameters are initiated with our newly designed initialization technique and\nare optimized to minimize our novel loss function. We build our method on a\ndifferentiable renderer with traditional segmentation algorithms to develop it\nas a model-free tool for raster-to-vector conversion. It is tested on various\ninputs to demonstrate its feasibility, independent of datasets, to synthesize\nvector graphics with improved visual quality and layer-wise topology compared\nto prior work.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15741v1.pdf",
        "similarity": 0.2981513682110285,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "PhysBERT: A Text Embedding Model for Physics Scientific Literature",
        "new_link": "http://arxiv.org/abs/2408.09574v1",
        "new_summary": "  The specialized language and complex concepts in physics pose significant\nchallenges for information extraction through Natural Language Processing\n(NLP). Central to effective NLP applications is the text embedding model, which\nconverts text into dense vector representations for efficient information\nretrieval and semantic analysis. In this work, we introduce PhysBERT, the first\nphysics-specific text embedding model. Pre-trained on a curated corpus of 1.2\nmillion arXiv physics papers and fine-tuned with supervised data, PhysBERT\noutperforms leading general-purpose models on physics-specific tasks including\nthe effectiveness in fine-tuning for specific physics subdomains.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.09574v1.pdf",
        "similarity": 0.2969362854233171,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-18"
    },
    {
        "new_title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models",
        "new_link": "http://arxiv.org/abs/2408.02085v3",
        "new_summary": "  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02085v3.pdf",
        "similarity": 0.29524503169351396,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-04"
    },
    {
        "new_title": "MEGen: Generative Backdoor in Large Language Models via Model Editing",
        "new_link": "http://arxiv.org/abs/2408.10722v1",
        "new_summary": "  Large language models (LLMs) have demonstrated remarkable capabilities. Their\npowerful generative abilities enable flexible responses based on various\nqueries or instructions. Emerging as widely adopted generalists for diverse\ntasks, LLMs are still vulnerable to backdoors. This paper proposes an\nediting-based generative backdoor, named MEGen, aiming to create a customized\nbackdoor for NLP tasks with the least side effects. In our approach, we first\nleverage a language model to insert a trigger selected on fixed metrics into\nthe input, then design a pipeline of model editing to directly embed a backdoor\ninto an LLM. By adjusting a small set of local parameters with a mini-batch of\nsamples, MEGen significantly enhances time efficiency and achieves high\nrobustness. Experimental results indicate that our backdoor attack strategy\nachieves a high attack success rate on poison data while maintaining the\nmodel's performance on clean data. Notably, the backdoored model, when\ntriggered, can freely output pre-set dangerous information while successfully\ncompleting downstream tasks. This suggests that future LLM applications could\nbe guided to deliver certain dangerous information, thus altering the LLM's\ngenerative style. We believe this approach provides insights for future LLM\napplications and the execution of backdoor attacks on conversational AI\nsystems.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.10722v1.pdf",
        "similarity": 0.2951038517795532,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-20"
    },
    {
        "new_title": "Transfer Learning from Simulated to Real Scenes for Monocular 3D Object\n  Detection",
        "new_link": "http://arxiv.org/abs/2408.15637v1",
        "new_summary": "  Accurately detecting 3D objects from monocular images in dynamic roadside\nscenarios remains a challenging problem due to varying camera perspectives and\nunpredictable scene conditions. This paper introduces a two-stage training\nstrategy to address these challenges. Our approach initially trains a model on\nthe large-scale synthetic dataset, RoadSense3D, which offers a diverse range of\nscenarios for robust feature learning. Subsequently, we fine-tune the model on\na combination of real-world datasets to enhance its adaptability to practical\nconditions. Experimental results of the Cube R-CNN model on challenging public\nbenchmarks show a remarkable improvement in detection performance, with a mean\naverage precision rising from 0.26 to 12.76 on the TUM Traffic A9 Highway\ndataset and from 2.09 to 6.60 on the DAIR-V2X-I dataset when performing\ntransfer learning. Code, data, and qualitative video results are available on\nthe project website: https://roadsense3d.github.io.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15637v1.pdf",
        "similarity": 0.2950011277635199,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Unlocking Efficiency: Adaptive Masking for Gene Transformer Models",
        "new_link": "http://arxiv.org/abs/2408.07180v1",
        "new_summary": "  Gene transformer models such as Nucleotide Transformer, DNABert, and LOGO are\ntrained to learn optimal gene sequence representations by using the Masked\nLanguage Modeling (MLM) training objective over the complete Human Reference\nGenome. However, the typical tokenization methods employ a basic sliding window\nof tokens, such as k-mers, that fail to utilize gene-centric semantics. This\ncould result in the (trivial) masking of easily predictable sequences, leading\nto inefficient MLM training. Time-variant training strategies are known to\nimprove pretraining efficiency in both language and vision tasks. In this work,\nwe focus on using curriculum masking where we systematically increase the\ndifficulty of masked token prediction task by using a Pointwise Mutual\nInformation-based difficulty criterion, as gene sequences lack well-defined\nsemantic units similar to words or sentences of NLP domain. Our proposed\nCurriculum Masking-based Gene Masking Strategy (CM-GEMS) demonstrates superior\nrepresentation learning capabilities compared to baseline masking approaches\nwhen evaluated on downstream gene sequence classification tasks. We perform\nextensive evaluation in both few-shot (five datasets) and full dataset settings\n(Genomic Understanding Evaluation benchmark consisting of 27 tasks). Our\nfindings reveal that CM-GEMS outperforms state-of-the-art models (DNABert-2,\nNucleotide transformer, DNABert) trained at 120K steps, achieving similar\nresults in just 10K and 1K steps. We also demonstrate that Curriculum-Learned\nLOGO (a 2-layer DNABert-like model) can achieve nearly 90% of the\nstate-of-the-art model performance of 120K steps. We will make the models and\ncodes publicly available at https://github.com/roysoumya/curriculum-GeneMask.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.07180v1.pdf",
        "similarity": 0.2945342211255103,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-13"
    },
    {
        "new_title": "LEGO: Self-Supervised Representation Learning for Scene Text Images",
        "new_link": "http://arxiv.org/abs/2408.02036v1",
        "new_summary": "  In recent years, significant progress has been made in scene text recognition\nby data-driven methods. However, due to the scarcity of annotated real-world\ndata, the training of these methods predominantly relies on synthetic data. The\ndistribution gap between synthetic and real data constrains the further\nperformance improvement of these methods in real-world applications. To tackle\nthis problem, a highly promising approach is to utilize massive amounts of\nunlabeled real data for self-supervised training, which has been widely proven\neffective in many NLP and CV tasks. Nevertheless, generic self-supervised\nmethods are unsuitable for scene text images due to their sequential nature. To\naddress this issue, we propose a Local Explicit and Global Order-aware\nself-supervised representation learning method (LEGO) that accounts for the\ncharacteristics of scene text images. Inspired by the human cognitive process\nof learning words, which involves spelling, reading, and writing, we propose\nthree novel pre-text tasks for LEGO to model sequential, semantic, and\nstructural features, respectively. The entire pre-training process is optimized\nby using a consistent Text Knowledge Codebook. Extensive experiments validate\nthat LEGO outperforms previous scene text self-supervised methods. The\nrecognizer incorporated with our pre-trained model achieves superior or\ncomparable performance compared to state-of-the-art scene text recognition\nmethods on six benchmarks. Furthermore, we demonstrate that LEGO can achieve\nsuperior performance in other text-related tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02036v1.pdf",
        "similarity": 0.2939604807673851,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-04"
    },
    {
        "new_title": "Discrete Shortest Paths in Optimal Power Flow Feasible Regions",
        "new_link": "http://arxiv.org/abs/2408.02172v1",
        "new_summary": "  Optimal power flow (OPF) is a critical optimization problem for power systems\nto operate at points where cost or operational objectives are optimized. Due to\nthe non-convexity of the set of feasible OPF operating points, it is\nnon-trivial to transition the power system from its current operating point to\nthe optimal one without violating constraints. On top of that, practical\nconsiderations dictate that the transition should be achieved using a small\nnumber of small-magnitude control actions. To solve this problem, this paper\nproposes an algorithm for computing a transition path by framing it as a\nshortest path problem. This problem is formulated in terms of a discretized\npiece-wise linear path, where the number of pieces is fixed a priori in order\nto limit the number of control actions. This formulation yields a nonlinear\noptimization problem (NLP) with a block tridiagonal structure, which we\nleverage by utilizing a specialized interior point method. An initial feasible\npath for our method is generated by solving a sequence of relaxations which are\nthen tightened in a homotopy-like procedure. Numerical experiments illustrate\nthe effectiveness of the algorithm.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02172v1.pdf",
        "similarity": 0.29368022249859244,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-05"
    },
    {
        "new_title": "Efficient LLM Scheduling by Learning to Rank",
        "new_link": "http://arxiv.org/abs/2408.15792v1",
        "new_summary": "  In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15792v1.pdf",
        "similarity": 0.2936736974811347,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Geometry-guided Feature Learning and Fusion for Indoor Scene\n  Reconstruction",
        "new_link": "http://arxiv.org/abs/2408.15608v1",
        "new_summary": "  In addition to color and textural information, geometry provides important\ncues for 3D scene reconstruction. However, current reconstruction methods only\ninclude geometry at the feature level thus not fully exploiting the geometric\ninformation.\n  In contrast, this paper proposes a novel geometry integration mechanism for\n3D scene reconstruction. Our approach incorporates 3D geometry at three levels,\ni.e. feature learning, feature fusion, and network supervision. First,\ngeometry-guided feature learning encodes geometric priors to contain\nview-dependent information. Second, a geometry-guided adaptive feature fusion\nis introduced which utilizes the geometric priors as a guidance to adaptively\ngenerate weights for multiple views. Third, at the supervision level, taking\nthe consistency between 2D and 3D normals into account, a consistent 3D normal\nloss is designed to add local constraints.\n  Large-scale experiments are conducted on the ScanNet dataset, showing that\nvolumetric methods with our geometry integration mechanism outperform\nstate-of-the-art methods quantitatively as well as qualitatively. Volumetric\nmethods with ours also show good generalization on the 7-Scenes and TUM RGB-D\ndatasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15608v1.pdf",
        "similarity": 0.2927718018923708,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled\n  Queries",
        "new_link": "http://arxiv.org/abs/2408.15813v1",
        "new_summary": "  LiDAR panoptic segmentation, which jointly performs instance and semantic\nsegmentation for things and stuff classes, plays a fundamental role in LiDAR\nperception tasks. While most existing methods explicitly separate these two\nsegmentation tasks and utilize different branches (i.e., semantic and instance\nbranches), some recent methods have embraced the query-based paradigm to unify\nLiDAR panoptic segmentation. However, the distinct spatial distribution and\ninherent characteristics of objects(things) and their surroundings(stuff) in 3D\nscenes lead to challenges, including the mutual competition of things/stuff and\nthe ambiguity of classification/segmentation. In this paper, we propose\ndecoupling things/stuff queries according to their intrinsic properties for\nindividual decoding and disentangling classification/segmentation to mitigate\nambiguity. To this end, we propose a novel framework dubbed DQFormer to\nimplement semantic and instance segmentation in a unified workflow.\nSpecifically, we design a decoupled query generator to propose informative\nqueries with semantics by localizing things/stuff positions and fusing\nmulti-level BEV embeddings. Moreover, a query-oriented mask decoder is\nintroduced to decode corresponding segmentation masks by performing masked\ncross-attention between queries and mask embeddings. Finally, the decoded masks\nare combined with the semantics of the queries to produce panoptic results.\nExtensive experiments on nuScenes and SemanticKITTI datasets demonstrate the\nsuperiority of our DQFormer framework.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15813v1.pdf",
        "similarity": 0.29218447604251563,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "A Survey on Evaluation of Multimodal Large Language Models",
        "new_link": "http://arxiv.org/abs/2408.15769v1",
        "new_summary": "  Multimodal Large Language Models (MLLMs) mimic human perception and reasoning\nsystem by integrating powerful Large Language Models (LLMs) with various\nmodality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and\nvarious modality encoders as sensory organs. This framework endows MLLMs with\nhuman-like capabilities, and suggests a potential pathway towards achieving\nartificial general intelligence (AGI). With the emergence of all-round MLLMs\nlike GPT-4V and Gemini, a multitude of evaluation methods have been developed\nto assess their capabilities across different dimensions. This paper presents a\nsystematic and comprehensive review of MLLM evaluation methods, covering the\nfollowing key aspects: (1) the background of MLLMs and their evaluation; (2)\n\"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks\nbased on the capabilities assessed, including general multimodal recognition,\nperception, reasoning and trustworthiness, and domain-specific applications\nsuch as socioeconomic, natural sciences and engineering, medical usage, AI\nagent, remote sensing, video and audio processing, 3D point cloud analysis, and\nothers; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into\ngeneral and specific benchmarks; (4) \"how to evaluate\" that reviews and\nillustrates MLLM evaluation steps and metrics; Our overarching goal is to\nprovide valuable insights for researchers in the field of MLLM evaluation,\nthereby facilitating the development of more capable and reliable MLLMs. We\nemphasize that evaluation should be regarded as a critical discipline,\nessential for advancing the field of MLLMs.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15769v1.pdf",
        "similarity": 0.291941416137713,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Do Large Language Models Speak All Languages Equally? A Comparative\n  Study in Low-Resource Settings",
        "new_link": "http://arxiv.org/abs/2408.02237v1",
        "new_summary": "  Large language models (LLMs) have garnered significant interest in natural\nlanguage processing (NLP), particularly their remarkable performance in various\ndownstream tasks in resource-rich languages. Recent studies have highlighted\nthe limitations of LLMs in low-resource languages, primarily focusing on binary\nclassification tasks and giving minimal attention to South Asian languages.\nThese limitations are primarily attributed to constraints such as dataset\nscarcity, computational costs, and research gaps specific to low-resource\nlanguages. To address this gap, we present datasets for sentiment and hate\nspeech tasks by translating from English to Bangla, Hindi, and Urdu,\nfacilitating research in low-resource language processing. Further, we\ncomprehensively examine zero-shot learning using multiple LLMs in English and\nwidely spoken South Asian languages. Our findings indicate that GPT-4\nconsistently outperforms Llama 2 and Gemini, with English consistently\ndemonstrating superior performance across diverse tasks compared to\nlow-resource languages. Furthermore, our analysis reveals that natural language\ninference (NLI) exhibits the highest performance among the evaluated tasks,\nwith GPT-4 demonstrating superior capabilities.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02237v1.pdf",
        "similarity": 0.2917294084698261,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-05"
    },
    {
        "new_title": "Controllable Text Generation for Large Language Models: A Survey",
        "new_link": "http://arxiv.org/abs/2408.12599v1",
        "new_summary": "  In Natural Language Processing (NLP), Large Language Models (LLMs) have\ndemonstrated high text generation quality. However, in real-world applications,\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\ninappropriate content, LLMs are also expected to cater to specific user needs,\nsuch as imitating particular writing styles or generating text with poetic\nrichness. These varied demands have driven the development of Controllable Text\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\ncontrol conditions--such as safety, sentiment, thematic consistency, and\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\ndiversity.\n  This paper systematically reviews the latest advancements in CTG for LLMs,\noffering a comprehensive definition of its core concepts and clarifying the\nrequirements for control conditions and text quality. We categorize CTG tasks\ninto two primary types: content control and attribute control. The key methods\nare discussed, including model retraining, fine-tuning, reinforcement learning,\nprompt engineering, latent space manipulation, and decoding-time intervention.\nWe analyze each method's characteristics, advantages, and limitations,\nproviding nuanced insights for achieving generation control. Additionally, we\nreview CTG evaluation methods, summarize its applications across domains, and\naddress key challenges in current research, including reduced fluency and\npracticality. We also propose several appeals, such as placing greater emphasis\non real-world applications in future research. This paper aims to offer\nvaluable guidance to researchers and developers in the field. Our reference\nlist and Chinese version are open-sourced at\nhttps://github.com/IAAR-Shanghai/CTGSurvey.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.12599v1.pdf",
        "similarity": 0.2917236177553445,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-22"
    },
    {
        "new_title": "Evaluating Model Robustness Using Adaptive Sparse L0 Regularization",
        "new_link": "http://arxiv.org/abs/2408.15702v1",
        "new_summary": "  Deep Neural Networks have demonstrated remarkable success in various domains\nbut remain susceptible to adversarial examples, which are slightly altered\ninputs designed to induce misclassification. While adversarial attacks\ntypically optimize under Lp norm constraints, attacks based on the L0 norm,\nprioritising input sparsity, are less studied due to their complex and non\nconvex nature. These sparse adversarial examples challenge existing defenses by\naltering a minimal subset of features, potentially uncovering more subtle DNN\nweaknesses. However, the current L0 norm attack methodologies face a trade off\nbetween accuracy and efficiency either precise but computationally intense or\nexpedient but imprecise. This paper proposes a novel, scalable, and effective\napproach to generate adversarial examples based on the L0 norm, aimed at\nrefining the robustness evaluation of DNNs against such perturbations.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15702v1.pdf",
        "similarity": 0.2898306986185394,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Pixels to Prose: Understanding the art of Image Captioning",
        "new_link": "http://arxiv.org/abs/2408.15714v1",
        "new_summary": "  In the era of evolving artificial intelligence, machines are increasingly\nemulating human-like capabilities, including visual perception and linguistic\nexpression. Image captioning stands at the intersection of these domains,\nenabling machines to interpret visual content and generate descriptive text.\nThis paper provides a thorough review of image captioning techniques, catering\nto individuals entering the field of machine learning who seek a comprehensive\nunderstanding of available options, from foundational methods to\nstate-of-the-art approaches. Beginning with an exploration of primitive\narchitectures, the review traces the evolution of image captioning models to\nthe latest cutting-edge solutions. By dissecting the components of these\narchitectures, readers gain insights into the underlying mechanisms and can\nselect suitable approaches tailored to specific problem requirements without\nduplicating efforts. The paper also delves into the application of image\ncaptioning in the medical domain, illuminating its significance in various\nreal-world scenarios.\n  Furthermore, the review offers guidance on evaluating the performance of\nimage captioning systems, highlighting key metrics for assessment. By\nsynthesizing theoretical concepts with practical application, this paper equips\nreaders with the knowledge needed to navigate the complex landscape of image\ncaptioning and harness its potential for diverse applications in machine\nlearning and beyond.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15714v1.pdf",
        "similarity": 0.2881861364880589,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model\n  with Multimodal Capabilities",
        "new_link": "http://arxiv.org/abs/2408.12902v1",
        "new_summary": "  In the field of multimodal large language models (MLLMs), common methods\ntypically involve unfreezing the language model during training to foster\nprofound visual understanding. However, the fine-tuning of such models with\nvision-language data often leads to a diminution of their natural language\nprocessing (NLP) capabilities. To avoid this performance degradation, a\nstraightforward solution is to freeze the language model while developing\nmultimodal competencies. Unfortunately, previous works have not attained\nsatisfactory outcomes. Building on the strategy of freezing the language model,\nwe conduct thorough structural exploration and introduce the Inner-Adaptor\nArchitecture (IAA). Specifically, the architecture incorporates multiple\nmultimodal adaptors at varying depths within the large language model to\nfacilitate direct interaction with the inherently text-oriented transformer\nlayers, thereby enabling the frozen language model to acquire multimodal\ncapabilities. Unlike previous approaches of freezing language models that\nrequire large-scale aligned data, our proposed architecture is able to achieve\nsuperior performance on small-scale datasets. We conduct extensive experiments\nto improve the general multimodal capabilities and visual grounding abilities\nof the MLLM. Our approach remarkably outperforms previous state-of-the-art\nmethods across various vision-language benchmarks without sacrificing\nperformance on NLP tasks. Code and models are available at\nhttps://github.com/360CVGroup/Inner-Adaptor-Architecture.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.12902v1.pdf",
        "similarity": 0.28755415353284786,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-23"
    },
    {
        "new_title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations\n  and Texts",
        "new_link": "http://arxiv.org/abs/2408.05346v2",
        "new_summary": "  Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.05346v2.pdf",
        "similarity": 0.28733069327340394,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-09"
    },
    {
        "new_title": "Unlocking the Power of LSTM for Long Term Time Series Forecasting",
        "new_link": "http://arxiv.org/abs/2408.10006v1",
        "new_summary": "  Traditional recurrent neural network architectures, such as long short-term\nmemory neural networks (LSTM), have historically held a prominent role in time\nseries forecasting (TSF) tasks. While the recently introduced sLSTM for Natural\nLanguage Processing (NLP) introduces exponential gating and memory mixing that\nare beneficial for long term sequential learning, its potential short memory\nissue is a barrier to applying sLSTM directly in TSF. To address this, we\npropose a simple yet efficient algorithm named P-sLSTM, which is built upon\nsLSTM by incorporating patching and channel independence. These modifications\nsubstantially enhance sLSTM's performance in TSF, achieving state-of-the-art\nresults. Furthermore, we provide theoretical justifications for our design, and\nconduct extensive comparative and analytical experiments to fully validate the\nefficiency and superior performance of our model.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.10006v1.pdf",
        "similarity": 0.2837887744759757,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-19"
    },
    {
        "new_title": "What is YOLOv8: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector",
        "new_link": "http://arxiv.org/abs/2408.15857v1",
        "new_summary": "  This study presents a detailed analysis of the YOLOv8 object detection model,\nfocusing on its architecture, training techniques, and performance improvements\nover previous iterations like YOLOv5. Key innovations, including the CSPNet\nbackbone for enhanced feature extraction, the FPN+PAN neck for superior\nmulti-scale object detection, and the transition to an anchor-free approach,\nare thoroughly examined. The paper reviews YOLOv8's performance across\nbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy\nand real-time capabilities across diverse hardware platforms. Additionally, the\nstudy explores YOLOv8's developer-friendly enhancements, such as its unified\nPython package and CLI, which streamline model training and deployment.\nOverall, this research positions YOLOv8 as a state-of-the-art solution in the\nevolving object detection field.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15857v1.pdf",
        "similarity": 0.28323517933518905,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Towards a Unified Benchmark and Framework for Deep Learning-Based\n  Prediction of Nuclear Magnetic Resonance Chemical Shifts",
        "new_link": "http://arxiv.org/abs/2408.15681v1",
        "new_summary": "  The study of structure-spectrum relationships is essential for spectral\ninterpretation, impacting structural elucidation and material design.\nPredicting spectra from molecular structures is challenging due to their\ncomplex relationships. Herein, we introduce NMRNet, a deep learning framework\nusing the SE(3) Transformer for atomic environment modeling, following a\npre-training and fine-tuning paradigm. To support the evaluation of NMR\nchemical shift prediction models, we have established a comprehensive benchmark\nbased on previous research and databases, covering diverse chemical systems.\nApplying NMRNet to these benchmark datasets, we achieve state-of-the-art\nperformance in both liquid-state and solid-state NMR datasets, demonstrating\nits robustness and practical utility in real-world scenarios. This marks the\nfirst integration of solid and liquid state NMR within a unified model\narchitecture, highlighting the need for domainspecific handling of different\natomic environments. Our work sets a new standard for NMR prediction, advancing\ndeep learning applications in analytical and structural chemistry.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15681v1.pdf",
        "similarity": 0.2830463121297767,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection",
        "new_link": "http://arxiv.org/abs/2408.15996v1",
        "new_summary": "  Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15996v1.pdf",
        "similarity": 0.2807584891420599,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "InstanSeg: an embedding-based instance segmentation algorithm optimized\n  for accurate, efficient and portable cell segmentation",
        "new_link": "http://arxiv.org/abs/2408.15954v1",
        "new_summary": "  Cell and nucleus segmentation are fundamental tasks for quantitative bioimage\nanalysis. Despite progress in recent years, biologists and other domain experts\nstill require novel algorithms to handle increasingly large and complex\nreal-world datasets. These algorithms must not only achieve state-of-the-art\naccuracy, but also be optimized for efficiency, portability and\nuser-friendliness. Here, we introduce InstanSeg: a novel embedding-based\ninstance segmentation pipeline designed to identify cells and nuclei in\nmicroscopy images. Using six public cell segmentation datasets, we demonstrate\nthat InstanSeg can significantly improve accuracy when compared to the most\nwidely used alternative methods, while reducing the processing time by at least\n60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript\nand supports GPU acceleration on a range of hardware. We provide an open-source\nimplementation of InstanSeg in Python, in addition to a user-friendly,\ninteractive QuPath extension for inference written in Java. Our code and\npre-trained models are available at https://github.com/instanseg/instanseg .\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15954v1.pdf",
        "similarity": 0.27902319449921,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals\n  (MoU) Is All You Need",
        "new_link": "http://arxiv.org/abs/2408.15997v1",
        "new_summary": "  Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15997v1.pdf",
        "similarity": 0.27894402959036424,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex\n  Optimization Problems",
        "new_link": "http://arxiv.org/abs/2408.15969v1",
        "new_summary": "  We examine stability properties of primal-dual gradient flow dynamics for\ncomposite convex optimization problems with multiple, possibly nonsmooth, terms\nin the objective function under the generalized consensus constraint. The\nproposed dynamics are based on the proximal augmented Lagrangian and they\nprovide a viable alternative to ADMM which faces significant challenges from\nboth analysis and implementation viewpoints in large-scale multi-block\nscenarios. In contrast to customized algorithms with individualized convergence\nguarantees, we provide a systematic approach for solving a broad class of\nchallenging composite optimization problems. We leverage various structural\nproperties to establish global (exponential) convergence guarantees for the\nproposed dynamics. Our assumptions are much weaker than those required to prove\n(exponential) stability of various primal-dual dynamics as well as (linear)\nconvergence of discrete-time methods, e.g., standard two-block and multi-block\nADMM and EXTRA algorithms. Finally, we show necessity of some of our structural\nassumptions for exponential stability and provide computational experiments to\ndemonstrate the convenience of the proposed dynamics for parallel and\ndistributed computing applications.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15969v1.pdf",
        "similarity": 0.277345085840492,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Investigating Instruction Tuning Large Language Models on Graphs",
        "new_link": "http://arxiv.org/abs/2408.05457v1",
        "new_summary": "  Inspired by the recent advancements of Large Language Models (LLMs) in NLP\ntasks, there's growing interest in applying LLMs to graph-related tasks. This\nstudy delves into the capabilities of instruction-following LLMs for engaging\nwith real-world graphs, aiming to offer empirical insights into how LLMs can\neffectively interact with graphs and generalize across graph tasks. We begin by\nconstructing a dataset designed for instruction tuning, which comprises a\ndiverse collection of 79 graph-related tasks from academic and e-commerce\ndomains, featuring 44,240 training instances and 18,960 test samples. Utilizing\nthis benchmark, our initial investigation focuses on identifying the optimal\ngraph representation that serves as a conduit for LLMs to understand complex\ngraph structures. Our findings indicate that JSON format for graph\nrepresentation consistently outperforms natural language and code formats\nacross various LLMs and graph types. Furthermore, we examine the key factors\nthat influence the generalization abilities of instruction-tuned LLMs by\nevaluating their performance on both in-domain and out-of-domain graph tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.05457v1.pdf",
        "similarity": 0.27481954020777555,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-10"
    },
    {
        "new_title": "RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images",
        "new_link": "http://arxiv.org/abs/2408.14802v1",
        "new_summary": "  sRGB images are now the predominant choice for pre-training visual models in\ncomputer vision research, owing to their ease of acquisition and efficient\nstorage. Meanwhile, the advantage of RAW images lies in their rich physical\ninformation under variable real-world challenging lighting conditions. For\ncomputer vision tasks directly based on camera RAW data, most existing studies\nadopt methods of integrating image signal processor (ISP) with backend\nnetworks, yet often overlook the interaction capabilities between the ISP\nstages and subsequent networks. Drawing inspiration from ongoing adapter\nresearch in NLP and CV areas, we introduce RAW-Adapter, a novel approach aimed\nat adapting sRGB pre-trained models to camera RAW data. RAW-Adapter comprises\ninput-level adapters that employ learnable ISP stages to adjust RAW inputs, as\nwell as model-level adapters to build connections between ISP stages and\nsubsequent high-level networks. Additionally, RAW-Adapter is a general\nframework that could be used in various computer vision frameworks. Abundant\nexperiments under different lighting conditions have shown our algorithm's\nstate-of-the-art (SOTA) performance, demonstrating its effectiveness and\nefficiency across a range of real-world and synthetic datasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.14802v1.pdf",
        "similarity": 0.2737685819547314,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-27"
    },
    {
        "new_title": "Perceive-IR: Learning to Perceive Degradation Better for All-in-One\n  Image Restoration",
        "new_link": "http://arxiv.org/abs/2408.15994v1",
        "new_summary": "  The limitations of task-specific and general image restoration methods for\nspecific degradation have prompted the development of all-in-one image\nrestoration techniques. However, the diversity of patterns among multiple\ndegradation, along with the significant uncertainties in mapping between\ndegraded images of different severities and their corresponding undistorted\nversions, pose significant challenges to the all-in-one restoration tasks. To\naddress these challenges, we propose Perceive-IR, an all-in-one image restorer\ndesigned to achieve fine-grained quality control that enables restored images\nto more closely resemble their undistorted counterparts, regardless of the type\nor severity of degradation. Specifically, Perceive-IR contains two stages: (1)\nprompt learning stage and (2) restoration stage. In the prompt learning stage,\nwe leverage prompt learning to acquire a fine-grained quality perceiver capable\nof distinguishing three-tier quality levels by constraining the prompt-image\nsimilarity in the CLIP perception space. Subsequently, this quality perceiver\nand difficulty-adaptive perceptual loss are integrated as a quality-aware\nlearning strategy to realize fine-grained quality control in restoration stage.\nFor the restoration stage, a semantic guidance module (SGM) and compact feature\nextraction (CFE) are proposed to further promote the restoration process by\nutilizing the robust semantic information from the pre-trained large scale\nvision models and distinguishing degradation-specific features. Extensive\nexperiments demonstrate that our Perceive-IR outperforms state-of-the-art\nmethods in all-in-one image restoration tasks and exhibit superior\ngeneralization ability when dealing with unseen tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15994v1.pdf",
        "similarity": 0.2724778001394316,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Q-MRS: A Deep Learning Framework for Quantitative Magnetic Resonance\n  Spectra Analysis",
        "new_link": "http://arxiv.org/abs/2408.15999v1",
        "new_summary": "  Magnetic resonance spectroscopy (MRS) is an established technique for\nstudying tissue metabolism, particularly in central nervous system disorders.\nWhile powerful and versatile, MRS is often limited by challenges associated\nwith data quality, processing, and quantification. Existing MRS quantification\nmethods face difficulties in balancing model complexity and reproducibility\nduring spectral modeling, often falling into the trap of either\noversimplification or over-parameterization. To address these limitations, this\nstudy introduces a deep learning (DL) framework that employs transfer learning,\nin which the model is pre-trained on simulated datasets before it undergoes\nfine-tuning on in vivo data. The proposed framework showed promising\nperformance when applied to the Philips dataset from the BIG GABA repository\nand represents an exciting advancement in MRS data analysis.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15999v1.pdf",
        "similarity": 0.271571536832461,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Persuasion Games using Large Language Models",
        "new_link": "http://arxiv.org/abs/2408.15879v1",
        "new_summary": "  Large Language Models (LLMs) have emerged as formidable instruments capable\nof comprehending and producing human-like text. This paper explores the\npotential of LLMs, to shape human perspectives and subsequently influence their\ndecisions on particular tasks. This capability finds applications in diverse\ndomains such as Investment, Credit cards and Insurance, wherein they assist\nusers in selecting appropriate insurance policies, investment plans, Credit\ncards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of\nagents operate in collaborative manner. The primary agent engages directly with\nusers through persuasive dialogue, while the auxiliary agents perform tasks\nsuch as information retrieval, response analysis, development of persuasion\nstrategies, and validation of facts. Empirical evidence from our experiments\ndemonstrates that this collaborative methodology significantly enhances the\npersuasive efficacy of the LLM. We analyze user resistance to persuasive\nefforts continuously and counteract it by employing a combination of rule-based\nand LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance,\nbanking, and retail domains to evaluate the proficiency of large language\nmodels (LLMs) in recognizing, adjusting to, and influencing various personality\ntypes. Concurrently, we examine the resistance mechanisms employed by LLM\nsimulated personas. Persuasion is quantified via measurable surveys before and\nafter interaction, LLM-generated scores on conversation, and user decisions\n(purchase or non-purchase).\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15879v1.pdf",
        "similarity": 0.2708033395491963,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining",
        "new_link": "http://arxiv.org/abs/2408.11294v1",
        "new_summary": "  The field of Natural Language Processing (NLP) has seen significant\nadvancements with the development of Large Language Models (LLMs). However,\nmuch of this research remains focused on English, often overlooking\nlow-resource languages like Korean. This oversight presents challenges due to\nthe unique non-alphabetic token structure of Korean and the substantial memory\nand computational demands required for LLM training, which frequently lead to\nmemory constraints and out-of-memory errors. To address these issues, we\npresent RedWhale, a model specifically tailored for Korean language processing.\nRedWhale is developed using an efficient continual pretraining approach that\nincludes a comprehensive Korean corpus preprocessing pipeline, a specialized\ntokenizer, an optimized model initialization technique, and a multistage\npretraining strategy. These innovations collectively reduce training time and\ncomputational costs while maintaining high levels of accuracy and\ncomprehension. By leveraging cross-lingual transfer learning, which exploits\nshared linguistic similarities across languages, RedWhale builds on English\nmodels to enhance Korean language processing. Experimental results demonstrate\nthat RedWhale outperforms other leading models on Korean NLP benchmarks,\nincluding the Korean Balanced Evaluation of Significant Tasks (KoBEST), showing\nsuperior understanding and generation of Korean text. Furthermore, RedWhale\nshowed no signs of convergence even after pretraining on 9.7 billion tokens,\nindicating the potential for further improvements with additional training.\nThis work represents a significant advancement in bridging the linguistic\ndivide, particularly in enhancing NLP capabilities for the Korean language.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11294v1.pdf",
        "similarity": 0.26994356504325745,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "Towards reliable respiratory disease diagnosis based on cough sounds and\n  vision transformers",
        "new_link": "http://arxiv.org/abs/2408.15667v1",
        "new_summary": "  Recent advancements in deep learning techniques have sparked performance\nboosts in various real-world applications including disease diagnosis based on\nmulti-modal medical data. Cough sound data-based respiratory disease (e.g.,\nCOVID-19 and Chronic Obstructive Pulmonary Disease) diagnosis has also\nattracted much attention. However, existing works usually utilise traditional\nmachine learning or deep models of moderate scales. On the other hand, the\ndeveloped approaches are trained and evaluated on small-scale data due to the\ndifficulty of curating and annotating clinical data on scale. To address these\nissues in prior works, we create a unified framework to evaluate various deep\nmodels from lightweight Convolutional Neural Networks (e.g., ResNet18) to\nmodern vision transformers and compare their performance in respiratory disease\nclassification. Based on the observations from such an extensive empirical\nstudy, we propose a novel approach to cough-based disease classification based\non both self-supervised and supervised learning on a large-scale cough data\nset. Experimental results demonstrate our proposed approach outperforms prior\narts consistently on two benchmark datasets for COVID-19 diagnosis and a\nproprietary dataset for COPD/non-COPD classification with an AUROC of 92.5%.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15667v1.pdf",
        "similarity": 0.2699083768724555,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders",
        "new_link": "http://arxiv.org/abs/2408.15998v1",
        "new_summary": "  The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\nModels and code: https://github.com/NVlabs/Eagle\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15998v1.pdf",
        "similarity": 0.26840224290808123,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil\n  Generation",
        "new_link": "http://arxiv.org/abs/2408.15898v1",
        "new_summary": "  The design of aerodynamic shapes, such as airfoils, has traditionally\nrequired significant computational resources and relied on predefined design\nparameters, which limit the potential for novel shape synthesis. In this work,\nwe introduce a data-driven methodology for airfoil generation using a diffusion\nmodel. Trained on a dataset of preexisting airfoils, our model can generate an\narbitrary number of new airfoils from random vectors, which can be conditioned\non specific aerodynamic performance metrics such as lift and drag, or geometric\ncriteria. Our results demonstrate that the diffusion model effectively produces\nairfoil shapes with realistic aerodynamic properties, offering substantial\nimprovements in efficiency, flexibility, and the potential for discovering\ninnovative airfoil designs. This approach significantly expands the design\nspace, facilitating the synthesis of high-performance aerodynamic shapes that\ntranscend the limitations of traditional methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15898v1.pdf",
        "similarity": 0.26809983931777265,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Generating Binary Species Range Maps",
        "new_link": "http://arxiv.org/abs/2408.15956v1",
        "new_summary": "  Accurately predicting the geographic ranges of species is crucial for\nassisting conservation efforts. Traditionally, range maps were manually created\nby experts. However, species distribution models (SDMs) and, more recently,\ndeep learning-based variants offer a potential automated alternative. Deep\nlearning-based SDMs generate a continuous probability representing the\npredicted presence of a species at a given location, which must be binarized by\nsetting per-species thresholds to obtain binary range maps. However, selecting\nappropriate per-species thresholds to binarize these predictions is non-trivial\nas different species can require distinct thresholds. In this work, we evaluate\ndifferent approaches for automatically identifying the best thresholds for\nbinarizing range maps using presence-only data. This includes approaches that\nrequire the generation of additional pseudo-absence data, along with ones that\nonly require presence data. We also propose an extension of an existing\npresence-only technique that is more robust to outliers. We perform a detailed\nevaluation of different thresholding techniques on the tasks of binary range\nestimation and large-scale fine-grained visual classification, and we\ndemonstrate improved performance over existing pseudo-absence free approaches\nusing our method.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15956v1.pdf",
        "similarity": 0.2666598484216484,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Paired Completion: Flexible Quantification of Issue-framing at Scale\n  with LLMs",
        "new_link": "http://arxiv.org/abs/2408.09742v1",
        "new_summary": "  Detecting and quantifying issue framing in textual discourse - the\nperspective one takes to a given topic (e.g. climate science vs. denialism,\nmisogyny vs. gender equality) - is highly valuable to a range of end-users from\nsocial and political scientists to program evaluators and policy analysts.\nHowever, conceptual framing is notoriously challenging for automated natural\nlanguage processing (NLP) methods since the words and phrases used by either\n`side' of an issue are often held in common, with only subtle stylistic\nflourishes separating their use. Here we develop and rigorously evaluate new\ndetection methods for issue framing and narrative analysis within large text\ndatasets. By introducing a novel application of next-token log probabilities\nderived from generative large language models (LLMs) we show that issue framing\ncan be reliably and efficiently detected in large corpora with only a few\nexamples of either perspective on a given issue, a method we call `paired\ncompletion'. Through 192 independent experiments over three novel, synthetic\ndatasets, we evaluate paired completion against prompt-based LLM methods and\nlabelled methods using traditional NLP and recent LLM contextual embeddings. We\nadditionally conduct a cost-based analysis to mark out the feasible set of\nperformant methods at production-level scales, and a model bias analysis.\nTogether, our work demonstrates a feasible path to scalable, accurate and\nlow-bias issue-framing in large corpora.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.09742v1.pdf",
        "similarity": 0.26640900617997954,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-19"
    },
    {
        "new_title": "Revisiting the Exit from Nuclear Energy in Germany with NLP",
        "new_link": "http://arxiv.org/abs/2408.13810v1",
        "new_summary": "  Annotation of political discourse is resource-intensive, but recent\ndevelopments in NLP promise to automate complex annotation tasks. Fine-tuned\ntransformer-based models outperform human annotators in some annotation tasks,\nbut they require large manually annotated training datasets. In our\ncontribution, we explore to which degree a manually annotated dataset can be\nautomatically replicated with today's NLP methods, using unsupervised machine\nlearning and zero- and few-shot learning.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.13810v1.pdf",
        "similarity": 0.26485403928017515,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-25"
    },
    {
        "new_title": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough",
        "new_link": "http://arxiv.org/abs/2408.15793v1",
        "new_summary": "  We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15793v1.pdf",
        "similarity": 0.26470699190401226,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature",
        "new_link": "http://arxiv.org/abs/2408.15836v1",
        "new_summary": "  The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15836v1.pdf",
        "similarity": 0.2643520446388969,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "BioMamba: A Pre-trained Biomedical Language Representation Model\n  Leveraging Mamba",
        "new_link": "http://arxiv.org/abs/2408.02600v1",
        "new_summary": "  The advancement of natural language processing (NLP) in biology hinges on\nmodels' ability to interpret intricate biomedical literature. Traditional\nmodels often struggle with the complex and domain-specific language in this\nfield. In this paper, we present BioMamba, a pre-trained model specifically\ndesigned for biomedical text mining. BioMamba builds upon the Mamba\narchitecture and is pre-trained on an extensive corpus of biomedical\nliterature. Our empirical studies demonstrate that BioMamba significantly\noutperforms models like BioBERT and general-domain Mamba across various\nbiomedical tasks. For instance, BioMamba achieves a 100 times reduction in\nperplexity and a 4 times reduction in cross-entropy loss on the BioASQ test\nset. We provide an overview of the model architecture, pre-training process,\nand fine-tuning techniques. Additionally, we release the code and trained model\nto facilitate further research.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02600v1.pdf",
        "similarity": 0.2635902702563841,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-05"
    },
    {
        "new_title": "Distributional Properties of Subword Regularization",
        "new_link": "http://arxiv.org/abs/2408.11443v1",
        "new_summary": "  Subword regularization, used widely in NLP, improves model performance by\nreducing the dependency on exact tokenizations, augmenting the training corpus,\nand exposing the model to more unique contexts during training. BPE and\nMaxMatch, two popular subword tokenization schemes, have stochastic dropout\nregularization variants. However, there has not been an analysis of the\ndistributions formed by them. We show that these stochastic variants are\nheavily biased towards a small set of tokenizations per word. If the benefits\nof subword regularization are as mentioned, we hypothesize that biasedness\nartificially limits the effectiveness of these schemes. Thus, we propose an\nalgorithm to uniformly sample tokenizations that we use as a drop-in\nreplacement for the stochastic aspects of existing tokenizers, and find that it\nimproves machine translation quality.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11443v1.pdf",
        "similarity": 0.26260596800244596,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "CoRe: Context-Regularized Text Embedding Learning for Text-to-Image\n  Personalization",
        "new_link": "http://arxiv.org/abs/2408.15914v1",
        "new_summary": "  Recent advances in text-to-image personalization have enabled high-quality\nand controllable image synthesis for user-provided concepts. However, existing\nmethods still struggle to balance identity preservation with text alignment.\nOur approach is based on the fact that generating prompt-aligned images\nrequires a precise semantic understanding of the prompt, which involves\naccurately processing the interactions between the new concept and its\nsurrounding context tokens within the CLIP text encoder. To address this, we\naim to embed the new concept properly into the input embedding space of the\ntext encoder, allowing for seamless integration with existing tokens. We\nintroduce Context Regularization (CoRe), which enhances the learning of the new\nconcept's text embedding by regularizing its context tokens in the prompt. This\nis based on the insight that appropriate output vectors of the text encoder for\nthe context tokens can only be achieved if the new concept's text embedding is\ncorrectly learned. CoRe can be applied to arbitrary prompts without requiring\nthe generation of corresponding images, thus improving the generalization of\nthe learned text embedding. Additionally, CoRe can serve as a test-time\noptimization technique to further enhance the generations for specific prompts.\nComprehensive experiments demonstrate that our method outperforms several\nbaseline methods in both identity preservation and text alignment. Code will be\nmade publicly available.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15914v1.pdf",
        "similarity": 0.2623127790386683,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Interactive Agents: Simulating Counselor-Client Psychological Counseling\n  via Role-Playing LLM-to-LLM Interactions",
        "new_link": "http://arxiv.org/abs/2408.15787v1",
        "new_summary": "  Virtual counselors powered by large language models (LLMs) aim to create\ninteractive support systems that effectively assist clients struggling with\nmental health challenges. To replicate counselor-client conversations,\nresearchers have built an online mental health platform that allows\nprofessional counselors to provide clients with text-based counseling services\nfor about an hour per session. Notwithstanding its effectiveness, challenges\nexist as human annotation is time-consuming, cost-intensive, privacy-protected,\nand not scalable. To address this issue and investigate the applicability of\nLLMs in psychological counseling conversation simulation, we propose a\nframework that employs two LLMs via role-playing for simulating\ncounselor-client interactions. Our framework involves two LLMs, one acting as a\nclient equipped with a specific and real-life user profile and the other\nplaying the role of an experienced counselor, generating professional responses\nusing integrative therapy techniques. We implement both the counselor and the\nclient by zero-shot prompting the GPT-4 model. In order to assess the\neffectiveness of LLMs in simulating counselor-client interactions and\nunderstand the disparities between LLM- and human-generated conversations, we\nevaluate the synthetic data from various perspectives. We begin by assessing\nthe client's performance through automatic evaluations. Next, we analyze and\ncompare the disparities between dialogues generated by the LLM and those\ngenerated by professional counselors. Furthermore, we conduct extensive\nexperiments to thoroughly examine the performance of our LLM-based counselor\ntrained with synthetic interactive dialogues by benchmarking against\nstate-of-the-art models for mental health.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15787v1.pdf",
        "similarity": 0.2614528007210875,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Zero-Shot Learning and Key Points Are All You Need for Automated\n  Fact-Checking",
        "new_link": "http://arxiv.org/abs/2408.08400v1",
        "new_summary": "  Automated fact-checking is an important task because determining the accurate\nstatus of a proposed claim within the vast amount of information available\nonline is a critical challenge. This challenge requires robust evaluation to\nprevent the spread of false information. Modern large language models (LLMs)\nhave demonstrated high capability in performing a diverse range of Natural\nLanguage Processing (NLP) tasks. By utilizing proper prompting strategies,\ntheir versatility due to their understanding of large context sizes and\nzero-shot learning ability enables them to simulate human problem-solving\nintuition and move towards being an alternative to humans for solving problems.\nIn this work, we introduce a straightforward framework based on Zero-Shot\nLearning and Key Points (ZSL-KeP) for automated fact-checking, which despite\nits simplicity, performed well on the AVeriTeC shared task dataset by robustly\nimproving the baseline and achieving 10th place.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.08400v1.pdf",
        "similarity": 0.260406137653146,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-15"
    },
    {
        "new_title": "Surrogate Models studies for laser-plasma accelerator electron source\n  design through numerical optimisation",
        "new_link": "http://arxiv.org/abs/2408.15845v1",
        "new_summary": "  The optimisation of the plasma target design for high quality beam\nlaser-driven plasma injector electron source relies on numerical parametric\nstudies using Particle in Cell (PIC) codes. The common input parameters to\nexplore are laser characteristics and plasma density profiles extracted from\ncomputational fluid dynamic studies compatible with experimental measurements\nof target plasma density profiles. We demonstrate the construction of surrogate\nmodels using machine learning technique for a laser-plasma injector (LPI)\nelectron source based on more than 12000 simulations of a laser wakefield\nacceleration performed for sparsely spaced input parameters [1]. Surrogate\nmodels are very interesting for LPI design and optimisation because they are\nmuch faster than PIC simulations. We develop and compare the performance of\nthree surrogate models, namely, Gaussian processes (GP), multilayer perceptron\n(MLP), and decision trees (DT). We then use the best surrogate model to quickly\nfind optimal working points to get a selected electron beam energy, charge and\nenergy spread using different methods, namely random search, Bayesian\noptimisation and multi-objective Bayesian optimisation\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15845v1.pdf",
        "similarity": 0.26011898002713707,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "A Review of Transformer-Based Models for Computer Vision Tasks:\n  Capturing Global Context and Spatial Relationships",
        "new_link": "http://arxiv.org/abs/2408.15178v1",
        "new_summary": "  Transformer-based models have transformed the landscape of natural language\nprocessing (NLP) and are increasingly applied to computer vision tasks with\nremarkable success. These models, renowned for their ability to capture\nlong-range dependencies and contextual information, offer a promising\nalternative to traditional convolutional neural networks (CNNs) in computer\nvision. In this review paper, we provide an extensive overview of various\ntransformer architectures adapted for computer vision tasks. We delve into how\nthese models capture global context and spatial relationships in images,\nempowering them to excel in tasks such as image classification, object\ndetection, and segmentation. Analyzing the key components, training\nmethodologies, and performance metrics of transformer-based models, we\nhighlight their strengths, limitations, and recent advancements. Additionally,\nwe discuss potential research directions and applications of transformer-based\nmodels in computer vision, offering insights into their implications for future\nadvancements in the field.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15178v1.pdf",
        "similarity": 0.2599415591392118,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-27"
    },
    {
        "new_title": "Improving Speech Recognition Error Prediction for Modern and\n  Off-the-shelf Speech Recognizers",
        "new_link": "http://arxiv.org/abs/2408.11258v1",
        "new_summary": "  Modeling the errors of a speech recognizer can help simulate errorful\nrecognized speech data from plain text, which has proven useful for tasks like\ndiscriminative language modeling, improving robustness of NLP systems, where\nlimited or even no audio data is available at train time. Previous work\ntypically considered replicating behavior of GMM-HMM based systems, but the\nbehavior of more modern posterior-based neural network acoustic models is not\nthe same and requires adjustments to the error prediction model. In this work,\nwe extend a prior phonetic confusion based model for predicting speech\nrecognition errors in two ways: first, we introduce a sampling-based paradigm\nthat better simulates the behavior of a posterior-based acoustic model. Second,\nwe investigate replacing the confusion matrix with a sequence-to-sequence model\nin order to introduce context dependency into the prediction. We evaluate the\nerror predictors in two ways: first by predicting the errors made by a\nSwitchboard ASR system on unseen data (Fisher), and then using that same\npredictor to estimate the behavior of an unrelated cloud-based ASR system on a\nnovel task. Sampling greatly improves predictive accuracy within a 100-guess\nparadigm, while the sequence model performs similarly to the confusion matrix.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11258v1.pdf",
        "similarity": 0.2594801424836437,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "Creating Arabic LLM Prompts at Scale",
        "new_link": "http://arxiv.org/abs/2408.05882v1",
        "new_summary": "  The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.05882v1.pdf",
        "similarity": 0.25947574253741224,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-12"
    },
    {
        "new_title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
        "new_link": "http://arxiv.org/abs/2408.15966v1",
        "new_summary": "  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15966v1.pdf",
        "similarity": 0.2585588234514865,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "A Quantum-Inspired Analysis of Human Disambiguation Processes",
        "new_link": "http://arxiv.org/abs/2408.07402v1",
        "new_summary": "  Formal languages are essential for computer programming and are constructed\nto be easily processed by computers. In contrast, natural languages are much\nmore challenging and instigated the field of Natural Language Processing (NLP).\nOne major obstacle is the ubiquity of ambiguities. Recent advances in NLP have\nled to the development of large language models, which can resolve ambiguities\nwith high accuracy. At the same time, quantum computers have gained much\nattention in recent years as they can solve some computational problems faster\nthan classical computers. This new computing paradigm has reached the fields of\nmachine learning and NLP, where hybrid classical-quantum learning algorithms\nhave emerged. However, more research is needed to identify which NLP tasks\ncould benefit from a genuine quantum advantage. In this thesis, we applied\nformalisms arising from foundational quantum mechanics, such as contextuality\nand causality, to study ambiguities arising from linguistics. By doing so, we\nalso reproduced psycholinguistic results relating to the human disambiguation\nprocess. These results were subsequently used to predict human behaviour and\noutperformed current NLP methods.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.07402v1.pdf",
        "similarity": 0.2583667269166301,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-14"
    },
    {
        "new_title": "A Structure-aware Generative Model for Biomedical Event Extraction",
        "new_link": "http://arxiv.org/abs/2408.06583v4",
        "new_summary": "  Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute to over 20% of the events in the\nbenchmark datasets. In this paper, we propose an event structure-aware\ngenerative model named GenBEE, which can capture complex event structures in\nbiomedical text for biomedical event extraction. In particular, GenBEE\nconstructs event prompts that distill knowledge from LLMs for incorporating\nboth label semantics and argument dependency relationships into the proposed\nmodel. In addition, GenBEE also generates prefixes with event structural\nprompts to incorporate structural features for improving the model's overall\nperformance. We have evaluated the proposed GenBEE model on three widely used\nbiomedical event extraction benchmark datasets, namely MLEE, GE11, and PHEE.\nExperimental results show that GenBEE has achieved state-of-the-art performance\non the MLEE and GE11 datasets, and achieved competitive results when compared\nto the state-of-the-art classification-based models on the PHEE dataset.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.06583v4.pdf",
        "similarity": 0.25499335711914084,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-13"
    },
    {
        "new_title": "Convexity-based Pruning of Speech Representation Models",
        "new_link": "http://arxiv.org/abs/2408.11858v1",
        "new_summary": "  Speech representation models based on the transformer architecture and\ntrained by self-supervised learning have shown great promise for solving tasks\nsuch as speech and speaker recognition, keyword spotting, emotion detection,\nand more. Typically, it is found that larger models lead to better performance.\nHowever, the significant computational effort involved in such large\ntransformer systems is a challenge for embedded and real-world applications.\nRecent work has shown that there is significant redundancy in the transformer\nmodels for NLP and massive layer pruning is feasible (Sajjad et al., 2023).\nHere, we investigate layer pruning in audio models. We base the pruning\ndecision on a convexity criterion. Convexity of classification regions has\nrecently been proposed as an indicator of subsequent fine-tuning performance in\na range of application domains, including NLP and audio. In empirical\ninvestigations, we find a massive reduction in the computational effort with no\nloss of performance or even improvements in certain cases.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11858v1.pdf",
        "similarity": 0.2529859334725413,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-16"
    },
    {
        "new_title": "Design Proteins Using Large Language Models: Enhancements and\n  Comparative Analyses",
        "new_link": "http://arxiv.org/abs/2408.06396v1",
        "new_summary": "  Pre-trained LLMs have demonstrated substantial capabilities across a range of\nconventional natural language processing (NLP) tasks, such as summarization and\nentity recognition. In this paper, we explore the application of LLMs in the\ngeneration of high-quality protein sequences. Specifically, we adopt a suite of\npre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and\ngemma-7B4, to produce valid protein sequences. All of these models are publicly\navailable.5 Unlike previous work in this field, our approach utilizes a\nrelatively small dataset comprising 42,000 distinct human protein sequences. We\nretrain these models to process protein-related data, ensuring the generation\nof biologically feasible protein structures. Our findings demonstrate that even\nwith limited data, the adapted models exhibit efficiency comparable to\nestablished protein-focused models such as ProGen varieties, ProtGPT2, and\nProLLaMA, which were trained on millions of protein sequences. To validate and\nquantify the performance of our models, we conduct comparative analyses\nemploying standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore,\nwe commit to making the trained versions of all four models publicly available,\nfostering greater transparency and collaboration in the field of computational\nbiology.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.06396v1.pdf",
        "similarity": 0.2517463523817552,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-12"
    },
    {
        "new_title": "A Survey on Facial Expression Recognition of Static and Dynamic Emotions",
        "new_link": "http://arxiv.org/abs/2408.15777v1",
        "new_summary": "  Facial expression recognition (FER) aims to analyze emotional states from\nstatic images and dynamic sequences, which is pivotal in enhancing\nanthropomorphic communication among humans, robots, and digital avatars by\nleveraging AI technologies. As the FER field evolves from controlled laboratory\nenvironments to more complex in-the-wild scenarios, advanced methods have been\nrapidly developed and new challenges and apporaches are encounted, which are\nnot well addressed in existing reviews of FER. This paper offers a\ncomprehensive survey of both image-based static FER (SFER) and video-based\ndynamic FER (DFER) methods, analyzing from model-oriented development to\nchallenge-focused categorization. We begin with a critical comparison of recent\nreviews, an introduction to common datasets and evaluation criteria, and an\nin-depth workflow on FER to establish a robust research foundation. We then\nsystematically review representative approaches addressing eight main\nchallenges in SFER (such as expression disturbance, uncertainties, compound\nemotions, and cross-domain inconsistency) as well as seven main challenges in\nDFER (such as key frame sampling, expression intensity variations, and\ncross-modal alignment). Additionally, we analyze recent advancements, benchmark\nperformances, major applications, and ethical considerations. Finally, we\npropose five promising future directions and development trends to guide\nongoing research. The project page for this paper can be found at\nhttps://github.com/wangyanckxx/SurveyFER.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15777v1.pdf",
        "similarity": 0.24984266867601987,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "CAPER: Enhancing Career Trajectory Prediction using Temporal Knowledge\n  Graph and Ternary Relationship",
        "new_link": "http://arxiv.org/abs/2408.15620v1",
        "new_summary": "  The problem of career trajectory prediction (CTP) aims to predict one's\nfuture employer or job position. While several CTP methods have been developed\nfor this problem, we posit that none of these methods (1) jointly considers the\nmutual ternary dependency between three key units (i.e., user, position, and\ncompany) of a career and (2) captures the characteristic shifts of key units in\ncareer over time, leading to an inaccurate understanding of the job movement\npatterns in the labor market. To address the above challenges, we propose a\nnovel solution, named as CAPER, that solves the challenges via sophisticated\ntemporal knowledge graph (TKG) modeling. It enables the utilization of a\ngraph-structured knowledge base with rich expressiveness, effectively\npreserving the changes in job movement patterns. Furthermore, we devise an\nextrapolated career reasoning task on TKG for a realistic evaluation. The\nexperiments on a real-world career trajectory dataset demonstrate that CAPER\nconsistently and significantly outperforms four baselines, two recent TKG\nreasoning methods, and five state-of-the-art CTP methods in predicting one's\nfuture companies and positions-i.e., on average, yielding 6.80% and 34.58% more\naccurate predictions, respectively.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15620v1.pdf",
        "similarity": 0.24977294939393252,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?",
        "new_link": "http://arxiv.org/abs/2408.11243v2",
        "new_summary": "  Self-supervised learning~(SSL) is essential to obtain foundation models in\nNLP and CV domains via effectively leveraging knowledge in large-scale\nunlabeled data. The reason for its success is that a suitable SSL design can\nhelp the model to follow the neural scaling law, i.e., the performance\nconsistently improves with increasing model and dataset sizes. However, it\nremains a mystery whether existing SSL in the graph domain can follow the\nscaling behavior toward building Graph Foundation Models~(GFMs) with\nlarge-scale pre-training. In this study, we examine whether existing graph SSL\ntechniques can follow the neural scaling behavior with the potential to serve\nas the essential component for GFMs. Our benchmark includes comprehensive SSL\ntechnique implementations with analysis conducted on both the conventional SSL\nsetting and many new settings adopted in other domains. Surprisingly, despite\nthe SSL loss continuously decreasing, no existing graph SSL techniques follow\nthe neural scaling behavior on the downstream performance. The model\nperformance only merely fluctuates on different data scales and model scales.\nInstead of the scales, the key factors influencing the performance are the\nchoices of model architecture and pretext task design. This paper examines\nexisting SSL techniques for the feasibility of Graph SSL techniques in\ndeveloping GFMs and opens a new direction for graph SSL design with the new\nevaluation prototype. Our code implementation is available online to ease\nreproducibility on https://github.com/GraphSSLScaling/GraphSSLScaling.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11243v2.pdf",
        "similarity": 0.24737376868538927,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-20"
    },
    {
        "new_title": "TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based\n  Computing",
        "new_link": "http://arxiv.org/abs/2408.12742v1",
        "new_summary": "  Due to the high computation overhead of Vision Transformers (ViTs), In-memory\nComputing architectures are being researched towards energy-efficient\ndeployment in edge-computing scenarios. Prior works have proposed efficient\nalgorithm-hardware co-design and IMC-architectural improvements to improve the\nenergy-efficiency of IMC-implemented ViTs. However, all prior works have\nneglected the overhead and co-depencence of attention blocks on the\naccuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose\nTReX- an attention-reuse-driven ViT optimization framework that effectively\nperforms attention reuse in ViT models to achieve optimal\naccuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer\nencoders for attention reuse to achieve near iso-accuracy performance while\nmeeting the user-specified delay requirement. Based on our analysis on the\nImagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and\n1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S\n(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP\nreduction compared to state-of-the-art token pruning and weight sharing\napproaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal\naccuracy compared to baseline at 1.6x lower EDAP.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.12742v1.pdf",
        "similarity": 0.24522597467241194,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-22"
    },
    {
        "new_title": "Implicit Regularization Paths of Weighted Neural Representations",
        "new_link": "http://arxiv.org/abs/2408.15784v1",
        "new_summary": "  We study the implicit regularization effects induced by (observation)\nweighting of pretrained features. For weight and feature matrices of bounded\noperator norms that are infinitesimally free with respect to (normalized) trace\nfunctionals, we derive equivalence paths connecting different weighting\nmatrices and ridge regularization levels. Specifically, we show that ridge\nestimators trained on weighted features along the same path are asymptotically\nequivalent when evaluated against test vectors of bounded norms. These paths\ncan be interpreted as matching the effective degrees of freedom of ridge\nestimators fitted with weighted features. For the special case of subsampling\nwithout replacement, our results apply to independently sampled random features\nand kernel features and confirm recent conjectures (Conjectures 7 and 8) of the\nauthors on the existence of such paths in Patil et al. We also present an\nadditive risk decomposition for ensembles of weighted estimators and show that\nthe risks are equivalent along the paths when the ensemble size goes to\ninfinity. As a practical consequence of the path equivalences, we develop an\nefficient cross-validation method for tuning and apply it to subsampled\npretrained representations across several models (e.g., ResNet-50) and datasets\n(e.g., CIFAR-100).\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15784v1.pdf",
        "similarity": 0.2440849172200522,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "A Stochastic Robust Adaptive Systems Level Approach to Stabilizing\n  Large-Scale Uncertain Markovian Jump Linear Systems",
        "new_link": "http://arxiv.org/abs/2408.15789v1",
        "new_summary": "  We propose a unified framework for robustly and adaptively stabilizing\nlarge-scale networked uncertain Markovian jump linear systems (MJLS) under\nexternal disturbances and mode switches that can change the network's topology.\nAdaptation is achieved by using minimal information on the disturbance to\nidentify modes that are consistent with observable data. Robust control is\nachieved by extending the system level synthesis (SLS) approach, which allows\nus to pose the problem of simultaneously stabilizing multiple plants as a\ntwo-step convex optimization procedure. Our control pipeline computes a\nlikelihood distribution of the system's current mode, uses them as\nprobabilistic weights during simultaneous stabilization, then updates the\nlikelihood via Bayesian inference. Because of this \"softer\" probabilistic\napproach to robust stabilization, our control pipeline does not suffer from\nabrupt destabilization issues due to changes in the system's true mode, which\nwere observed in a previous method. Separability of SLS also lets us compute\nlocalized robust controllers for each subsystem, allowing for network\nscalability; we use several information consensus methods so that mode\nestimation can also be done locally. We apply our algorithms to\ndisturbance-rejection on two sample dynamic power grid networks, a small-scale\nsystem with 7 nodes and a large-scale grid of 25 nodes.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15789v1.pdf",
        "similarity": 0.2416666048755925,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble\n  Approach Using Kolmogorov-Arnold Networks",
        "new_link": "http://arxiv.org/abs/2408.15886v1",
        "new_summary": "  In recent years, the evolution of machine learning techniques has\nsignificantly impacted the field of intrusion detection, particularly within\nthe context of the Internet of Things (IoT). As IoT networks expand, the need\nfor robust security measures to counteract potential threats has become\nincreasingly critical. This paper introduces a hybrid Intrusion Detection\nSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)\nwith the XGBoost algorithm. Our proposed IDS leverages the unique capabilities\nof KANs, which utilize learnable activation functions to model complex\nrelationships within data, alongside the powerful ensemble learning techniques\nof XGBoost, known for its high performance in classification tasks. This hybrid\napproach not only enhances the detection accuracy but also improves the\ninterpretability of the model, making it suitable for dynamic and intricate IoT\nenvironments. Experimental evaluations demonstrate that our hybrid IDS achieves\nan impressive detection accuracy exceeding 99% in distinguishing between benign\nand malicious activities. Additionally, we were able to achieve F1 scores,\nprecision, and recall that exceeded 98%. Furthermore, we conduct a comparative\nanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessing\nperformance metrics such as Precision, Recall, and F1-score. The results\nunderscore the efficacy of integrating KANs with XGBoost, highlighting the\npotential of this innovative approach to significantly strengthen the security\nframework of IoT networks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15886v1.pdf",
        "similarity": 0.2416207412206543,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "mbrs: A Library for Minimum Bayes Risk Decoding",
        "new_link": "http://arxiv.org/abs/2408.04167v1",
        "new_summary": "  Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs\n",
        "pdf_link": "https://arxiv.org/pdf/2408.04167v1.pdf",
        "similarity": 0.24137497488086013,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-08"
    },
    {
        "new_title": "PTM4Tag+: Tag Recommendation of Stack Overflow Posts with Pre-trained\n  Models",
        "new_link": "http://arxiv.org/abs/2408.02311v1",
        "new_summary": "  Stack Overflow is one of the most influential Software Question & Answer\n(SQA) websites, hosting millions of programming-related questions and answers.\nTags play a critical role in efficiently organizing the contents in Stack\nOverflow and are vital to support a range of site operations, e.g., querying\nrelevant content. Poorly selected tags often raise problems like tag ambiguity\nand tag explosion. Thus, a precise and accurate automated tag recommendation\ntechnique is demanded.\n  Inspired by the recent success of pre-trained models (PTMs) in natural\nlanguage processing (NLP), we present PTM4Tag+, a tag recommendation framework\nfor Stack Overflow posts that utilizes PTMs in language modeling. PTM4Tag+ is\nimplemented with a triplet architecture, which considers three key components\nof a post, i.e., Title, Description, and Code, with independent PTMs. We\nutilize a number of popular pre-trained models, including the BERT-based models\n(e.g., BERT, RoBERTa, CodeBERT, BERTOverflow, and ALBERT), and encoder-decoder\nmodels (e.g., PLBART, CoTexT, and CodeT5). Our results show that leveraging\nCodeT5 under the PTM4Tag+ framework achieves the best performance among the\neight considered PTMs and outperforms the state-of-the-art Convolutional Neural\nNetwork-based approach by a substantial margin in terms of average P\nrecision@k, Recall@k, and F1-score@k (k ranges from 1 to 5). Specifically,\nCodeT5 improves the performance of F1-score@1-5 by 8.8%, 12.4%, 15.3%, 16.4%,\nand 16.6%. Moreover, to address the concern with inference latency, we\nexperiment PTM4Tag+ with smaller PTM models (i.e., DistilBERT, DistilRoBERTa,\nCodeBERT-small, and CodeT5-small). We find that although smaller PTMs cannot\noutperform larger PTMs, they still maintain over 93.96% of the performance on\naverage, meanwhile shortening the mean inference time by more than 47.2%\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02311v1.pdf",
        "similarity": 0.24073753124775954,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-05"
    },
    {
        "new_title": "Bridging the Gap: Unpacking the Hidden Challenges in Knowledge\n  Distillation for Online Ranking Systems",
        "new_link": "http://arxiv.org/abs/2408.14678v1",
        "new_summary": "  Knowledge Distillation (KD) is a powerful approach for compressing a large\nmodel into a smaller, more efficient model, particularly beneficial for\nlatency-sensitive applications like recommender systems. However, current KD\nresearch predominantly focuses on Computer Vision (CV) and NLP tasks,\noverlooking unique data characteristics and challenges inherent to recommender\nsystems. This paper addresses these overlooked challenges, specifically: (1)\nmitigating data distribution shifts between teacher and student models, (2)\nefficiently identifying optimal teacher configurations within time and\nbudgetary constraints, and (3) enabling computationally efficient and rapid\nsharing of teacher labels to support multiple students. We present a robust KD\nsystem developed and rigorously evaluated on multiple large-scale personalized\nvideo recommendation systems within Google. Our live experiment results\ndemonstrate significant improvements in student model performance while\nensuring consistent and reliable generation of high quality teacher labels from\na continuous data stream of data.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.14678v1.pdf",
        "similarity": 0.24060252395332968,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-26"
    },
    {
        "new_title": "Leveraging FourierKAN Classification Head for Pre-Trained\n  Transformer-based Text Classification",
        "new_link": "http://arxiv.org/abs/2408.08803v1",
        "new_summary": "  For many years, transformer-based pre-trained models with Multi-layer\nPerceptron (MLP) heads have been the standard for text classification tasks.\nHowever, the fixed non-linear functions employed by MLPs often fall short of\ncapturing the intricacies of the contextualized embeddings produced by\npre-trained encoders. Furthermore, MLPs usually require a significant number of\ntraining parameters, which can be computationally expensive. In this work, we\nintroduce FourierKAN (FR-KAN), a variant of the promising MLP alternative\ncalled Kolmogorov-Arnold Networks (KANs), as classification heads for\ntransformer-based encoders. Our studies reveal an average increase of 10% in\naccuracy and 11% in F1-score when incorporating FR-KAN heads instead of\ntraditional MLP heads for several transformer-based pre-trained models across\nmultiple text classification tasks. Beyond improving model accuracy, FR-KAN\nheads train faster and require fewer parameters. Our research opens new grounds\nfor broader applications of KAN across several Natural Language Processing\n(NLP) tasks.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.08803v1.pdf",
        "similarity": 0.23770393227718545,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-16"
    },
    {
        "new_title": "CyberPal.AI: Empowering LLMs with Expert-Driven Cybersecurity\n  Instructions",
        "new_link": "http://arxiv.org/abs/2408.09304v1",
        "new_summary": "  Large Language Models (LLMs) have significantly advanced natural language\nprocessing (NLP), providing versatile capabilities across various applications.\nHowever, their application to complex, domain-specific tasks, such as\ncyber-security, often faces substantial challenges. In this study, we introduce\nSecKnowledge and CyberPal.AI to address these challenges and train\nsecurity-expert LLMs. SecKnowledge is a domain-knowledge-driven cyber-security\ninstruction dataset, meticulously designed using years of accumulated expert\nknowledge in the domain through a multi-phase generation process. CyberPal.AI\nrefers to a family of LLMs fine-tuned using SecKnowledge, aimed at building\nsecurity-specialized LLMs capable of answering and following complex\nsecurity-related instructions. Additionally, we introduce SecKnowledge-Eval, a\ncomprehensive and diverse cyber-security evaluation benchmark, composed of an\nextensive set of cyber-security tasks we specifically developed to assess LLMs\nin the field of cyber-security, along with other publicly available security\nbenchmarks. Our results show a significant average improvement of up to 24%\nover the baseline models, underscoring the benefits of our expert-driven\ninstruction dataset generation process. These findings contribute to the\nadvancement of AI-based cyber-security applications, paving the way for\nsecurity-expert LLMs that can enhance threat-hunting and investigation\nprocesses.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.09304v1.pdf",
        "similarity": 0.23691359497519776,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-17"
    },
    {
        "new_title": "Fine-tuning multilingual language models in Twitter/X sentiment\n  analysis: a study on Eastern-European V4 languages",
        "new_link": "http://arxiv.org/abs/2408.02044v1",
        "new_summary": "  The aspect-based sentiment analysis (ABSA) is a standard NLP task with\nnumerous approaches and benchmarks, where large language models (LLM) represent\nthe current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data\nin underrepresented languages. On such narrow tasks, small tuned language\nmodels can often outperform universal large ones, providing available and cheap\nsolutions.\n  We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for\nclassification of sentiment towards Russia and Ukraine in the context of the\nongoing military conflict. The training/testing dataset was obtained from the\nacademic API from Twitter/X during 2023, narrowed to the languages of the V4\ncountries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their\nperformance under a variety of settings including translations, sentiment\ntargets, in-context learning and more, using GPT4 as a reference model. We\ndocument several interesting phenomena demonstrating, among others, that some\nmodels are much better fine-tunable on multilingual Twitter tasks than others,\nand that they can reach the SOTA level with a very small training set. Finally\nwe identify combinations of settings providing the best results.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.02044v1.pdf",
        "similarity": 0.23609701126481278,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-04"
    },
    {
        "new_title": "Claim Verification in the Age of Large Language Models: A Survey",
        "new_link": "http://arxiv.org/abs/2408.14317v1",
        "new_summary": "  The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.14317v1.pdf",
        "similarity": 0.23601536764825104,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-26"
    },
    {
        "new_title": "A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic\n  Text Classification",
        "new_link": "http://arxiv.org/abs/2408.09629v1",
        "new_summary": "  Transformer models have achieved state-of-the-art results, with Large\nLanguage Models (LLMs), an evolution of first-generation transformers (1stTR),\nbeing considered the cutting edge in several NLP tasks. However, the literature\nhas yet to conclusively demonstrate that LLMs consistently outperform 1stTRs\nacross all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and\nBART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis\ndatasets. The results indicate that open LLMs may moderately outperform or\nmatch 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this\nsubstantial cost for only moderate gains, the practical applicability of these\nmodels in cost-sensitive scenarios is questionable. In this context, a\nconfidence-based strategy that seamlessly integrates 1stTRs with open LLMs\nbased on prediction certainty is proposed. High-confidence documents are\nclassified by the more cost-effective 1stTRs, while uncertain cases are handled\nby LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned\nversions. Experiments in sentiment analysis demonstrate that our solution not\nonly outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely\nwith fine-tuned LLMs at a fraction of the cost.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.09629v1.pdf",
        "similarity": 0.23400212202367485,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-19"
    },
    {
        "new_title": "Large Language Models are Good Attackers: Efficient and Stealthy Textual\n  Backdoor Attacks",
        "new_link": "http://arxiv.org/abs/2408.11587v1",
        "new_summary": "  With the burgeoning advancements in the field of natural language processing\n(NLP), the demand for training data has increased significantly. To save costs,\nit has become common for users and businesses to outsource the labor-intensive\ntask of data collection to third-party entities. Unfortunately, recent research\nhas unveiled the inherent risk associated with this practice, particularly in\nexposing NLP systems to potential backdoor attacks. Specifically, these attacks\nenable malicious control over the behavior of a trained model by poisoning a\nsmall portion of the training data. Unlike backdoor attacks in computer vision,\ntextual backdoor attacks impose stringent requirements for attack stealthiness.\nHowever, existing attack methods meet significant trade-off between\neffectiveness and stealthiness, largely due to the high information entropy\ninherent in textual data. In this paper, we introduce the Efficient and\nStealthy Textual backdoor attack method, EST-Bad, leveraging Large Language\nModels (LLMs). Our EST-Bad encompasses three core strategies: optimizing the\ninherent flaw of models as the trigger, stealthily injecting triggers with\nLLMs, and meticulously selecting the most impactful samples for backdoor\ninjection. Through the integration of these techniques, EST-Bad demonstrates an\nefficient achievement of competitive attack performance while maintaining\nsuperior stealthiness compared to prior methods across various text classifier\ndatasets.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11587v1.pdf",
        "similarity": 0.23005098299984011,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "A prototype-based model for set classification",
        "new_link": "http://arxiv.org/abs/2408.13720v1",
        "new_summary": "  Classification of sets of inputs (e.g., images and texts) is an active area\nof research within both computer vision (CV) and natural language processing\n(NLP). A common way to represent a set of vectors is to model them as linear\nsubspaces. In this contribution, we present a prototype-based approach for\nlearning on the manifold formed from such linear subspaces, the Grassmann\nmanifold. Our proposed method learns a set of subspace prototypes capturing the\nrepresentative characteristics of classes and a set of relevance factors\nautomating the selection of the dimensionality of the subspaces. This leads to\na transparent classifier model which presents the computed impact of each input\nvector on its decision. Through experiments on benchmark image and text\ndatasets, we have demonstrated the efficiency of our proposed classifier,\ncompared to the transformer-based models in terms of not only performance and\nexplainability but also computational resource requirements.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.13720v1.pdf",
        "similarity": 0.22993717784521342,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-25"
    },
    {
        "new_title": "ReMamba: Equip Mamba with Effective Long-Sequence Modeling",
        "new_link": "http://arxiv.org/abs/2408.15496v1",
        "new_summary": "  While the Mamba architecture demonstrates superior inference efficiency and\ncompetitive performance on short-context natural language processing (NLP)\ntasks, empirical evidence suggests its capacity to comprehend long contexts is\nlimited compared to transformer-based models. In this study, we investigate the\nlong-context efficiency issues of the Mamba models and propose ReMamba, which\nenhances Mamba's ability to comprehend long contexts. ReMamba incorporates\nselective compression and adaptation techniques within a two-stage re-forward\nprocess, incurring minimal additional inference costs overhead. Experimental\nresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,\nimproving over the baselines by 3.2 and 1.6 points, respectively, and attaining\nperformance almost on par with same-size transformer models.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15496v1.pdf",
        "similarity": 0.22927752080719438,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Leveraging Persistent Homology for Differential Diagnosis of Mild\n  Cognitive Impairment",
        "new_link": "http://arxiv.org/abs/2408.15647v1",
        "new_summary": "  Mild cognitive impairment (MCI) is characterized by subtle changes in\ncognitive functions, often associated with disruptions in brain connectivity.\nThe present study introduces a novel fine-grained analysis to examine\ntopological alterations in neurodegeneration pertaining to six different brain\nnetworks of MCI subjects (Early/Late MCI). To achieve this, fMRI time series\nfrom two distinct populations are investigated: (i) the publicly accessible\nADNI dataset and (ii) our in-house dataset. The study utilizes sliding window\nembedding to convert each fMRI time series into a sequence of 3-dimensional\nvectors, facilitating the assessment of changes in regional brain topology.\nDistinct persistence diagrams are computed for Betti descriptors of\ndimension-0, 1, and 2. Wasserstein distance metric is used to quantify\ndifferences in topological characteristics. We have examined both (i)\nROI-specific inter-subject interactions and (ii) subject-specific inter-ROI\ninteractions. Further, a new deep learning model is proposed for\nclassification, achieving a maximum classification accuracy of 95% for the ADNI\ndataset and 85% for the in-house dataset. This methodology is further adapted\nfor the differential diagnosis of MCI sub-types, resulting in a peak accuracy\nof 76.5%, 91.1% and 80% in classifying HC Vs. EMCI, HC Vs. LMCI and EMCI Vs.\nLMCI, respectively. We showed that the proposed approach surpasses current\nstate-of-the-art techniques designed for classifying MCI and its sub-types\nusing fMRI.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15647v1.pdf",
        "similarity": 0.22922702576196222,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Investigating Complex HPV Dynamics Using Emulation and History Matching",
        "new_link": "http://arxiv.org/abs/2408.15805v1",
        "new_summary": "  The study of transmission and progression of human papillomavirus (HPV) is\ncrucial for understanding the incidence of cervical cancers, and has been\nidentified as a priority worldwide. The complexity of the disease necessitates\na detailed model of HPV transmission and its progression to cancer; to infer\nproperties of the above we require a careful process that can match to\nimperfect or incomplete observational data. In this paper, we describe the\nHPVsim simulator to satisfy the former requirement; to satisfy the latter we\ncouple this stochastic simulator to a process of emulation and history matching\nusing the R package hmer. With these tools, we are able to obtain a\ncomprehensive collection of parameter combinations that could give rise to\nobserved cancer data, and explore the implications of the variability of these\nparameter sets as it relates to future health interventions.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15805v1.pdf",
        "similarity": 0.22861030374277533,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "An Empirical Study of API Misuses of Data-Centric Libraries",
        "new_link": "http://arxiv.org/abs/2408.15853v1",
        "new_summary": "  Developers rely on third-party library Application Programming Interfaces\n(APIs) when developing software. However, libraries typically come with\nassumptions and API usage constraints, whose violation results in API misuse.\nAPI misuses may result in crashes or incorrect behavior. Even though API misuse\nis a well-studied area, a recent study of API misuse of deep learning libraries\nshowed that the nature of these misuses and their symptoms are different from\nmisuses of traditional libraries, and as a result highlighted potential\nshortcomings of current misuse detection tools. We speculate that these\nobservations may not be limited to deep learning API misuses but may stem from\nthe data-centric nature of these APIs. Data-centric libraries often deal with\ndiverse data structures, intricate processing workflows, and a multitude of\nparameters, which can make them inherently more challenging to use correctly.\nTherefore, understanding the potential misuses of these libraries is important\nto avoid unexpected application behavior. To this end, this paper contributes\nan empirical study of API misuses of five data-centric libraries that cover\nareas such as data processing, numerical computation, machine learning, and\nvisualization. We identify misuses of these libraries by analyzing data from\nboth Stack Overflow and GitHub. Our results show that many of the\ncharacteristics of API misuses observed for deep learning libraries extend to\nmisuses of the data-centric library APIs we study. We also find that developers\ntend to misuse APIs from data-centric libraries, regardless of whether the API\ndirective appears in the documentation. Overall, our work exposes the\nchallenges of API misuse in data-centric libraries, rather than only focusing\non deep learning libraries. Our collected misuses and their characterization\nlay groundwork for future research to help reduce misuses of these libraries.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15853v1.pdf",
        "similarity": 0.22714523306351447,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot\n  Learning",
        "new_link": "http://arxiv.org/abs/2408.15924v1",
        "new_summary": "  Few-shot image classification is a challenging task in the field of machine\nlearning, involving the identification of new categories using a limited number\nof labeled samples. In recent years, methods based on local descriptors have\nmade significant progress in this area. However, the key to improving\nclassification accuracy lies in effectively filtering background noise and\naccurately selecting critical local descriptors highly relevant to image\ncategory information.\n  To address this challenge, we propose an innovative weighted adaptive\nthreshold filtering (WATF) strategy for local descriptors. This strategy can\ndynamically adjust based on the current task and image context, thereby\nselecting local descriptors most relevant to the image category. This enables\nthe model to better focus on category-related information while effectively\nmitigating interference from irrelevant background regions.\n  To evaluate the effectiveness of our method, we adopted the N-way K-shot\nexperimental framework. Experimental results show that our method not only\nimproves the clustering effect of selected local descriptors but also\nsignificantly enhances the discriminative ability between image categories.\nNotably, our method maintains a simple and lightweight design philosophy\nwithout introducing additional learnable parameters. This feature ensures\nconsistency in filtering capability during both training and testing phases,\nfurther enhancing the reliability and practicality of the method.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15924v1.pdf",
        "similarity": 0.22668942821589558,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Convergent Differential Privacy Analysis for General Federated Learning:\n  the f-DP Perspective",
        "new_link": "http://arxiv.org/abs/2408.15621v1",
        "new_summary": "  Federated learning (FL) is an efficient collaborative training paradigm\nextensively developed with a focus on local privacy protection, and\ndifferential privacy (DP) is a classical approach to capture and ensure the\nreliability of local privacy. The powerful cooperation of FL and DP provides a\npromising learning framework for large-scale private clients, juggling both\nprivacy securing and trustworthy learning. As the predominant algorithm of DP,\nthe noisy perturbation has been widely studied and incorporated into various\nfederated algorithms, theoretically proven to offer significant privacy\nprotections. However, existing analyses in noisy FL-DP mostly rely on the\ncomposition theorem and cannot tightly quantify the privacy leakage challenges,\nwhich is nearly tight for small numbers of communication rounds but yields an\narbitrarily loose and divergent bound under the large communication rounds.\nThis implies a counterintuitive judgment, suggesting that FL may not provide\nadequate privacy protection during long-term training. To further investigate\nthe convergent privacy and reliability of the FL-DP framework, in this paper,\nwe comprehensively evaluate the worst privacy of two classical methods under\nthe non-convex and smooth objectives based on the f-DP analysis, i.e.\nNoisy-FedAvg and Noisy-FedProx methods. With the aid of the\nshifted-interpolation technique, we successfully prove that the worst privacy\nof the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover,\nin the Noisy-FedProx method, with the regularization of the proxy term, the\nworst privacy has a stable constant lower bound. Our analysis further provides\na solid theoretical foundation for the reliability of privacy protection in\nFL-DP. Meanwhile, our conclusions can also be losslessly converted to other\nclassical DP analytical frameworks, e.g. $(\\epsilon,\\delta)$-DP and\nR$\\acute{\\text{e}}$nyi-DP (RDP).\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15621v1.pdf",
        "similarity": 0.22413260201081872,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Value Alignment from Unstructured Text",
        "new_link": "http://arxiv.org/abs/2408.10392v1",
        "new_summary": "  Aligning large language models (LLMs) to value systems has emerged as a\nsignificant area of research within the fields of AI and NLP. Currently, this\nalignment process relies on the availability of high-quality supervised and\npreference data, which can be both time-consuming and expensive to curate or\nannotate. In this paper, we introduce a systematic end-to-end methodology for\naligning LLMs to the implicit and explicit values represented in unstructured\ntext data. Our proposed approach leverages the use of scalable synthetic data\ngeneration techniques to effectively align the model to the values present in\nthe unstructured data. Through two distinct use-cases, we demonstrate the\nefficiency of our methodology on the Mistral-7B-Instruct model. Our approach\ncredibly aligns LLMs to the values embedded within documents, and shows\nimproved performance against other approaches, as quantified through the use of\nautomatic metrics and win rates.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.10392v1.pdf",
        "similarity": 0.22380665482134668,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-19"
    },
    {
        "new_title": "Improving Large Language Model (LLM) fidelity through context-aware\n  grounding: A systematic approach to reliability and veracity",
        "new_link": "http://arxiv.org/abs/2408.04023v1",
        "new_summary": "  As Large Language Models (LLMs) become increasingly sophisticated and\nubiquitous in natural language processing (NLP) applications, ensuring their\nrobustness, trustworthiness, and alignment with human values has become a\ncritical challenge. This paper presents a novel framework for contextual\ngrounding in textual models, with a particular emphasis on the Context\nRepresentation stage. Our approach aims to enhance the reliability and ethical\nalignment of these models through a comprehensive, context-aware methodology.\nBy explicitly capturing and representing relevant situational, cultural, and\nethical contexts in a machine-readable format, we lay the foundation for\nanchoring a model's behavior within these contexts. Our approach leverages\ntechniques from knowledge representation and reasoning, such as ontologies,\nsemantic web technologies, and logic-based formalisms. We evaluate our\nframework on real-world textual datasets, demonstrating its effectiveness in\nimproving model performance, fairness, and alignment with human expectations,\nwhile maintaining high accuracy. Furthermore, we discuss the other key\ncomponents of the framework, including context-aware encoding, context-aware\nlearning, interpretability and explainability, and continuous monitoring and\nadaptation. This research contributes to the growing body of work on\nresponsible AI, offering a practical approach to developing more reliable,\ntrustworthy, and ethically-aligned language models. Our findings have\nsignificant implications for the deployment of LLMs in sensitive domains such\nas healthcare, legal systems, and social services, where contextual\nunderstanding is paramount.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.04023v1.pdf",
        "similarity": 0.22090333914605578,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-07"
    },
    {
        "new_title": "SITransformer: Shared Information-Guided Transformer for Extreme\n  Multimodal Summarization",
        "new_link": "http://arxiv.org/abs/2408.15829v1",
        "new_summary": "  Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an\nattractive summarization approach by integrating various types of information\nto create extremely concise yet informative summaries for individual\nmodalities. Existing methods overlook the issue that multimodal data often\ncontains more topic irrelevant information, which can mislead the model into\nproducing inaccurate summaries especially for extremely short ones. In this\npaper, we propose SITransformer, a \\textbf{S}hared \\textbf{I}nformation-guided\n\\textbf{T}ransformer for extreme multimodal summarization. It has a shared\ninformation guided pipeline which involves a cross-modal shared information\nextractor and a cross-modal interaction module. The extractor formulates\nsemantically shared salient information from different modalities by devising a\nnovel filtering process consisting of a differentiable top-k selector and a\nshared-information guided gating unit. As a result, the common, salient, and\nrelevant contents across modalities are identified. Next, a transformer with\ncross-modal attentions is developed for intra- and inter-modality learning with\nthe shared information guidance to produce the extreme summary. Comprehensive\nexperiments demonstrate that SITransformer significantly enhances the\nsummarization quality for both video and text summaries for XMSMO. Our code\nwill be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15829v1.pdf",
        "similarity": 0.22090307187617175,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Characterization of dynamical systems with scanty data using Persistent\n  Homology and Machine Learning",
        "new_link": "http://arxiv.org/abs/2408.15834v1",
        "new_summary": "  Determination of the nature of the dynamical state of a system as a function\nof its parameters is an important problem in the study of dynamical systems.\nThis problem becomes harder in experimental systems where the obtained data is\ninadequate (low-res) or has missing values. Recent developments in the field of\ntopological data analysis have given a powerful methodology, viz. persistent\nhomology, that is particularly suited for the study of dynamical systems.\nEarlier studies have mapped the dynamical features with the topological\nfeatures of some systems. However, these mappings between the dynamical\nfeatures and the topological features are notional and inadequate for accurate\nclassification on two counts. First, the methodologies employed by the earlier\nstudies heavily relied on human validation and intervention. Second, this\nmapping done on the chaotic dynamical regime makes little sense because\nessentially the topological summaries in this regime are too noisy to extract\nmeaningful features from it. In this paper, we employ Machine Learning (ML)\nassisted methodology to minimize the human intervention and validation of\nextracting the topological summaries from the dynamical states of systems.\nFurther, we employ a metric that counts in the noisy topological summaries,\nwhich are normally discarded, to characterize the state of the dynamical system\nas periodic or chaotic. This is surprisingly different from the conventional\nmethodologies wherein only the persisting (long-lived) topological features are\ntaken into consideration while the noisy (short-lived) topological features are\nneglected. We have demonstrated our ML-assisted method on well-known systems\nsuch as the Lorentz, Duffing, and Jerk systems. And we expect that our\nmethodology will be of utility in characterizing other dynamical systems\nincluding experimental systems that are constrained with limited data.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15834v1.pdf",
        "similarity": 0.22030828664624655,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "A Model-Free Method to Quantify Memory Utilization in Neural Point\n  Processes",
        "new_link": "http://arxiv.org/abs/2408.15875v1",
        "new_summary": "  Quantifying the predictive capacity of a neural system, intended as the\ncapability to store information and actively use it for dynamic system\nevolution, is a key component of neural information processing. Information\nstorage (IS), the main measure quantifying the active utilization of memory in\na dynamic system, is only defined for discrete-time processes. While recent\ntheoretical work laid the foundations for the continuous-time analysis of the\npredictive capacity stored in a process, methods for the effective computation\nof the related measures are needed to favor widespread utilization on neural\ndata. This work introduces a method for the model-free estimation of the\nso-called memory utilization rate (MUR), the continuous-time counterpart of the\nIS, specifically designed to quantify the predictive capacity stored in neural\npoint processes. The method employs nearest-neighbor entropy estimation applied\nto the inter-spike intervals measured from point-process realizations to\nquantify the extent of memory used by a spike train. An empirical procedure\nbased on surrogate data is implemented to compensate the estimation bias and\ndetect statistically significant levels of memory. The method is validated in\nsimulated Poisson processes and in realistic models of coupled cortical\ndynamics and heartbeat dynamics. It is then applied to real spike trains\nreflecting central and autonomic nervous system activities: in spontaneously\ngrowing cortical neuron cultures, the MUR detected increasing memory\nutilization across maturation stages, associated to emergent bursting\nsynchronized activity; in the study of the neuro-autonomic modulation of human\nheartbeats, the MUR reflected the sympathetic activation occurring with\npostural but not with mental stress. The proposed approach offers a\ncomputationally reliable tool to analyze spike train data in computational\nneuroscience and physiology.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15875v1.pdf",
        "similarity": 0.2202523485420437,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in\n  Subjective Tasks?",
        "new_link": "http://arxiv.org/abs/2408.14141v1",
        "new_summary": "  Subjective tasks in NLP have been mostly relegated to objective standards,\nwhere the gold label is decided by taking the majority vote. This obfuscates\nannotator disagreement and the inherent uncertainty of the label. We argue that\nsubjectivity should factor into model decisions and play a direct role via\ncalibration under a selective prediction setting. Specifically, instead of\ncalibrating confidence purely from the model's perspective, we calibrate models\nfor subjective tasks based on crowd worker agreement. Our method,\nCrowd-Calibrator, models the distance between the distribution of crowd worker\nlabels and the model's own distribution over labels to inform whether the model\nshould abstain from a decision. On two highly subjective tasks, hate speech\ndetection and natural language inference, our experiments show Crowd-Calibrator\neither outperforms or achieves competitive performance with existing selective\nprediction baselines. Our findings highlight the value of bringing human\ndecision-making into model predictions.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.14141v1.pdf",
        "similarity": 0.21949205208683412,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-26"
    },
    {
        "new_title": "A New Method for Cross-Lingual-based Semantic Role Labeling",
        "new_link": "http://arxiv.org/abs/2408.15896v1",
        "new_summary": "  Semantic role labeling is a crucial task in natural language processing,\nenabling better comprehension of natural language. However, the lack of\nannotated data in multiple languages has posed a challenge for researchers. To\naddress this, a deep learning algorithm based on model transfer has been\nproposed. The algorithm utilizes a dataset consisting of the English portion of\nCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency\nof training, only ten percent of the educational data from each language is\nused. The results of the proposed model demonstrate significant improvements\ncompared to Niksirt et al.'s model. In monolingual mode, the proposed model\nachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,\nthe improvement was even more substantial, reaching 6.23 percent. Worth noting\nis that the compared model only trained two of the four stages of semantic role\nlabeling and employed golden data for the remaining two stages. This suggests\nthat the actual superiority of the proposed model surpasses the reported\nnumbers by a significant margin. The development of cross-lingual methods for\nsemantic role labeling holds promise, particularly in addressing the scarcity\nof annotated data for various languages. These advancements pave the way for\nfurther research in understanding and processing natural language across\ndifferent linguistic contexts.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15896v1.pdf",
        "similarity": 0.2184043822024524,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Training an NLP Scholar at a Small Liberal Arts College: A Backwards\n  Designed Course Proposal",
        "new_link": "http://arxiv.org/abs/2408.05664v1",
        "new_summary": "  The rapid growth in natural language processing (NLP) over the last couple\nyears has generated student interest and excitement in learning more about the\nfield. In this paper, we present two types of students that NLP courses might\nwant to train. First, an \"NLP engineer\" who is able to flexibly design, build\nand apply new technologies in NLP for a wide range of tasks. Second, an \"NLP\nscholar\" who is able to pose, refine and answer questions in NLP and how it\nrelates to the society, while also learning to effectively communicate these\nanswers to a broader audience. While these two types of skills are not mutually\nexclusive -- NLP engineers should be able to think critically, and NLP scholars\nshould be able to build systems -- we think that courses can differ in the\nbalance of these skills. As educators at Small Liberal Arts Colleges, the\nstrengths of our students and our institution favors an approach that is better\nsuited to train NLP scholars. In this paper we articulate what kinds of skills\nan NLP scholar should have, and then adopt a backwards design to propose course\ncomponents that can aid the acquisition of these skills.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.05664v1.pdf",
        "similarity": 0.2157105468611708,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-11"
    },
    {
        "new_title": "Automating Pruning in Top-Down Enumeration for Program Synthesis\n  Problems with Monotonic Semantics",
        "new_link": "http://arxiv.org/abs/2408.15822v1",
        "new_summary": "  In top-down enumeration for program synthesis, abstraction-based pruning uses\nan abstract domain to approximate the set of possible values that a partial\nprogram, when completed, can output on a given input. If the set does not\ncontain the desired output, the partial program and all its possible\ncompletions can be pruned. In its general form, abstraction-based pruning\nrequires manually designed, domain-specific abstract domains and semantics, and\nthus has only been used in domain-specific synthesizers.\n  This paper provides sufficient conditions under which a form of\nabstraction-based pruning can be automated for arbitrary synthesis problems in\nthe general-purpose Semantics-Guided Synthesis (SemGuS) framework without\nrequiring manually-defined abstract domains. We show that if the semantics of\nthe language for which we are synthesizing programs exhibits some monotonicity\nproperties, one can obtain an abstract interval-based semantics for free from\nthe concrete semantics of the programming language, and use such semantics to\neffectively prune the search space. We also identify a condition that ensures\nsuch abstract semantics can be used to compute a precise abstraction of the set\nof values that a program derivable from a given hole in a partial program can\nproduce. These precise abstractions make abstraction-based pruning more\neffective.\n  We implement our approach in a tool, Moito, which can tackle synthesis\nproblems defined in the SemGuS framework. Moito can automate interval-based\npruning without any a-priori knowledge of the problem domain, and solve\nsynthesis problems that previously required domain-specific, abstraction-based\nsynthesizers -- e.g., synthesis of regular expressions, CSV file schema, and\nimperative programs from examples.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15822v1.pdf",
        "similarity": 0.21533556745165408,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Scaling Up Summarization: Leveraging Large Language Models for Long Text\n  Extractive Summarization",
        "new_link": "http://arxiv.org/abs/2408.15801v1",
        "new_summary": "  In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15801v1.pdf",
        "similarity": 0.21448704183205894,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Synthetic Forehead-creases Biometric Generation for Reliable User\n  Verification",
        "new_link": "http://arxiv.org/abs/2408.15693v1",
        "new_summary": "  Recent studies have emphasized the potential of forehead-crease patterns as\nan alternative for face, iris, and periocular recognition, presenting\ncontactless and convenient solutions, particularly in situations where faces\nare covered by surgical masks. However, collecting forehead data presents\nchallenges, including cost and time constraints, as developing and optimizing\nforehead verification methods requires a substantial number of high-quality\nimages. To tackle these challenges, the generation of synthetic biometric data\nhas gained traction due to its ability to protect privacy while enabling\neffective training of deep learning-based biometric verification methods. In\nthis paper, we present a new framework to synthesize forehead-crease image data\nwhile maintaining important features, such as uniqueness and realism. The\nproposed framework consists of two main modules: a Subject-Specific Generation\nModule (SSGM), based on an image-to-image Brownian Bridge Diffusion Model\n(BBDM), which learns a one-to-many mapping between image pairs to generate\nidentity-aware synthetic forehead creases corresponding to real subjects, and a\nSubject-Agnostic Generation Module (SAGM), which samples new synthetic\nidentities with assistance from the SSGM. We evaluate the diversity and realism\nof the generated forehead-crease images primarily using the Fr\\'echet Inception\nDistance (FID) and the Structural Similarity Index Measure (SSIM). In addition,\nwe assess the utility of synthetically generated forehead-crease images using a\nforehead-crease verification system (FHCVS). The results indicate an\nimprovement in the verification accuracy of the FHCVS by utilizing synthetic\ndata.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15693v1.pdf",
        "similarity": 0.21245231950878213,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Mining Field Data for Tree Species Recognition at Scale",
        "new_link": "http://arxiv.org/abs/2408.15816v1",
        "new_summary": "  Individual tree species labels are particularly hard to acquire due to the\nexpert knowledge needed and the limitations of photointerpretation. Here, we\npresent a methodology to automatically mine species labels from public forest\ninventory data, using available pretrained tree detection models. We identify\ntree instances in aerial imagery and match them with field data with close to\nzero human involvement. We conduct a series of experiments on the resulting\ndataset, and show a beneficial effect when adding noisy or even unlabeled data\npoints, highlighting a strong potential for large-scale individual species\nmapping.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15816v1.pdf",
        "similarity": 0.2117816007349819,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "CatalogBank: A Structured and Interoperable Catalog Dataset with a\n  Semi-Automatic Annotation Tool (DocumentLabeler) for Engineering System\n  Design",
        "new_link": "http://arxiv.org/abs/2408.08238v1",
        "new_summary": "  In the realm of document engineering and Natural Language Processing (NLP),\nthe integration of digitally born catalogs into product design processes\npresents a novel avenue for enhancing information extraction and\ninteroperability. This paper introduces CatalogBank, a dataset developed to\nbridge the gap between textual descriptions and other data modalities related\nto engineering design catalogs. We utilized existing information extraction\nmethodologies to extract product information from PDF-based catalogs to use in\ndownstream tasks to generate a baseline metric. Our approach not only supports\nthe potential automation of design workflows but also overcomes the limitations\nof manual data entry and non-standard metadata structures that have\nhistorically impeded the seamless integration of textual and other data\nmodalities. Through the use of DocumentLabeler, an open-source annotation tool\nadapted for our dataset, we demonstrated the potential of CatalogBank in\nsupporting diverse document-based tasks such as layout analysis and knowledge\nextraction. Our findings suggest that CatalogBank can contribute to document\nengineering and NLP by providing a robust dataset for training models capable\nof understanding and processing complex document formats with relatively less\neffort using the semi-automated annotation tool DocumentLabeler.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.08238v1.pdf",
        "similarity": 0.21149043337140522,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-15"
    },
    {
        "new_title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark",
        "new_link": "http://arxiv.org/abs/2408.14845v1",
        "new_summary": "  Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.14845v1.pdf",
        "similarity": 0.20939580330862848,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-27"
    },
    {
        "new_title": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP\n  Dataset and large language model",
        "new_link": "http://arxiv.org/abs/2408.11557v2",
        "new_summary": "  Large Language Model (LLM) has demonstrated significant success in a range of\nnatural language processing (NLP) tasks within general domain. The emergence of\nLLM has introduced innovative methodologies across diverse fields, including\nthe natural sciences. Researchers aim to implement automated, concurrent\nprocess driven by LLM to supplant conventional manual, repetitive and\nlabor-intensive work. In the domain of spectral analysis and detection, it is\nimperative for researchers to autonomously acquire pertinent knowledge across\nvarious research objects, which encompasses the spectroscopic techniques and\nthe chemometric methods that are employed in experiments and analysis.\nParadoxically, despite the recognition of spectroscopic detection as an\neffective analytical method, the fundamental process of knowledge retrieval\nremains both time-intensive and repetitive. In response to this challenge, we\nfirst introduced the Spectral Detection and Analysis Based Paper(SDAAP)\ndataset, which is the first open-source textual knowledge dataset for spectral\nanalysis and detection and contains annotated literature data as well as\ncorresponding knowledge instruction data. Subsequently, we also designed an\nautomated Q\\&A framework based on the SDAAP dataset, which can retrieve\nrelevant knowledge and generate high-quality responses by extracting entities\nin the input as retrieval parameters. It is worth noting that: within this\nframework, LLM is only used as a tool to provide generalizability, while RAG\ntechnique is used to accurately capture the source of the knowledge.This\napproach not only improves the quality of the generated responses, but also\nensures the traceability of the knowledge. Experimental results show that our\nframework generates responses with more reliable expertise compared to the\nbaseline.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11557v2.pdf",
        "similarity": 0.20902298471232247,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-21"
    },
    {
        "new_title": "Grand canonical generative diffusion model for crystalline phases and\n  grain boundaries",
        "new_link": "http://arxiv.org/abs/2408.15601v1",
        "new_summary": "  The diffusion model has emerged as a powerful tool for generating atomic\nstructures for materials science. This work calls attention to the deficiency\nof current particle-based diffusion models, which represent atoms as a point\ncloud, in generating even the simplest ordered crystalline structures. The\nproblem is attributed to particles being trapped in local minima during the\nscore-driven simulated annealing of the diffusion process, similar to the\nphysical process of force-driven simulated annealing. We develop a solution,\nthe grand canonical diffusion model, which adopts an alternative voxel-based\nrepresentation with continuous rather than fixed number of particles. The\nmethod is applied towards generation of several common crystalline phases as\nwell as the technologically important and challenging problem of grain boundary\nstructures.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15601v1.pdf",
        "similarity": 0.20726246838447918,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    },
    {
        "new_title": "Inside the Black Box: Detecting Data Leakage in Pre-trained Language\n  Encoders",
        "new_link": "http://arxiv.org/abs/2408.11046v1",
        "new_summary": "  Despite being prevalent in the general field of Natural Language Processing\n(NLP), pre-trained language models inherently carry privacy and copyright\nconcerns due to their nature of training on large-scale web-scraped data. In\nthis paper, we pioneer a systematic exploration of such risks associated with\npre-trained language encoders, specifically focusing on the membership leakage\nof pre-training data exposed through downstream models adapted from pre-trained\nlanguage encoders-an aspect largely overlooked in existing literature. Our\nstudy encompasses comprehensive experiments across four types of pre-trained\nencoder architectures, three representative downstream tasks, and five\nbenchmark datasets. Intriguingly, our evaluations reveal, for the first time,\nthe existence of membership leakage even when only the black-box output of the\ndownstream model is exposed, highlighting a privacy risk far greater than\npreviously assumed. Alongside, we present in-depth analysis and insights toward\nguiding future researchers and practitioners in addressing the privacy\nconsiderations in developing pre-trained language models.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.11046v1.pdf",
        "similarity": 0.2069269590797381,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-20"
    },
    {
        "new_title": "Identifying Influential and Vulnerable Nodes in Interaction Networks\n  through Estimation of Transfer Entropy Between Univariate and Multivariate\n  Time Series",
        "new_link": "http://arxiv.org/abs/2408.15811v1",
        "new_summary": "  Transfer entropy (TE) is a powerful tool for measuring causal relationships\nwithin interaction networks. Traditionally, TE and its conditional variants are\napplied pairwise between dynamic variables to infer these causal relationships.\nHowever, identifying the most influential or vulnerable node in a system\nrequires measuring the causal influence of each component on the entire system\nand vice versa. In this paper, I propose using outgoing and incoming transfer\nentropy-where outgoing TE quantifies the influence of a node on the rest of the\nsystem, and incoming TE measures the influence of the rest of the system on the\nnode. The node with the highest outgoing TE is identified as the most\ninfluential, or \"hub\", while the node with the highest incoming TE is the most\nvulnerable, or \"anti-hub\". Since these measures involve transfer entropy\nbetween univariate and multivariate time series, naive estimation methods can\nresult in significant errors, particularly when the number of variables is\ncomparable to or exceeds the number of samples. To address this, I introduce a\nnovel estimation scheme that computes outgoing and incoming TE only between\nsignificantly interacting partners. The feasibility of this approach is\ndemonstrated by using synthetic data, and by applying it to a real data of oral\nmicrobiota. The method successfully identifies the bacterial species known to\nbe key players in the bacterial community, demonstrating the power of the new\nmethod.\n",
        "pdf_link": "https://arxiv.org/pdf/2408.15811v1.pdf",
        "similarity": 0.20650906607428496,
        "existing_title": "Unknown Title",
        "existing_link": "https://arxiv.org/abs/2408.15905v1",
        "published": "2024-08-28"
    }
]